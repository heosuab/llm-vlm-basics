# 4.4 Serving Frameworks

> LLM을 production 환경에서 효율적으로 서빙하는 주요 프레임워크들을 비교합니다.

---

## vLLM

UC Berkeley에서 개발한 LLM serving 프레임워크입니다. 현재 오픈소스 serving의 표준입니다.

### 핵심 혁신

**PagedAttention** (자세한 내용은 4.2 참조):
- KV-cache를 OS 가상 메모리 방식으로 관리
- 메모리 낭비 최소화
- Prefix caching (공통 prefix의 KV-cache 재사용)

**Continuous batching**: 요청이 들어오는 즉시 처리, 높은 GPU utilization

### 사용 예시

```python
from vllm import LLM, SamplingParams

llm = LLM(model="meta-llama/Llama-3.1-8B-Instruct",
          tensor_parallel_size=2,
          gpu_memory_utilization=0.90)

outputs = llm.generate(
    ["Hello, how are you?", "What is the capital of France?"],
    SamplingParams(temperature=0.8, max_tokens=256)
)
```

### 특징

- OpenAI-compatible API server
- Multi-GPU tensor parallel 지원
- LoRA serving (여러 LoRA adapter를 동시에)
- Speculative decoding 지원
- FP8/INT8 quantization 지원

---

## TGI (Text Generation Inference)

HuggingFace의 LLM serving solution입니다.

### 특징

- **Flash Attention 통합**: 빠른 attention 연산
- **Tensor Parallelism**: 다중 GPU 지원
- **Continuous batching**
- **Token streaming**: 생성되는 token을 즉시 클라이언트에 전달
- **Quantization**: GPTQ, AWQ, bitsandbytes 지원

### Docker 기반 배포

```bash
docker run --gpus all \
  -p 8080:80 \
  -v $PWD/data:/data \
  ghcr.io/huggingface/text-generation-inference:latest \
  --model-id meta-llama/Llama-3.1-8B-Instruct \
  --num-shard 2
```

### vLLM vs TGI

| 항목 | vLLM | TGI |
|------|------|-----|
| 개발사 | UC Berkeley | HuggingFace |
| KV-cache 관리 | PagedAttention | 자체 구현 |
| 생태계 | Python 중심 | HF 생태계 통합 |
| 배포 | pip install | Docker 중심 |
| 커스터마이징 | 높음 | 중간 |

---

## TensorRT-LLM

NVIDIA에서 개발한 LLM 추론 최적화 라이브러리입니다. H100 같은 최신 NVIDIA GPU에서 최고 성능을 냅니다.

### 핵심 기능

**Kernel Fusion**: 여러 GPU 연산을 하나의 커널로 통합합니다:
```
Without fusion:
  1. LayerNorm → 2. Linear → 3. Activation → 4. Linear → 5. Add Bias
  = 5 kernel launches (각 kernel마다 메모리 read/write)

With fusion:
  LayerNorm_Linear_Activation_Linear_AddBias
  = 1 kernel launch (중간 결과를 레지스터에 유지)
```

**INT8/FP8 최적화**: H100의 FP8 tensor core 완전 활용

**In-flight batching**: continuous batching의 NVIDIA 구현

**Multi-GPU 지원**: TP, PP, TP+PP 모두 지원

### 단점

- 특정 모델 아키텍처는 별도 구현 필요
- NVIDIA GPU 전용
- 빌드 과정이 복잡

---

## SGLang

Language model programming을 위한 structured generation 프레임워크입니다.

### 핵심 기능

**RadixAttention (Prefix Caching)**:
Radix tree를 이용하여 KV-cache를 prefix 단위로 재사용합니다:

```
Request 1: "System prompt..." + "User: Hello" → 생성
Request 2: "System prompt..." + "User: World" → "System prompt..."의 KV-cache 재사용!

→ 긴 공통 prefix(system prompt)가 있는 경우 매우 효율적
```

**Structured Generation DSL**:
```python
import sglang as sgl

@sgl.function
def multi_turn(s, question):
    s += sgl.user(question)
    s += sgl.assistant(sgl.gen("answer", max_tokens=256))
    s += sgl.user("Can you explain more?")
    s += sgl.assistant(sgl.gen("explanation", max_tokens=512))
```

**JSON/Regex 제약 생성** 내장 지원

---

## Triton (OpenAI)

Python 비슷한 문법으로 **커스텀 GPU 커널**을 작성하는 언어/컴파일러입니다.

### 기존 방법 vs Triton

```
CUDA C++:
  __global__ void add_kernel(float* x, float* y, float* z, int n) {
      int idx = blockIdx.x * blockDim.x + threadIdx.x;
      if (idx < n) z[idx] = x[idx] + y[idx];
  }

Triton:
  @triton.jit
  def add_kernel(x_ptr, y_ptr, z_ptr, n, BLOCK_SIZE: tl.constexpr):
      pid = tl.program_id(axis=0)
      block_start = pid * BLOCK_SIZE
      offsets = block_start + tl.arange(0, BLOCK_SIZE)
      mask = offsets < n
      x = tl.load(x_ptr + offsets, mask=mask)
      y = tl.load(y_ptr + offsets, mask=mask)
      tl.store(z_ptr + offsets, x + y, mask=mask)
```

Triton은 CUDA보다 훨씬 쉽게 효율적인 GPU 커널을 작성할 수 있게 합니다.

**FlashAttention-2가 Triton으로 구현**되어 있습니다.

**사용 사례**:
- Custom attention variants
- Novel quantization 커널
- Specialized activation functions

---

## CUDA Kernel Fusion

### 개념

여러 연산을 하나의 GPU 커널로 합쳐서 메모리 접근 횟수를 줄입니다.

### 문제: 메모리 Bottleneck

```
개별 연산:
  GPU SRAM → HBM: write(LayerNorm result)
  HBM → GPU SRAM: read(LayerNorm result)
  GPU SRAM → HBM: write(Linear result)
  HBM → GPU SRAM: read(Linear result)
  ...
→ 메모리 bandwidth가 bottleneck
```

### 해결: Fused Kernel

```
Fused LayerNorm + Linear:
  GPU SRAM → HBM: 입력 1회 read
  GPU SRAM 내에서: LayerNorm → Linear → output
  GPU SRAM → HBM: 출력 1회 write
→ 중간 결과 메모리 이동 없음
```

**실제 예시**:
- FlashAttention: QK^T, softmax, ×V를 하나의 커널로
- Fused QKV projection: Q, K, V linear을 하나의 GEMM으로
- RMSNorm + activation: 하나의 커널로

**이점**: H100에서 최대 2~4배 speedup 가능
