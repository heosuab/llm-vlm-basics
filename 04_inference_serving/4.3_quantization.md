# 4.3 Quantization

> 모델의 파라미터를 낮은 precision으로 표현하여 메모리와 추론 속도를 개선합니다.

---

## 양자화(Quantization)란?

FP32/BF16의 고정밀도 값을 INT8/INT4/FP8 등 낮은 정밀도로 근사합니다:

```
원래 weight: 0.4231567...  (FP32, 32 bits)
INT8 양자화: round(0.4231567 / scale) = 54  (INT8, 8 bits)
역양자화:    54 × scale ≈ 0.423...

scale = max(|weights|) / 127
```

**왜 유용한가**:
- 메모리: FP16 → INT4 = 4배 절약
- 속도: INT8 연산은 FP16보다 2~4배 빠름 (INT8 tensor core 활용)
- 트레이드오프: 약간의 정확도 저하

---

## Post-Training Quantization (PTQ)

**학습 후**에 이미 완성된 모델을 양자화합니다. 재학습이 필요 없습니다.

### 과정

```
1. 원래 모델 (FP16/BF16)
2. Calibration dataset으로 각 레이어의 activation 분포 측정
3. Scale, zero-point 계산
4. 양자화 적용
```

**Calibration dataset**: 실제 입력과 비슷한 소량의 샘플 (보통 512~1024개)

**장점**: 별도 학습 불필요, 빠르게 적용 가능
**단점**: QAT(Quantization-Aware Training)보다 성능 저하 가능

---

## Weight-Only Quantization

**Weight만** INT4/INT8으로 양자화하고, activation은 FP16으로 유지합니다.

### 동작 방식

```
저장: INT4 weight
추론: INT4 → FP16 dequantize → FP16 matmul
```

dequantize 오버헤드가 있지만, **메모리 bandwidth가 병목인 decode 단계**에서는 매우 효과적입니다.

**Group Quantization**: 모든 weight를 하나의 scale로 처리하는 대신, g개 weight 그룹마다 독립적인 scale 적용:

```
group_size = 128  (128개 weight마다 별도 scale)
더 정확하지만 scale 저장 오버헤드 있음
```

---

## Activation Quantization (W8A8)

Weight와 Activation **둘 다** INT8로 양자화합니다:

```
INT8 weight × INT8 activation → INT32 accumulation → scale → INT8 output
```

**장점**: INT8 tensor core를 완전히 활용 → 최대 속도
**문제**: LLM의 activation에는 **outlier**가 있습니다

### Outlier 문제

일부 hidden dimension에서 activation이 갑자기 매우 큰 값(100~1000배)을 가집니다. 이를 INT8 범위 내로 clamp하면 다른 값들의 정밀도가 매우 낮아집니다.

**LLM.int8() (bitsandbytes)**: outlier 차원은 FP16으로, 나머지는 INT8로 처리하는 mixed precision approach.

---

## KV Quantization

KV-cache를 INT8 또는 INT4로 양자화합니다:

```
FP16 KV-cache: n_layers × seq_len × n_heads × d_head × 2 bytes
INT8 KV-cache: n_layers × seq_len × n_heads × d_head × 1 byte
→ 메모리 2배 절약
```

**주의**: key와 value의 양자화 민감도가 다릅니다. Key는 attention score 계산에 직접 사용되어 더 민감합니다.

**KVSharer, SnapKV** 등: 중복 KV-cache를 제거하거나 압축하는 추가 기법들.

---

## AWQ (Activation-aware Weight Quantization)

### 아이디어

모든 weight가 똑같이 중요하지 않습니다. **활성화 크기가 큰(salient) weight**는 더 정밀하게, 그렇지 않은 weight는 낮은 정밀도로 양자화합니다.

```
Activation 크기가 큰 input dimension의 weight:
  → scale을 키워서 상대적 정밀도 유지
  → 대신 quantized range 내에서 더 촘촘한 구간 확보
```

### 핵심 수식

```
W_quantized = quantize(W / s) × s

여기서 s (per-channel scale): activation 통계로부터 결정
  s_j = mean(|X_j|)^α  (j: input channel)
```

**결과**:
- 중요한 weight(큰 activation이 통과하는 곳)는 더 정확하게 양자화
- 전체적으로 INT4에서도 FP16에 가까운 성능 유지

---

## GPTQ

### 아이디어

Layer-wise 양자화를 **2차 정보(Hessian)**를 활용하여 수행합니다.

```
목표: min ||WX - Ŵ X||²_F  (W: 원래 weight, Ŵ: 양자화된 weight)
```

각 weight를 양자화할 때 발생하는 오차를 **나머지 weight 조정으로 보상**합니다:

```
1. Weight 한 개 양자화 (e = W_ij - quantize(W_ij))
2. 오차 e를 같은 row의 나머지 weight들에 분산
   → Hessian의 inverse로 보상 크기 결정
3. 다음 weight로 이동
```

**결과**: 단순 라운딩보다 훨씬 낮은 양자화 오차

**실용 사양**:
- INT4 + group_size=128이 가장 많이 사용
- 70B 모델 → ~35 GB (vs FP16 140 GB)
- llama.cpp, AutoGPTQ 구현

---

## FP8 Inference

H100 GPU에서 FP8 tensor core를 네이티브 지원합니다.

### FP8 포맷

```
E4M3: 4 exponent bits + 3 mantissa bits → 작은 값에 정밀, 범위 ±448
E5M2: 5 exponent bits + 2 mantissa bits → 더 넓은 범위, 더 낮은 정밀도
```

**활용**:
- Weight: E4M3 (정밀도 중요)
- Activation gradient: E5M2 (범위 중요)

**DeepSeek-V3**: FP8 mixed-precision으로 학습하여 비용을 크게 줄였습니다.

### 비교 표

| 포맷 | 비트 | 메모리 | 속도 | 정밀도 |
|------|------|--------|------|--------|
| FP32 | 32 | 1× | 1× | 최고 |
| BF16 | 16 | 2× | 2× | 높음 |
| FP8 | 8 | 4× | 4× | 좋음 |
| INT8 | 8 | 4× | 4× | 좋음 |
| INT4 | 4 | 8× | 2× (decode) | 보통 |

**주의**: 모든 수치는 대략적이며 GPU 아키텍처와 구현에 따라 다릅니다.
