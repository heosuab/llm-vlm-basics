# 4.5 Inference Metrics

> LLM 서빙 시스템의 성능을 측정하고 비교하는 지표들을 설명합니다.

---

## Throughput (tokens/sec)

### 정의

단위 시간당 생성된 output token 수입니다:

```
Throughput = total_output_tokens / total_time (seconds)
```

**측정 방법**:
```
총 10,000개 요청, 각 256 output tokens, 총 소요 시간 60초
→ Throughput = 10,000 × 256 / 60 ≈ 42,667 tokens/sec
```

### Throughput과 GPU Utilization

높은 throughput = GPU를 효율적으로 사용한다는 의미입니다.

**Throughput에 영향을 주는 요소**:
- Batch size: 클수록 좋음 (연산 효율 증가)
- KV-cache 용량: 더 많은 request를 동시에 처리 가능
- Model quantization: INT4는 더 큰 batch 허용
- Tensor parallel: 여러 GPU에 분산

**Input vs Output throughput**: Output token 생성이 더 병목이 됩니다 (decode phase는 memory-bound).

---

## TTFT (Time To First Token)

### 정의

요청이 들어온 시점부터 **첫 번째 output token**이 반환될 때까지의 시간입니다:

```
t_TTFT = t_first_token - t_request_received
```

### 왜 중요한가?

사용자 경험에서 "응답이 시작되는 느낌"을 결정합니다. Streaming 응답에서 특히 중요합니다.

**영향 요소**:
1. **Prompt 길이**: 긴 prompt → 긴 prefill → 높은 TTFT
2. **Batch 내 다른 request의 prefill**: 다른 request의 prefill이 진행 중이면 queue 대기
3. **Model 크기**: 큰 모델 = 긴 forward pass

**일반적인 값**:
- 짧은 prompt (100 tokens), 8B 모델: ~50-200ms
- 긴 prompt (4096 tokens), 70B 모델: ~2-10초

### Chunked Prefill

긴 prompt를 청크로 나눠서 prefill과 decode를 번갈아 처리합니다:
→ 긴 prompt가 decode(다른 request)를 블락하는 시간을 줄여 TTFT를 개선합니다.

---

## Tail Latency

### 정의

P95, P99 latency입니다. 요청의 95% 또는 99%가 이 시간 내에 완료됩니다.

```
Latency 분포 예시:
  Median (P50): 500ms
  P90:          1000ms
  P95:          2000ms
  P99:          5000ms
```

### 왜 Median이 아닌 Tail Latency가 중요한가?

- **SLA (Service Level Agreement)**: "요청의 99%는 X초 내에 응답" 형태의 계약
- 배치 처리에서 하나의 느린 요청이 전체를 지연시킴
- 사용자가 가끔씩 매우 느린 응답을 경험하는 것이 큰 불만족

### Tail Latency가 높은 이유

- **긴 생성 요청**: 많은 token을 생성하는 요청은 시간이 오래 걸림
- **Queue buildup**: 트래픽 spike 시 waiting time 증가
- **메모리 pressure**: KV-cache가 꽉 찰 때 일부 요청이 preempt되어 재시작

---

## Cost per Token

### 정의

output token 1개를 생성하는 비용입니다:

```
Cost per token = GPU cost per hour / (throughput × 3600)
```

**예시 계산**:
```
H100 GPU 임대 비용: $3/hour
Throughput: 10,000 tokens/sec

Cost per output token = $3 / (10,000 × 3600) = $0.0000000833
≈ $0.083 per million tokens
```

실제 API 가격 참고 (2024 기준):
- GPT-4o: ~$5-15 per million tokens
- LLaMA-3 70B (self-hosted): ~$0.5-2 per million tokens

### Cost 최적화 방법

| 방법 | 효과 |
|------|------|
| Quantization (INT4) | 2~4배 더 많은 request 처리 |
| Smaller model | 직접 비용 감소 |
| Speculative decoding | 같은 비용으로 더 많은 tokens |
| Continuous batching | GPU utilization 향상 |
| Prefix caching | 반복 요청 비용 감소 |

---

## 종합 비교 표

| 지표 | 측정 대상 | 최적화 방법 | 트레이드오프 |
|------|----------|------------|------------|
| Throughput | GPU 전체 효율 | 큰 batch, quantization | latency 증가 가능 |
| TTFT | 첫 응답 속도 | chunked prefill, 작은 모델 | throughput 감소 |
| Tail latency | SLA, 최악의 경험 | request 길이 제한, priority queue | — |
| Cost/token | 경제성 | 위 모두 | quality 저하 가능 |

**실제 production에서의 우선순위**:
- Chat 서비스: TTFT + Tail latency 우선
- Batch processing: Throughput + Cost 우선
- 실시간 응용(코드 자동완성): TTFT가 가장 중요
