# 4.2 Inference Efficiency

> LLM 추론을 빠르고 효율적으로 만드는 기법들.

---

## KV-Cache

자세한 내용은 [1.2 Attention Details](../01_transformer_architecture/1.2_attention_details.md)에서 다뤘으며, 여기서는 실용적 관점들에 대한 추가/.

### KV-Cache 메모리 계산

```
KV-Cache 크기 = 2 × n_layers × n_kv_heads × d_head × seq_len × dtype_bytes

예시 (LLaMA-3 8B):
  n_layers = 32
  n_kv_heads = 8  (GQA)
  d_head = 128
  seq_len = 8192
  BF16 = 2 bytes
  → 2 × 32 × 8 × 128 × 8192 × 2 = 1.07 GB

예시 (LLaMA-3 70B):
  n_layers = 80
  n_kv_heads = 8
  d_head = 128
  seq_len = 8192
  → 2 × 80 × 8 × 128 × 8192 × 2 = 2.68 GB
```

긴 시퀀스와 많은 동시 요청이 있으면 KV-cache가 GPU 메모리의 상당 부분을 차지함.

---

## Prefill vs Decode Phase

LLM 추론은 두 단계로 나뉘며, 각각 다른 병목이 있음.

### Prefill (Prompt Processing)

```
입력: 전체 prompt ["How", "are", "you", "?"]  (T tokens)
출력: 첫 번째 output token + KV-cache 완성

특성:
- 모든 T개 token을 한 번에 병렬 처리 (행렬 × 행렬)
- Compute-bound: GPU core 연산이 병목
- 빠름: T tokens를 거의 동시에 처리
- TTFT(Time to First Token)와 직결
```

### Decode (Token Generation)

```
입력: 이전 token 1개 + KV-cache
출력: 다음 token 1개

특성:
- 한 번에 1 token만 처리 (행렬 × 벡터)
- Memory-bandwidth-bound: KV-cache 읽기가 병목
- 느림: token당 전체 KV-cache를 읽어야 함
- Throughput과 latency 모두에 영향
```

**최적화 방향**:
- Prefill: 큰 batch, FlashAttention → GPU utilization 높임
- Decode: KV-cache 압축, GQA/MQA, quantization → bandwidth 줄임

---

## PagedAttention

vLLM에서 개발한 혁신적인 KV-cache 메모리 관리 방법.

### 문제: KV-Cache 메모리 낭비

기존 방식에서는 각 request에 **최대 시퀀스 길이만큼** 메모리를 미리 할당함:

```
Request A: 최대 2048 token 할당 → 실제 사용: 500 tokens → 1548 token 낭비
Request B: 최대 2048 token 할당 → 실제 사용: 1800 tokens → 248 token 낭비
```

→ 메모리 조각화(fragmentation)와 낭비로 동시 처리 가능한 request 수가 제한됨.

### 해결책: OS Virtual Memory 차용

OS의 페이지 테이블 방식을 KV-cache에 적용함:

```
KV-cache를 고정 크기 "block"으로 나눔 (예: 16 token 단위)
각 request는 필요한 만큼의 block만 할당 (연속적 물리 메모리 불필요)
Block 테이블로 논리적 위치 → 물리적 위치 매핑
```

**결과**:
- 메모리 낭비 거의 없음 (fragmentation < 4%)
- 같은 GPU에서 더 많은 request 동시 처리
- Copy-on-write로 prefix sharing 가능

---

## Continuous Batching

### 문제: Static Batching의 비효율

기존 static batching:
- Batch 내 모든 request가 완료될 때까지 기다림
- 짧은 request가 먼저 끝나도, 긴 request가 끝날 때까지 GPU idle
- 새 request는 현재 batch가 완전히 끝날 때까지 대기

```
Static:
  Batch 1: [req A (200 tokens), req B (50 tokens), req C (180 tokens)]
  req B 완료 → GPU 일부 idle, req D는 Batch 2까지 대기
```

### 해결책: Continuous (Iteration-level) Batching

**각 iteration(step)마다** 완료된 request를 제거하고 새 request를 추가함:

```
Step 1: [req A, req B, req C]
Step 2: [req A, req B, req C]
...
Step 51: req B 완료 → [req A, req D (새로 추가), req C]  ← 즉시 새 request 처리
Step 52: [req A, req D, req C]
...
```

**효과**: GPU utilization이 크게 향상됨. 처리량이 최대 수십 배 증가하는 경우도 있음.

---

## Speculative Decoding

### 아이디어

작은 draft model로 여러 token을 미리 생성하고, 큰 target model이 한 번의 forward pass로 검증함.

### 과정

```
1. Draft model (작고 빠름):
   "The" → "cat" "sat" "on" "the" "mat" (5개 token 제안)

2. Target model (크고 느림):
   ["The", "cat", "sat", "on", "the", "mat"] 을 한 번에 parallel로 처리
   → 각 위치에서 draft token을 accept/reject 결정

3. Accept/Reject 기준:
   P_target(token) / P_draft(token) ≥ U[0,1] → Accept
   otherwise → Reject (target model의 분포에서 새로 샘플링)

4. 결과:
   모든 5개 accept → 5 tokens 생성 (1 forward pass)
   3번째에서 reject → 3 tokens 생성 (1 forward pass)
   → 평균적으로 1 forward pass로 1개 이상 생성
```

**보장**: Target model의 분포와 **완전히 동일한** 출력 분포를 유지함 (exact decoding).

### 실용적 고려사항

- Draft model과 target model은 **같은 tokenizer**를 사용해야 함
- Draft model이 target model과 유사할수록 accept rate 높음
- 일반적으로 2~3배 throughput 향상
- **EAGLE, Medusa**: target model 자체에서 draft head를 학습하는 방법

---

## Early Exit Decoding

각 token을 항상 모든 레이어를 거칠 필요 없이, **충분히 확실하면** 중간 레이어에서 exit함.

```
Layer 8:  P(next_token) = 0.60  ← 아직 불확실
Layer 16: P(next_token) = 0.85  ← exit (threshold=0.8 기준)
Layer 32: P(next_token) = 0.91  ← 생략

결과: 해당 token은 16/32 레이어만 사용 → 약 2배 빠름
```

**도전**:
- 각 레이어에 prediction head가 필요 (추가 파라미터)
- Threshold 설정이 어려움
- 품질과 속도의 트레이드오프

---

## Draft Models (Speculative Decoding용)

### 특성

- Target model과 같은 tokenizer 필수
- 훨씬 작고 빨라야 함 (보통 10~100배 작음)
- 비슷한 분포여야 accept rate가 높음

### 방법들

**별도 모델 사용**: LLaMA-3 70B의 draft model로 LLaMA-3 8B 사용

**Self-speculative**: 같은 모델의 일부 레이어만 사용
→ 처음 N개 레이어로 draft, 전체 레이어로 verify

**EAGLE**: target model의 중간 hidden state를 활용하여 next-next token을 예측하는 작은 head 학습

**Medusa**: 여러 개의 parallel decode head를 target model에 추가
→ 1 forward pass로 k개 token의 후보를 동시에 생성
