# 4.1 Decoding Strategies

> Autoregressive generation에서 다음 token을 어떻게 선택할지 결정하는 전략들입니다.

---

## Greedy Decoding

### 방법

매 step마다 가장 높은 확률의 token을 선택합니다:

```python
next_token = argmax(logits)
```

**장점**: 결정론적(deterministic), 빠름
**단점**:
- 지역 최적(local optimum)에 쉽게 빠짐
- 반복 생성 경향 ("I think I think I think...")
- Creative generation에 부적합

---

## Beam Search

### 방법

상위 k개의 가설(hypothesis)을 동시에 유지하면서 탐색합니다:

```
k=3 beam search 예시:
Step 1: ["The", "A", "In"] (top-3 token)
Step 2: "The" → ["The cat", "The dog", "The bird"]
        "A"   → ["A cat", "A dog", "A lion"]
        "In"  → ["In the", "In a", "In my"]
        → 전체 9개 중 상위 3개만 유지
Step 3: 계속 반복...
```

**Score**: 각 가설의 log probability 합

```python
score(sequence) = Σ_t log P(token_t | token_{<t}) / len(sequence)^α
```

α: length normalization 파라미터 (0.6~0.8이 일반적)
길이 페널티 없으면 짧은 시퀀스가 유리합니다 (log probability가 음수이므로 길수록 낮아짐).

**장점**: Greedy보다 더 나은 global solution
**단점**:
- Open-ended generation에서 다양성 부족 (하나의 "최선"을 추구)
- 계산 비용 k배 증가
- 번역·요약에는 좋지만 자유로운 대화에는 부적합

---

## Top-k Sampling

### 방법

상위 k개 token 중에서만 랜덤 샘플링합니다:

```python
# 확률 분포에서 상위 k개만 남기고 나머지는 0으로
top_k_probs, top_k_indices = torch.topk(probs, k=40)
# 정규화 후 샘플링
next_token = top_k_indices[multinomial_sample(top_k_probs)]
```

**k 값 선택**:
- k가 작으면 (k=10): 안전하고 일관되지만 다양성 부족
- k가 크면 (k=100): 다양하지만 저품질 token도 포함될 수 있음
- 일반적으로 k=40~50

**문제**: 확률 분포가 뾰족하거나 넓은 경우를 구분하지 못합니다.
- 분포가 뾰족한 경우: k=50이어도 99%의 확률이 상위 3개에 집중 → 사실상 greedy
- 분포가 넓은 경우: k=50이어도 많은 저품질 token이 포함

---

## Top-p (Nucleus) Sampling

### 방법

누적 확률이 p를 넘을 때까지의 token들로만 샘플링합니다:

```python
# 확률을 내림차순으로 정렬
sorted_probs, sorted_indices = torch.sort(probs, descending=True)
# 누적 확률 계산
cumsum = torch.cumsum(sorted_probs, dim=-1)
# 누적 확률이 p를 초과하는 지점 찾기
sorted_indices_to_remove = cumsum > p
# 첫 번째 초과 token까지는 유지
sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
sorted_indices_to_remove[..., 0] = False
```

**직관**: 분포가 뾰족하면 nucleus가 작고 (소수의 token), 분포가 넓으면 nucleus가 큽니다.
→ 상황에 맞게 자동으로 후보 수가 조정됩니다.

**p=0.9**: 상위 90% 확률 질량에 해당하는 token들만 사용
**p=0.95**: 더 많은 후보, 더 다양한 출력

Top-p는 top-k보다 더 robust한 방법으로, 현재 가장 널리 사용됩니다.

---

## Temperature Scaling

### 방법

Softmax 전에 logits를 temperature T로 나눕니다:

```python
adjusted_logits = logits / temperature
probs = softmax(adjusted_logits)
```

**효과**:

| Temperature | 결과 | 사용 시기 |
|-------------|------|---------|
| T → 0 | Greedy (가장 높은 확률 token 선택) | 결정론적 출력 필요 시 |
| T = 1 | 원래 모델 분포 그대로 | 기본값 |
| T > 1 | 분포가 평탄해짐 (더 다양한 선택) | Creative writing |
| T < 1 | 분포가 뾰족해짐 (더 확실한 선택) | 사실 기반 질문 |

**일반 사용 패턴**:
- Coding/Math: T=0 (greedy) or T=0.1
- Chat: T=0.7~0.9
- Creative writing: T=0.9~1.2

---

## Repetition Penalty

### 방법

이미 생성한 token들의 logit에 패널티를 적용합니다:

```python
for token_id in set(generated_tokens):
    if logits[token_id] > 0:
        logits[token_id] /= penalty
    else:
        logits[token_id] *= penalty
```

패널티 > 1 (예: 1.1~1.3): 이미 생성된 token이 다시 선택될 확률을 낮춤

**장점**: 반복 생성 방지
**단점**: 너무 크면 자연스러운 반복(관사, 연결어 등)도 억제됨

---

## Length Penalty

Beam search에서 길이에 대한 편향을 조정합니다:

```
score = log_prob / (length^α)
```

- α > 0: 긴 시퀀스를 선호 (번역에서 충분히 긴 번역문 생성)
- α < 0: 짧은 시퀀스를 선호
- α = 0: 길이 무관

일반적으로 α = 0.6~1.0을 사용합니다.

---

## Constrained Decoding

출력이 특정 제약 조건을 만족하도록 강제합니다.

### Trie 기반 제약

특정 단어나 구문이 **반드시 포함**되어야 할 때 사용합니다:

```
Trie: ["Barack Obama", "Joe Biden", "Donald Trump"]

생성 중 prefix가 "Barack"이면:
  → "Obama"만 다음 token으로 허용 (trie에 의해 강제)
```

### Logit Bias

특정 token의 logit을 수동으로 조정:
```python
logit_bias = {
    "token_id_A": -100,  # 이 token은 절대 생성 안 함
    "token_id_B": 10,    # 이 token의 확률을 높임
}
logits[token_id] += logit_bias.get(token_id, 0)
```

---

## JSON Schema Decoding

출력이 반드시 valid JSON이 되도록, 그리고 특정 schema를 따르도록 강제합니다.

### Character-Level FSM (Finite State Machine)

JSON의 문법을 FSM으로 표현하고, 각 state에서 valid한 다음 token만 허용합니다:

```
State: "JSON object 시작"
  → 허용 token: '{'
State: "key 시작"
  → 허용 token: '"'
State: "key 내부"
  → 허용 token: 모든 string character
...
```

### Token Masking

각 생성 step에서 현재 JSON state에서 **invalid한 token들의 logit을 -inf**로 설정합니다:

```python
valid_token_mask = compute_valid_tokens(current_json_state)
logits[~valid_token_mask] = -float('inf')
next_token = sample(softmax(logits))
```

**활용**:
- Function calling (structured output)
- 데이터 추출 (항상 valid JSON 반환 보장)
- API response 형식 강제

Outlines, Guidance, SGLang 라이브러리가 이 기능을 제공합니다.
