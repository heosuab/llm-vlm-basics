# 8.5 VLM Model Families: 모델 계보 총정리 (2021~2026년 2월)

## 1부: Foundation Visual-Language Models

### 1.1 CLIP (2021, OpenAI)

**논문**: "Learning Transferable Visual Models From Natural Language Supervision" (Radford et al., 2021)

CLIP은 현대 VLM 생태계의 **근간**을 형성한 모델.

**핵심 혁신**:
- **Contrastive Language-Image Pretraining**: 4억 개의 인터넷 이미지-텍스트 쌍으로 대조 학습
- **Zero-shot Transfer**: 별도 파인튜닝 없이 ImageNet 포함 다양한 분류 태스크에서 우수한 성능
- **범용 Visual Encoder**: CLIP의 ViT는 이후 수천 개의 다운스트림 모델에서 Visual Encoder로 재사용됨

**구조**:
- Image Encoder: ViT-B/32, ViT-B/16, ViT-L/14, ViT-H/14 (다양한 크기)
- Text Encoder: 63M 파라미터 Transformer
- InfoNCE Loss (배치 크기 32,768)

**영향**: CLIP이 없었다면 현재의 VLM 발전 속도는 훨씬 느렸을 것. 대부분의 오픈소스 VLM이 CLIP ViT-L/14를 Visual Encoder로 사용했음.

---

### 1.2 ALIGN (2021, Google)

**논문**: "Scaling Up Visual and Language Representations with Noisy Text Supervision" (Jia et al., 2021)

**핵심 혁신**:
- CLIP과 같은 시기에 독립적으로 발표된 비슷한 접근법
- **더 큰 데이터셋**: 18억 개(1.8B) 이미지-텍스트 쌍 (CLIP의 4배 이상)
- "노이즈가 많아도 스케일이 충분하면 성능이 나온다"는 것을 증명
- EfficientNet을 Image Encoder로 사용 (CNN 기반)

**의의**: CLIP과 함께 "대규모 Noisy 웹 데이터로 강력한 Vision-Language 표현을 학습할 수 있다"는 패러다임을 확립.

---

### 1.3 SigLIP (2023, Google)

**논문**: "Sigmoid Loss for Language Image Pre-Training" (Zhai et al., 2023)

**핵심 혁신**:
- **Sigmoid Loss**: CLIP의 Softmax(InfoNCE)를 독립적인 이진 분류로 대체
- **배치 독립성**: 작은 배치에서도 효과적 → 분산 학습에서 All-gather 불필요
- **VLM 성능 향상**: SigLIP-pretrained ViT를 VLM의 Visual Encoder로 사용하면 CLIP보다 일관되게 우수

**구조 변형**:
- SigLIP-B/16 (86M 파라미터)
- SigLIP-L/14 (307M 파라미터)
- SigLIP-So400m (400M 파라미터, "So"는 Shape-Optimized)

**현재 상태**: InternVL, LLaVA-NeXT/OneVision, Gemma 3 Vision 등 많은 최신 VLM이 SigLIP으로 사전학습된 ViT를 Visual Encoder로 채택.

---

## 2부: Early VLMs (2022~2023)

### 2.1 Flamingo (2022, DeepMind)

**논문**: "Flamingo: a Visual Language Model for Few-Shot Learning" (Alayrac et al., 2022)

멀티모달 분야에서 GPT-3에 비견될 수 있는 **기념비적인 모델**.

**핵심 혁신**:

**1) Few-shot Multimodal In-context Learning**:
```
프롬프트 예시 (3-shot):
[이미지1: 개] "이것은 강아지입니다." →
[이미지2: 고양이] "이것은 고양이입니다." →
[이미지3: ?] "이것은 ___입니다." → 모델이 자동으로 완성
```
GPT-3처럼 몇 개의 예시만으로 새로운 멀티모달 태스크를 수행.

**2) Perceiver Resampler**:
- 가변 크기의 이미지 특징을 고정 크기(64개)의 시각 표현으로 압축
- Vision Encoder와 LLM 사이의 효율적인 연결

**3) Gated Cross-Attention**:
- LLM의 각 레이어 사이에 Cross-Attention 레이어 삽입
- `tanh(gate)` 로 시각 정보의 영향도 조절
- 학습 초기에는 LLM과 동일하게 동작, 점차 시각 정보 통합

**4) Interleaved Image-Text Training**:
- "이미지와 텍스트가 섞인" 웹 문서(MultiPage, M3W 데이터셋)로 학습
- 긴 멀티모달 문서에서 이미지와 텍스트의 관계를 이해

**구조**:
- Vision Encoder: NFNet (CNN 기반, 80M 파라미터)
- LLM: Chinchilla (70B)
- Flamingo 3B, 9B, 80B 버전

**영향**: 멀티모달 In-context Learning, Perceiver Resampler, Gated Cross-Attention은 이후 많은 모델에 영향.

---

### 2.2 BLIP (2022, Salesforce)

**논문**: "BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation" (Li et al., 2022)

**핵심 혁신**:
- **Bootstrapped Caption Filtering**: 웹 데이터의 Noisy 캡션을 먼저 학습된 모델로 필터링 → 더 깨끗한 데이터로 재학습 (부트스트래핑)
- **Unified 아키텍처**: 이미지-텍스트 이해(분류) + 생성(캡셔닝) 모두 하나의 모델로 처리
- **ITC, ITM, LM** 세 가지 학습 목표 동시 사용:
  - ITC (Image-Text Contrastive): CLIP 스타일 대조 학습
  - ITM (Image-Text Matching): 이미지-텍스트 매칭 여부 분류
  - LM (Language Modeling): 이미지 조건부 텍스트 생성

---

### 2.3 BLIP-2 (2023, Salesforce)

**논문**: "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models" (Li et al., 2023)

**핵심 혁신**:
- **Q-Former (Querying Transformer)**: 32개의 학습 가능한 Query Token이 이미지 특징에서 정보 추출 (8.2절 참조)
- **Modular Design**: Vision Encoder(동결) + Q-Former(학습) + 임의의 LLM(동결)
- "다양한 LLM을 플러그인처럼 교체 가능한 VLM"이라는 패러다임 확립
- 매우 효율적인 학습 (전체 파라미터의 일부만 학습)

```
BLIP-2의 모듈 조합 예시:
- EVA-CLIP ViT-G/14 + Q-Former + OPT-6.7B
- EVA-CLIP ViT-G/14 + Q-Former + FlanT5-XXL
```

---
### 2.4 InstructBLIP (2023, Salesforce)

논문:  
**"InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning"**  
(Dai et al., 2023)

---

### 핵심 혁신:

- **Instruction-aware Q-Former**
  - 기존 BLIP-2의 Q-Former 확장
  - 단순히 이미지 특징만 추출하는 것이 아니라,
    **텍스트 Instruction을 함께 입력받아 이미지에서 필요한 정보만 선택적으로 추출**
  - → *Instruction-conditioned visual feature extraction*

- **Vision-Language Instruction Tuning**
  - 다양한 VLM 태스크를 하나의 포맷으로 통합:
    ```
    Instruction + Image → Text Output
    ```
  - 예시:
    - "Describe the image."
    - "What is the man holding?"
    - "Is this image safe?"
  - → 자연어 지시문 기반 범용 모델로 확장

- **2-Stage 학습 구조**
  1. **Stage 1:** BLIP-2 방식 (Vision Encoder + Q-Former 학습)
  2. **Stage 2:** 대규모 Instruction Tuning
     - 다중 VQA, Captioning, Reasoning 데이터 활용
     - LLM과 alignment

- **Modular Design 유지**
  - Vision Encoder (동결)
  - Instruction-aware Q-Former (학습)
  - LLM (대부분 동결)
  - → Parameter-efficient 학습 유지
---

## 3부: Open-Source VLM Wave — LLaVA Era (2023~2024)

### 3.1 LLaVA (2023, UW-Madison & Microsoft)

**논문**: "Visual Instruction Tuning" (Liu et al., 2023)

오픈소스 VLM 연구의 흐름을 바꾼 **게임 체인저**.

**핵심 혁신**:
- **극도로 단순한 구조**: CLIP ViT-L/14 + Linear Projector + Vicuna-13B
- **Visual Instruction Tuning**: GPT-4를 이용해 생성한 멀티모달 지시 데이터로 파인튜닝
- "단순한 구조 + 좋은 데이터 = 강력한 VLM"이라는 교훈 제시

```
LLaVA 아키텍처:
이미지 → CLIP ViT-L/14 → 256 패치 토큰 → Linear (1024→4096) → Vicuna LLM
```

**데이터 생성**: GPT-4(텍스트 전용)에게 이미지의 캡션과 바운딩 박스 정보를 주고, 해당 이미지에 대한 대화 데이터를 생성시킴.

---

### 3.2 LLaVA-1.5 (2023, UW-Madison)

**논문**: "Improved Baselines with Visual Instruction Tuning" (Liu et al., 2023)

**핵심 혁신**:
- **MLP Projector**: Linear 대신 2-layer MLP (GELU 활성화) → 성능 향상
- **ShareGPT4V 데이터**: GPT-4V를 활용해 더 풍부한 상세 설명 데이터 생성
- **더 큰 LLM**: LLaMA-2 13B, Vicuna-13B 기반
- **더 높은 해상도**: 336×336 (기존 224×224)
- 당시 기준 최고 성능을 **매우 적은 학습 비용**으로 달성

**의의**: "VLM은 비싸고 복잡해야 한다"는 통념을 깨고, 효율적인 학습 방법의 가능성 증명.

---

### 3.3 LLaVA-NeXT / LLaVA-1.6 (2024, UW-Madison)

**핵심 혁신**:
- **AnyRes**: 고해상도 타일링 방식으로 최대 4개 타일 + 썸네일 처리 (8.3절 참조)
- **더 강력한 LLM**: Mistral-7B, Mixtral-8×7B, LLaMA-3 기반
- OCR, 문서 이해 성능 대폭 향상
- 다양한 해상도를 지원하는 Dynamic Resolution 도입

---

### 3.4 LLaVA-OneVision (2024, UW-Madison & ByteDance)

**핵심 혁신**:
- **Single-image, Multi-image, Video** 모든 태스크를 하나의 모델에서 통합
- SigLIP-So400m을 Visual Encoder로 채택
- AnyRes 방식을 비디오에도 적용
- 강력한 비디오 이해 능력
- 오픈소스 모델 중 최고 수준의 성능 (2024 중반 기준)

---

## 4부: Strong Open-Source Models

### 4.1 InternVL 시리즈 (Shanghai AI Lab)

InternVL은 현재 **오픈소스 VLM 중 가장 강력한 계보** 중 하나.

**InternVL 1 (2023)**:
- **InternViT**: CLIP과 별개로 자체 개발한 대규모 Vision Transformer
  - InternViT-6B: 60억 파라미터의 거대 Vision Encoder
- LLaMA 계열 LLM과 결합
- 특히 Chinese(중국어) 멀티모달 이해에 강점

**InternVL 1.5 (2024)**:
- **Dynamic High Resolution**: 이미지를 최대 40개 타일로 분할 (4K 이상 해상도 지원)
- OCR, 문서 이해에서 GPT-4V에 필적하는 성능
- InternLM2-20B LLM과 결합

**InternVL 2 (2024)**:
- **Multi-scale Vision Features**: 여러 레이어의 시각 특징을 결합
- 더 작은 모델(2B, 4B, 8B, 26B, 40B, 76B)에서 우수한 성능
- 강력한 OCR, 다국어 지원, 수학/과학 추론

**InternVL 2.5 (2024~2025)**:
- InternViT-6B + InternLM2.5-20B 또는 LLaMA-3 결합
- MMMU, DocVQA, ChartQA 등 주요 벤치마크에서 최고 수준
- 비디오 이해 능력 추가

**InternVL3 (2025)**:
- **Jointly Trained Vision Encoder**: Vision Encoder와 LLM을 처음부터 함께 학습
- 별도 사전학습된 CLIP/SigLIP 없이 End-to-end 학습
- 더 나은 Vision-Language 정렬 달성
- 오픈소스 모델 최고 수준 성능 (2025 기준)

---

### 4.2 Qwen-VL / Qwen2-VL 시리즈 (Alibaba)

**Qwen-VL (2023)**:
- Qwen-7B LLM 기반
- 448×448 해상도, 256 시각 토큰
- 강력한 중국어 멀티모달 이해
- 위치 지정 태스크 (Grounding) 지원

**Qwen2-VL (2024)**:

**핵심 혁신 — M-RoPE (Multimodal Rotary Position Embedding)**:
```
기존 LLM의 1D RoPE: 텍스트 토큰에 1차원 위치 정보
M-RoPE: 이미지는 2D (행, 열), 비디오는 3D (시간, 행, 열), 텍스트는 1D
→ 각 모달리티의 자연스러운 위치 구조를 그대로 표현
```

- **Native Resolution Processing**: 이미지를 원래 해상도로 처리, 최소한의 패딩
- **Dynamic Token Count**: 해상도에 따라 시각 토큰 수가 자동 조절 (128~1280개)
- 강력한 문서 이해, 비디오 이해, 에이전트 태스크
- 72B 버전은 상업 모델에 필적하는 성능

**Qwen2.5-VL (2025)**:

**핵심 혁신**:
- **Video Understanding**: 긴 비디오(몇 시간 분량)의 이해 능력
- **Agentic Visual Tasks**: 스크린샷을 보고 GUI를 조작하는 능력 (웹 브라우저, 앱 조작)
- **Document Understanding**: 복잡한 표, 수식이 포함된 문서 정확히 파싱
- 7B 모델이 GPT-4V에 필적하는 수준
- **오픈소스 VLM 중 현재 최고 수준** (2025~2026 기준)

```
Qwen2.5-VL 모델 크기: 3B, 7B, 72B
- 3B: 모바일/엣지 배포
- 7B: 범용 고성능
- 72B: 최고 성능, 상업 모델에 근접
```

---

## 5부: Closed Frontier Models

### 5.1 GPT-4V (2023, OpenAI)

**출시**: 2023년 9월

**핵심 혁신**:
- 최초의 **Frontier Multimodal Model** — 일반인도 사용 가능한 고성능 VLM
- **강력한 시각 추론**: 복잡한 차트, 수식, 다이어그램 이해
- 이미지 내 텍스트 읽기(OCR), 공간적 관계 파악
- 멀티턴 대화에서 이미지 참조 유지

**의의**: "AI가 정말로 이미지를 이해한다"는 것을 대중에게 처음으로 인상적으로 보여준 모델. GPT-4V 출시 이후 VLM 연구 붐이 시작됨.

---

### 5.2 GPT-4o (2024, OpenAI)

**출시**: 2024년 5월

**핵심 혁신**:
- **"o" = omni**: 텍스트, 이미지, 오디오를 **하나의 통합 모델**에서 처리
- Native Multimodal: 각 모달리티가 별도 모델이 아닌 단일 Transformer에서 처리
- **실시간 대화**: 음성 입력 → 즉각적인 응답 (이전 GPT-4V보다 훨씬 빠름)
- 이미지 생성도 동일 모델에서 가능 (DALL·E 별도 호출 불필요)
- GPT-4V 대비 향상된 성능과 훨씬 낮은 응답 지연(Latency)

---

### 5.3 Gemini 1.0 Ultra (2024, Google DeepMind)

**출시**: 2024년 2월

**핵심 혁신**:
- **처음부터 Natively Multimodal**: 텍스트, 이미지, 오디오, 비디오를 처음부터 함께 학습
- 기존 VLM처럼 텍스트 LLM에 Vision을 추가한 것이 아님
- MMMU, HumanEval, GSM8K 등 다양한 벤치마크에서 GPT-4V를 초과

**구조 철학**: "각 모달리티를 별도로 처리하고 나중에 합치는 것보다, 처음부터 함께 학습하는 것이 더 강력하다"

---

### 5.4 Gemini 1.5 Pro (2024, Google DeepMind)

**출시**: 2024년 2월 (공개 접근: 2024년 5월)

**핵심 혁신**:
- **1M Token Context Window**: 비디오, 긴 문서, 대규모 코드베이스 처리 가능
- **Long Video Understanding**: 한 시간 이상의 비디오에서 특정 순간 정확히 찾기
- Mixture of Experts(MoE) 아키텍처로 효율성 달성
- 오디오 이해, 자막 없는 비디오 이해

```
Gemini 1.5 Pro가 할 수 있는 것:
- 1시간 영화를 보고 "10분 12초의 씬에 대해 설명해주세요"에 정확히 답변
- 1000페이지 PDF 문서를 한 번에 분석
- 하나의 대화에서 수십 개의 이미지 참조
```

---

### 5.5 Gemini 2.0 Flash / Pro (2025, Google DeepMind)

**출시**: 2025년 초

**핵심 혁신**:
- **Agentic Capabilities**: 웹 브라우징, 코드 실행, 외부 API 호출 등 에이전트 태스크
- **실시간 스트리밍**: 카메라 피드를 실시간으로 분석하며 대화
- 이미지 생성, 음성 합성을 단일 모델에서
- **Gemini 2.0 Flash**: 속도와 비용 효율에 최적화, 실시간 애플리케이션용
- **Gemini 2.0 Pro**: 최고 성능, 복잡한 추론 태스크

---

### 5.6 Claude 3 / 3.5 / 3.7 (Anthropic)

**Claude 3 Family (2024)**:
- Haiku (소), Sonnet (중), Opus (대) 세 가지 버전
- Vision + Text: 이미지와 텍스트 혼합 입력 처리
- **강력한 문서 이해**: PDF, 스캔 문서, 복잡한 표 정확히 파싱
- 안전성(Safety) 최우선 설계

**Claude 3.5 Family (2024~2025)**:
- Claude 3.5 Sonnet: 비용 효율 + 고성능의 최적 균형
- Claude 3.5 Haiku: 빠르고 저렴한 일상 태스크
- **Computer Use**: 스크린샷을 보고 마우스/키보드 조작 → GUI 에이전트 능력
- Artifacts: 코드, 문서를 시각적으로 생성하고 실행

**Claude 3.7 Sonnet (2025)**:
- **Extended Thinking**: 복잡한 멀티모달 추론 태스크에서 더 오래 "생각"
- 이미지가 포함된 수학, 과학 문제에서 강력한 성능
- Vision + 추론 능력의 결합

**Anthropic의 차별점**:
- Constitutional AI로 안전하고 유용한 응답 중심 설계
- 문서 분석, 코딩 지원에서 특히 강점
- Enterprise 환경에서 많이 사용

---

## 6부: 2025년의 Open-Source Models

### 6.1 LLaMA-3.2 Vision (2024년 9월, Meta)

**Meta의 첫 번째 공개 Vision-Language Model**

**모델 크기**: 11B, 90B

**핵심 혁신**:
- LLaMA-3.1 (텍스트) 기반에 Vision 모듈 추가
- **Cross-attention 기반 Vision 통합**: Flamingo와 유사하게 LLM 레이어 사이에 Cross-attention 삽입
- 동결된 LLM 파라미터 + 학습된 Cross-attention 레이어
- 최대 128K 컨텍스트 지원

**의의**: Meta가 처음으로 고품질 VLM을 완전 공개(Weights 공개), 오픈소스 생태계에 큰 기여.

---

### 6.2 Gemma 3 Vision (2025, Google)

**핵심 혁신**:
- **Google의 오픈 멀티모달 모델**: Gemma 계열의 최초 Vision 지원 버전
- SigLIP 기반 Vision Encoder 사용
- 다양한 크기: 1B, 4B, 12B, 27B
- **Natively Multimodal**: 처음부터 텍스트와 이미지를 함께 처리하도록 설계
- 128K 컨텍스트 지원
- 5개 언어 멀티링구얼 지원
- 상업적 사용 허용 라이선스

**특징**: 비교적 소규모 모델(4B)도 강력한 시각 이해 능력을 보유, 온디바이스 배포에 적합.

---

### 6.3 Qwen2.5-VL (2025, Alibaba)

(4.2절에서 자세히 다룸 — 여기서는 추가 세부사항)

**Agentic Visual Tasks의 핵심 능력**:

```
GUI Agent 시나리오:
사용자: "내 구글 캘린더에서 다음 주 화요일 오전 10시에 '팀 미팅'을 추가해줘"

Qwen2.5-VL 처리:
1. 스크린샷 캡처
2. 캘린더 앱의 "+" 버튼 위치 확인
3. 클릭 → 새 이벤트 생성
4. 날짜/시간 필드에 데이터 입력
5. 저장 버튼 클릭
```

**Video Understanding의 특성**:
- 최대 수 시간 분량의 비디오를 이해
- 특정 이벤트의 시작/종료 시간을 정확히 특정
- 비디오 내 텍스트(자막, 간판) 인식

---

### 6.4 InternVL3 (2025, Shanghai AI Lab)

(4.1절에서 언급 — 추가 세부사항)

**Jointly Trained Vision Encoder의 의미**:

기존 VLM의 일반적인 방법:
```
Step 1: CLIP/SigLIP으로 Visual Encoder를 별도 학습
Step 2: LLM과 연결해 VLM 학습
→ 두 단계가 분리되어 최적화 불일치 발생
```

InternVL3의 방법:
```
처음부터 Vision Encoder + LLM을 함께 학습
→ Vision Encoder가 VLM 태스크에 최적화된 표현 학습
→ 더 나은 Vision-Language Alignment
```

**성능**: 2025년 기준 오픈소스 VLM 최고 수준 (MMMU, DocVQA, MathVista 등에서 1위).

---

### 6.5 Mistral의 Multimodal 노력

**Pixtral 12B (2024)**:
- Mistral이 발표한 첫 멀티모달 모델
- 12B 파라미터
- Native resolution 처리 지원
- Apache 2.0 라이선스 (완전 상업적 사용 가능)

**Pixtral Large (2024)**:
- 124B 파라미터
- Mistral Large 2 기반
- 강력한 멀티모달 추론 능력

**Mistral의 접근법**: 상업적으로 완전히 자유로운 라이선스로 기업 채택을 적극 유도.

---

## 7부: 시대별 핵심 트렌드 정리

### 2021: Foundation 확립
- CLIP, ALIGN이 "웹 스케일 대조 학습"의 패러다임을 확립
- Vision과 Language를 동일한 임베딩 공간에 투영

### 2022: 모듈형 VLM
- Flamingo: Perceiver Resampler + Gated Cross-Attention
- BLIP: Bootstrapped 데이터 필터링
- "사전학습된 LLM + Vision 모듈" 조합의 시작

### 2023: 오픈소스 폭발
- LLaVA: 단순한 구조 + Visual Instruction Tuning
- BLIP-2: Q-Former + 모듈형 설계
- GPT-4V: Closed Model이 기준 설정
- SigLIP: 더 나은 Visual Encoder Pretraining

### 2024: 고해상도 + 강력한 LLM
- LLaVA-NeXT/OneVision: AnyRes 고해상도
- Qwen2-VL: M-RoPE Native Resolution
- InternVL 2/2.5: 강력한 OCR, 다국어
- GPT-4o: Native Multimodal
- Gemini 1.5 Pro: 1M Context Video Understanding

### 2025~2026: 에이전트 + 통합 학습
- Qwen2.5-VL: Agentic Visual Tasks
- InternVL3: Jointly Trained Encoder
- Gemma 3: 오픈 멀티모달
- Gemini 2.0: Real-time Multimodal Agent
- Claude 3.7: Extended Thinking + Vision

---

## 8부: 아키텍처 접근 방식 비교

| 모델 | Visual Encoder | Connector | LLM Base | 특이점 |
|------|---------------|-----------|----------|--------|
| **LLaVA** | CLIP ViT-L/14 | Linear | Vicuna | 극도로 단순한 구조 |
| **LLaVA-1.5** | CLIP ViT-L/14 | MLP | LLaMA-2 | MLP + 더 나은 데이터 |
| **LLaVA-NeXT** | CLIP/SigLIP | MLP + AnyRes | LLaMA-3/Mistral | 고해상도 타일링 |
| **BLIP-2** | EVA-CLIP ViT-G | Q-Former | OPT/FlanT5 | 32 고정 쿼리 |
| **Flamingo** | NFNet | Perceiver Resampler | Chinchilla | Gated Cross-Attention |
| **InternVL 2.5** | InternViT-6B | MLP | InternLM2 | 자체 대형 ViT |
| **Qwen2-VL** | Custom ViT | M-RoPE | Qwen2 | Native Resolution |
| **LLaMA-3.2 V** | Custom ViT | Cross-Attention | LLaMA-3.1 | 동결 LLM + CA 삽입 |
| **Gemma 3 V** | SigLIP ViT | - | Gemma 3 | Google 오픈소스 |
| **GPT-4o** | 미공개 | 미공개 | 미공개 | Native Multimodal |
| **Gemini 2.0** | 미공개 | 미공개 | 미공개 | Agentic, 실시간 |

---

## 핵심 용어 정리

| 용어 | 설명 |
|------|------|
| **CLIP** | OpenAI의 대조적 이미지-언어 사전학습 모델 |
| **SigLIP** | Google의 Sigmoid Loss 기반 CLIP 변형 |
| **ALIGN** | Google의 대규모 Noisy 데이터 기반 대조 학습 모델 |
| **Flamingo** | DeepMind의 Few-shot Multimodal 모델 |
| **BLIP-2** | Q-Former 기반 모듈형 VLM |
| **LLaVA** | Linear/MLP Projector + Visual Instruction Tuning |
| **AnyRes** | 이미지를 타일로 분할하는 고해상도 처리 전략 |
| **InternVL** | Shanghai AI Lab의 자체 Vision Encoder 기반 VLM 계열 |
| **Qwen2-VL** | Alibaba의 M-RoPE Native Resolution VLM |
| **M-RoPE** | Multimodal Rotary Position Embedding, 모달리티별 위치 인코딩 |
| **GPT-4V** | OpenAI의 첫 Frontier VLM |
| **GPT-4o** | OpenAI의 Native Multimodal 통합 모델 |
| **Gemini** | Google의 Native Multimodal 모델 계열 |
| **LLaMA-3.2 Vision** | Meta의 첫 공개 VLM |
| **Gemma 3 Vision** | Google의 오픈소스 멀티모달 모델 |
| **Pixtral** | Mistral의 멀티모달 모델 계열 |
| **Claude 3.7** | Anthropic의 Extended Thinking 멀티모달 모델 |
| **Agentic VLM** | 이미지/스크린을 보고 능동적으로 행동하는 VLM |
| **Jointly Trained** | Vision Encoder와 LLM을 처음부터 함께 학습하는 방식 |
| **Native Multimodal** | 텍스트-비전-오디오를 단일 모델에서 처음부터 통합 학습 |
