# 8.2 VLM Architecture: 비전과 언어를 연결하는 방법

Vision Encoder가 이미지를 벡터로 변환했다면, 이제 그 벡터를 Language Model이 이해할 수 있는 형태로 연결해야 함. 이 연결 방식(Connector/Projector)의 선택이 VLM의 성능, 효율성, 그리고 능력에 큰 영향을 미침. 다양한 연결 방식과 융합(Fusion) 전략에 대한 방법들.

---

## 1. Vision Encoder + LLM 패러다임

### 1.1 표준 Connector 아키텍처

현재 VLM의 주류 아키텍처는 다음과 같이 구성됨:

```
[Vision Encoder]       [Connector/Projector]    [LLM]
이미지 → ViT/CNN → 시각 특징 → 변환/매핑 → 언어 모델 토큰 공간
```

각 구성 요소의 역할:
- **Vision Encoder**: 이미지를 고차원 특징 벡터(시퀀스)로 변환 (8.1절 참조)
- **Connector**: 시각 특징을 LLM이 처리할 수 있는 형태로 변환 (이 섹션의 핵심)
- **LLM**: 텍스트와 시각 정보를 함께 처리해 응답 생성

이 아키텍처의 핵심 장점은 **모듈성(Modularity)**. 각 구성 요소를 독립적으로 업그레이드하거나 교체할 수 있음.

### 1.2 Vision Encoder의 동결/파인튜닝 선택

Vision Encoder를 어떻게 처리할지는 중요한 설계 결정:

**Freeze (동결) Vision Encoder**
- CLIP/SigLIP으로 사전학습된 표현을 그대로 유지
- 컴퓨팅 비용 절감
- Overfitting 방지
- 단점: Vision Encoder가 VLM 태스크에 최적화되지 않을 수 있음

**Finetune (파인튜닝) Vision Encoder**
- End-to-end 학습으로 VLM 태스크에 더 잘 맞는 표현 학습
- 더 많은 컴퓨팅 필요
- 주의: 학습률을 낮게 유지하지 않으면 사전학습 표현이 망가질 수 있음 (Catastrophic Forgetting, 8.4절 참조)

**현재 추세**: 초기 학습 단계에서는 동결, 후기 단계에서는 낮은 학습률로 파인튜닝하는 **2단계 전략**이 일반적.

### 1.3 LLM의 동결/파인튜닝 선택

| 전략 | 방법 | 장점 | 단점 |
|------|------|------|------|
| Freeze LLM | Connector만 학습 | 빠름, LLM 능력 보존 | 시각 정보를 LLM에 완전히 통합 어려움 |
| Finetune LLM | LLM도 함께 학습 | 더 나은 멀티모달 이해 | 많은 컴퓨팅, 텍스트 능력 저하 위험 |
| LoRA/PEFT | 일부 파라미터만 학습 | 효율적, 좋은 균형 | 복잡한 하이퍼파라미터 튜닝 |

---

## 2. Linear Projector: 가장 단순한 연결

### 2.1 구조

Linear Projector는 Vision Encoder의 출력과 LLM 입력 공간 사이에 단순한 선형 변환을 적용함:

```
Visual Feature: (N_patches, D_vision) → Linear Layer → (N_patches, D_llm)

예시:
- Vision Encoder 출력 차원: D_vision = 1024 (ViT-L)
- LLM 임베딩 차원: D_llm = 4096 (LLaMA-7B)
- Linear Layer: W ∈ R^{1024 × 4096}
```

```python
# 구현 예시 (PyTorch)
class LinearProjector(nn.Module):
    def __init__(self, vision_dim, llm_dim):
        super().__init__()
        self.proj = nn.Linear(vision_dim, llm_dim)

    def forward(self, visual_features):
        # visual_features: (batch, num_patches, vision_dim)
        return self.proj(visual_features)
        # 출력: (batch, num_patches, llm_dim)
        # 이 출력이 텍스트 토큰 임베딩과 같은 차원이 됨
```

### 2.2 장점과 단점

**장점**
- 극도로 단순한 구조 → 파라미터 수 최소화
- 빠른 학습과 추론
- 시각 토큰의 수가 변하지 않음 (N_patches 유지) → 공간 정보 완전 보존

**단점**
- 표현력이 제한적 (비선형 변환 없음)
- Vision Encoder 공간과 LLM 공간 간의 복잡한 매핑을 학습하기 어려울 수 있음

### 2.3 사용 사례: LLaVA 초기 버전

2023년 발표된 LLaVA (Large Language and Vision Assistant)의 첫 번째 버전은 이 단순한 Linear Projector를 사용해 놀라운 성능을 보였음. "단순함이 때로는 최선"이라는 것을 증명한 사례.

---

## 3. MLP Projector: 비선형성 추가

### 3.1 구조

MLP Projector는 Linear Projector에 비선형 활성화 함수와 추가 레이어를 더해 표현력을 높임:

```
Visual Feature → Linear → Activation → Linear → LLM Token Space

구체적으로 (LLaVA-1.5):
(N_patches, D_vision) → Linear → GELU → Linear → (N_patches, D_llm)
```

```python
# 2-layer MLP Projector (LLaVA-1.5 스타일)
class MLPProjector(nn.Module):
    def __init__(self, vision_dim, llm_dim):
        super().__init__()
        self.proj = nn.Sequential(
            nn.Linear(vision_dim, llm_dim),
            nn.GELU(),
            nn.Linear(llm_dim, llm_dim)
        )

    def forward(self, visual_features):
        # visual_features: (batch, num_patches, vision_dim)
        return self.proj(visual_features)
        # 출력: (batch, num_patches, llm_dim)
```

### 3.2 Linear vs MLP 성능 비교

LLaVA-1.5 논문에서 보고된 결과에 따르면, 2-layer MLP Projector는 Linear Projector보다 다양한 VQA 및 멀티모달 벤치마크에서 일관되게 우수한 성능을 보임. 특히 세밀한 시각 이해가 필요한 태스크에서 차이가 뚜렷함.

**왜 비선형성이 도움될까?**
- Vision Encoder 공간과 LLM 공간 사이의 **비선형 매핑**을 학습 가능
- 더 복잡한 특징 변환을 표현 가능
- 추가 파라미터 수는 적지만 (Linear 대비 약 2배) 성능 향상은 상당함

### 3.3 현재 위치

MLP Projector는 현재 많은 효율적인 VLM의 표준 선택. 단순함, 낮은 계산 비용, 좋은 성능의 균형을 제공함. LLaVA-1.5 이후 MLP Projector가 사실상 기본값이 됨.

---

## 4. Q-Former (Querying Transformer): 고정 크기 시각 표현

### 4.1 아이디어: 학습 가능한 쿼리 토큰

Q-Former는 BLIP-2와 InstructBLIP에서 도입된 더 복잡한 연결 모듈. 핵심 아이디어는 **고정된 수의 학습 가능한 Query Token**이 Image Feature에서 정보를 추출한다는 것임.

```
[학습 가능한 Query Tokens]
  Q_1, Q_2, ..., Q_K  (K = 32, 고정)
       ↓
  Cross-Attention ←── Image Features (N 개, 가변)
       ↓
  K개의 Visual Representation
       ↓
  Linear Projection → LLM Token Space
```

이미지 해상도나 패치 수에 관계없이 항상 **K=32개의 고정된 시각 토큰**만 LLM에 전달됨.

### 4.2 Q-Former가 필요한 이유.
- N은 해상도/백본에 따라 바뀔 수 있음(가변).
- LLM은 입력 길이에 민감.
- “그냥 전부 LLM에 넣기”는 비용/성능 면에서 비효율적.

### 4.3 Q-Former의 내부 구조

```python
# Q-Former 개념적 구현
class QFormer(nn.Module):
    def __init__(self, num_queries=32, d_model=768):
        super().__init__()
        # 학습 가능한 Query 파라미터
        self.query_tokens = nn.Parameter(
            torch.zeros(1, num_queries, d_model)
        )
        # Self-Attention (Query 간)
        self.self_attn = nn.MultiheadAttention(d_model, num_heads=12)
        # Cross-Attention (Query → Image Features)
        self.cross_attn = nn.MultiheadAttention(d_model, num_heads=12)
        # Feed-Forward
        self.ffn = nn.Sequential(
            nn.Linear(d_model, d_model * 4),
            nn.GELU(),
            nn.Linear(d_model * 4, d_model)
        )

    def forward(self, image_features):
        # image_features: (batch, N_patches, d_model)
        batch_size = image_features.shape[0]
        queries = self.query_tokens.expand(batch_size, -1, -1)

        # Query 간 Self-Attention
        queries = self.self_attn(queries, queries, queries)[0]

        # Query → Image Feature Cross-Attention
        queries = self.cross_attn(queries, image_features, image_features)[0]

        # FFN
        queries = self.ffn(queries)

        return queries  # (batch, 32, d_model) - 항상 32개!
```

### 4.4 Q-Former의 장점과 단점

**장점**
- **고정 크기 출력**: 이미지 해상도에 무관하게 항상 32개의 토큰 → LLM의 Context Length 절약
- **정보 압축**: 이미지의 가장 중요한 정보를 선택적으로 추출
- **모듈성**: Vision Encoder를 완전히 동결하고 Q-Former만 학습 가능

**단점**
- **고정 토큰 수**: 복잡하거나 세밀한 이미지에서 정보 손실 가능
- **추가 파라미터**: Q-Former 자체가 학습이 필요한 복잡한 모듈
- **고해상도 부적합**: OCR, 세밀한 문서 이해 등에서 정보 손실이 큼

### 4.5 BLIP-2의 학습 전략

BLIP-2는 Q-Former를 효율적으로 학습하기 위해 3단계 학습을 도입함:

```
Stage 1: Vision-Language Representation Learning
  - Frozen Image Encoder + Q-Former 학습
  - Image-Text Contrastive Learning
  - Image-grounded Text Generation
  - Image-Text Matching

Stage 2: Vision-to-Language Generative Learning
  - Frozen Image Encoder + Frozen LLM + Q-Former + Linear Projection 학습
  - Q-Former의 출력을 Soft Visual Prompt로 LLM에 제공
```

---

## 5. Perceiver Resampler: Q-Former의 연속 버전

### 5.1 Flamingo에서의 사용

Perceiver Resampler는 DeepMind의 Flamingo 모델에서 도입된 연결 모듈. Q-Former와 매우 유사하지만 몇 가지 차이점이 있음:

```
[학습 가능한 Latent Queries]
  (고정 개수, 예: 64개)
       ↓
  Perceiver Resampler Layer ←── Image Features (가변)
  (Cross-Attention + Self-Attention 교대)
       ↓
  LLM Decoder (Gated Cross-Attention 레이어 삽입)
```

### 5.2 Q-Former vs Perceiver Resampler

| 특성 | Q-Former (BLIP-2) | Perceiver Resampler (Flamingo) |
|------|------------------|-------------------------------|
| 사용 위치 | Vision-LLM 연결 모듈 | Vision-LLM 연결 모듈 |
| LLM 수정 | 없음 (토큰으로 삽입) | 있음 (Gated Cross-Attention 추가) |
| Query 학습 | ✓ | ✓ |
| 출력 크기 | 고정 (32개) | 고정 (64개) |
| Few-shot | 제한적 | 우수 (Interleaved 학습) |

**Gated Cross-Attention**: Flamingo에서 LLM의 각 레이어에 시각 정보를 주입하는 메커니즘. 원래 LLM 레이어 사이에 새로운 Cross-Attention 레이어를 삽입하고, Gate 파라미터로 그 영향도를 조절함:

```python
# Gated Cross-Attention
x = x + tanh(self.gate) * self.cross_attn(x, visual_features)
```

처음에 `gate=0`이면 원래 LLM과 동일하게 동작하다가, 학습을 통해 점점 시각 정보를 더 활용함.

---

## 6. Early Fusion vs Late Fusion

### 6.1 Early Fusion (조기 융합)

시각 토큰과 텍스트 토큰을 **초기 단계에서 합쳐** 함께 처리함:

```
이미지 패치 토큰: [V_1, V_2, ..., V_N]
텍스트 토큰:     [T_1, T_2, ..., T_M]
              ↓ 합치기
통합 시퀀스: [V_1, ..., V_N, T_1, ..., T_M]  또는 인터리빙
              ↓
Transformer (모든 레이어에서 Vision-Text 상호작용)
```

**특성**
- 이미지와 텍스트가 모든 Transformer 레이어에서 서로 Attend 가능
- **Cross-modal 상호작용이 풍부**
- 현재 대부분의 VLM(LLaVA, InternVL 등)이 이 방식 사용
- LLM의 Self-Attention이 시각 토큰과 텍스트 토큰을 동시에 처리

**장점**: 깊은 수준에서의 시각-언어 통합, 강력한 멀티모달 이해
**단점**: 시각 토큰이 많으면 LLM의 Sequence Length가 길어져 KV-Cache 메모리 증가

### 6.2 Late Fusion (후기 융합)

각 모달리티를 **독립적으로 처리**한 뒤 최종 단계에서 결합함:

```
이미지 → [Vision Encoder] → 이미지 표현 ─────────┐
                                                  ↓
텍스트 → [Text Encoder]   → 텍스트 표현 → [Fusion] → 최종 출력
```

**특성**
- 각 모달리티가 독립적으로 깊이 처리됨
- 처리 파이프라인의 **유연성**이 높음
- Cross-modal 상호작용은 Fusion 단계에서만 발생
- 검색(Retrieval) 시스템에서 효과적 (각 모달리티를 별도로 인덱싱 가능)

**장점**: 효율적인 독립 처리, 유연한 모달리티 결합 방법 선택
**단점**: 깊은 수준의 Cross-modal 상호작용 부족, VQA 등 세밀한 이해 태스크에서 불리

### 6.3 Intermediate Fusion (중간 융합)

실제로는 Early와 Late 사이 어딘가에서 융합하는 경우도 많음:

- Flamingo: LLM의 특정 레이어들에 Gated Cross-Attention을 삽입 → 중간 융합
- Q-Former: 시각 정보를 압축한 뒤 LLM에 입력 → 어느 정도 중간 융합

---

## 7. Unified Multimodal Transformer: 진정한 통합

### 7.1 개념

Unified Multimodal Transformer는 처음부터 텍스트와 시각 토큰을 **동일한 Transformer로 함께 처리**함. 별도의 Vision Encoder나 Connector 없이, 단일 Transformer가 모든 모달리티를 처리함.

```
이미지 패치 → [Patch Embedding] ──────┐
텍스트 →      [Token Embedding] ──────┤
                                      ↓
          [단일 통합 Transformer]
          (텍스트 + 시각 토큰을 함께 처리)
                                      ↓
                              다음 토큰 예측
                              (텍스트 또는 이미지)
```

### 7.2 Positional Encoding 설계의 중요성

Unified Transformer에서 가장 큰 도전은 서로 다른 모달리티에 대한 위치 정보 인코딩:

**텍스트 토큰**: 1D 시퀀스 순서가 의미있음
**이미지 패치**: 2D 공간 위치가 의미있음

이를 동시에 처리하기 위한 접근법:
- **1D 위치 임베딩 통합**: 이미지 패치를 Raster Scan으로 1D 시퀀스로 취급, 텍스트와 동일한 방식으로 처리
- **2D RoPE**: 이미지 패치에는 2D 위치(행, 열), 텍스트에는 1D 위치를 Rotary Embedding으로 인코딩 (Qwen2-VL의 M-RoPE 접근)
- **모달리티 임베딩**: 위치 임베딩에 더해 각 토큰이 텍스트인지 이미지인지를 나타내는 임베딩 추가

### 7.3 장점과 현재 상황

**장점**
- 설계가 매우 단순하고 우아함
- 모달리티 간 가장 자연스러운 상호작용
- Next Token Prediction으로 이미지 생성도 가능 (멀티모달 생성)

**단점**
- 처음부터 학습해야 함 → 기존 사전학습된 LLM을 재활용하기 어려움
- 대규모 멀티모달 데이터가 필요
- 이미지 토큰화(Tokenization) 방식을 별도로 설계해야 함

**현재 동향**: Unified 접근은 GPT-4o, Gemini와 같은 최첨단 상업용 모델들이 지향하는 방향. 오픈소스에서는 아직 Connector 기반이 주류이지만, Unified 방식으로의 이행이 진행 중.

---

## 8. Connector 방식 종합 비교

| Connector 유형 | 출력 토큰 수 | 표현력 | 계산 비용 | 대표 모델 |
|----------------|------------|--------|----------|----------|
| **Linear Projector** | N_patches (가변) | 낮음 | 최소 | LLaVA v1 |
| **MLP Projector** | N_patches (가변) | 중간 | 낮음 | LLaVA-1.5 |
| **Q-Former** | 고정 (32) | 높음 | 중간 | BLIP-2, InstructBLIP |
| **Perceiver Resampler** | 고정 (64) | 높음 | 중간 | Flamingo |
| **Unified Transformer** | N/A | 최고 | 높음 | GPT-4o, Gemini |

---

## 9. 전체 아키텍처 흐름

현재 가장 일반적인 VLM 아키텍처(LLaVA 계열)의 전체 흐름:

```
사용자 입력
  ├── 이미지
  │     ↓
  │   [Vision Encoder (ViT, 동결 또는 파인튜닝)]
  │     ↓
  │   시각 특징: (N_patches, D_vision)
  │     ↓
  │   [MLP Projector (학습)]
  │     ↓
  │   시각 토큰: (N_patches, D_llm)
  │     ↓
  └── 텍스트 프롬프트
        ↓
      [Token Embedding]
        ↓
      텍스트 토큰: (N_text, D_llm)
        ↓
  [시각 토큰 + 텍스트 토큰 연결]
        ↓
  [LLM (Causal Language Model)]
  (모든 토큰이 서로 Attend, Self-Attention)
        ↓
  응답 텍스트 생성 (자기회귀)
```

이 구조에서 LLM은 시각 토큰을 "그림 단어"처럼 처리함. Projector가 시각 특징을 LLM의 Token Embedding 공간으로 매핑했기 때문에, LLM 입장에서 시각 토큰과 텍스트 토큰의 형태가 동일함.

---

## 핵심 용어 정리

| 용어 | 설명 |
|------|------|
| **Connector / Projector** | Vision Encoder와 LLM을 연결하는 모듈 |
| **Linear Projector** | 단일 선형 변환으로 시각 특징을 LLM 공간으로 매핑 |
| **MLP Projector** | 비선형 활성화를 포함한 다층 퍼셉트론 변환 |
| **Q-Former** | 학습 가능한 Query Token으로 시각 특징에서 고정 크기 표현 추출 |
| **Perceiver Resampler** | Flamingo의 연결 모듈, Q-Former와 유사하나 Gated Cross-Attention 사용 |
| **Gated Cross-Attention** | LLM 레이어에 삽입된 시각 정보 주입 메커니즘 |
| **Early Fusion** | 초기 단계에서 시각-텍스트 토큰 통합 |
| **Late Fusion** | 모달리티를 독립 처리 후 최종 단계에서 결합 |
| **Unified Multimodal Transformer** | 단일 Transformer로 모든 모달리티를 처음부터 처리 |
| **Freeze** | 파라미터를 고정해 학습되지 않게 함 |
| **Finetune** | 사전학습된 파라미터를 추가로 학습 |
| **Modality** | 텍스트, 이미지, 오디오 등 서로 다른 입력 형태 |
