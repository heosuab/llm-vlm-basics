# 8.1 Vision Encoder: 이미지를 토큰으로 만드는 방법

Vision-Language Model(VLM)의 출발점은 **이미지를 언어 모델이 이해할 수 있는 형태로 변환**하는 것. 이 역할을 담당하는 것이 바로 Vision Encoder. 두 가지 주요 아키텍처(CNN과 ViT), 대표적인 사전학습 방법(CLIP, SigLIP), 그리고 핵심 메커니즘인 Patch Embedding과 Contrastive Pretraining.

---

## 1. CNN vs ViT: 두 가지 패러다임

### 1.1 CNN (Convolutional Neural Network)

CNN은 이미지 인식 분야에서 오랫동안 지배적인 위치를 차지해온 아키텍처. 핵심 아이디어는 **Local Convolutional Filter**를 사용해 이미지의 지역적 패턴을 추출하는 것.

**핵심 특성: Translation Equivariance (이동 등변성)**

이동 등변성이란, 입력 이미지에서 물체의 위치가 바뀌면 출력 Feature Map에서도 같은 방향으로 위치가 바뀐다는 성질. 이는 같은 필터(커널)가 이미지 전체를 슬라이딩하며 적용되기 때문.

```
입력 이미지 (예: 고양이가 왼쪽에 있음)
    → Conv Layer → Feature Map (왼쪽에 반응)

입력 이미지 (예: 고양이가 오른쪽으로 이동)
    → Conv Layer → Feature Map (오른쪽에 반응)
```

**Inductive Bias (귀납적 편향)**

CNN은 두 가지 중요한 귀납적 편향을 내장하고 있음:

1. **Locality (지역성)**: 이미지의 의미있는 패턴은 인접 픽셀 간의 관계에서 나옴. → 작은 커널(3×3, 5×5)로도 충분히 표현 가능
2. **Translation Equivariance**: 동일한 패턴은 위치에 관계없이 같은 방식으로 인식되어야 함.

이러한 귀납적 편향 덕분에 CNN은 **비교적 적은 데이터로도 좋은 성능**을 낼 수 있음.

**CNN의 한계**

- 픽셀 간의 **장거리 의존성(Long-range Dependency)** 포착이 어려움. 이미지 왼쪽 끝과 오른쪽 끝의 관계를 파악하려면 매우 깊은 네트워크가 필요함.
- 아키텍처 설계에 많은 수작업(Hand-crafting)이 필요함 (Pooling 위치, Stride, Padding 등).

---

### 1.2 ViT (Vision Transformer)

2020년 Google이 발표한 ViT는 NLP에서 성공을 거둔 Transformer를 이미지에 그대로 적용하는 아이디어. 핵심은 이미지를 **Patch의 시퀀스**로 취급하는 것.

**처리 과정**

```
원본 이미지 (예: 224×224)
    ↓
이미지를 16×16 패치로 분할 → 14×14 = 196개의 패치
    ↓
각 패치를 Flatten → Linear Projection → d_model 차원의 벡터
    ↓
[CLS] 토큰 + 196개 패치 토큰 + Positional Embedding
    ↓
Transformer Encoder (Multi-Head Self-Attention)
    ↓
[CLS] 토큰의 출력 → 이미지 전체 표현
```

**Global Self-Attention의 힘**

ViT의 Self-Attention은 **모든 패치 간의 관계를 직접 계산**함. 즉, 이미지의 어느 두 부분이든 하나의 Attention 연산으로 관계를 파악할 수 있음. CNN이 여러 레이어를 쌓아야 간신히 포착하던 장거리 의존성을 ViT는 첫 번째 레이어에서부터 볼 수 있음.

**ViT가 CNN보다 더 잘 Scale되는 이유**

| 특성 | CNN | ViT |
|------|-----|-----|
| Inductive Bias | 강함 (Locality, Equivariance) | 약함 (거의 없음) |
| 소규모 데이터 성능 | 좋음 | 나쁨 |
| 대규모 데이터 성능 | 좋음 | 매우 좋음 (CNN 초과) |
| Scalability | 제한적 | 우수 (모델 크기 ↑ = 성능 ↑) |
| Long-range Dependency | 어려움 | 쉬움 |

ViT는 귀납적 편향이 적기 때문에, **데이터로부터 직접 이미지의 구조를 학습**해야 함. 이는 데이터가 적을 때는 단점이지만, 대규모 데이터셋(JFT-300M 등)으로 학습하면 CNN을 능가하는 성능을 보임.

---

### 1.3 Hybrid Approaches

CNN과 ViT의 장점을 결합하려는 시도도 많음:

- **CNN + ViT Hybrid**: CNN으로 초기 Feature Map을 추출한 뒤, 이를 Patch로 취급해 ViT에 입력. CNN의 Locality Bias + ViT의 Global Attention을 모두 활용
- **ConvNeXt**: CNN 구조를 유지하지만 ViT의 학습 방법(Layer Norm, GELU 등)을 도입해 성능 향상
- **Swin Transformer**: 로컬 윈도우 내에서만 Attention을 계산한 뒤 윈도우를 이동(Shift)하는 방식으로 효율성 향상

VLM에서는 대부분 **순수 ViT 기반의 Vision Encoder**를 사용하는 추세. 특히 CLIP으로 사전학습된 ViT-L/14 또는 ViT-H/14가 표준으로 자리잡았음.

---

## 2. Patch Embedding: 이미지를 토큰으로 분해하기

Patch Embedding은 ViT의 핵심 입력 처리 단계. 이미지를 NLP의 단어 토큰과 유사한 형태로 변환함.

### 2.1 기본 과정

**Step 1: 이미지를 패치로 분할**

이미지를 고정 크기의 패치로 나눔. 예를 들어 224×224 이미지를 16×16 패치로 나누면:

```
이미지 크기: 224 × 224 픽셀
패치 크기: 16 × 16 픽셀
패치 수: (224/16) × (224/16) = 14 × 14 = 196개
각 패치의 차원: 16 × 16 × 3(RGB) = 768
```

일반적으로 사용되는 패치 크기:
- **16×16**: ViT-B/16, ViT-L/16 — 빠르고 좋은 균형
- **14×14**: ViT-L/14, ViT-H/14 — CLIP에서 주로 사용, 더 세밀한 표현

**Step 2: Flatten + Linear Projection**

각 패치를 1차원 벡터로 펼치고(Flatten), 학습 가능한 Linear Layer로 모델 차원(d_model)에 맞게 투영함:

```python
# 의사 코드 (PyTorch 스타일)
# 입력: 이미지 (B, C, H, W) = (배치, 채널, 높이, 너비)
patch_size = 16
num_patches = (H // patch_size) * (W // patch_size)  # 196

# Conv2d로 구현하는 것이 실제 코드에서 일반적
# kernel_size=patch_size, stride=patch_size이면 겹치지 않는 패치 추출과 동일
patch_embed = nn.Conv2d(
    in_channels=3,
    out_channels=d_model,  # 예: 768 (ViT-Base)
    kernel_size=patch_size,
    stride=patch_size
)
# 출력: (B, d_model, H/patch_size, W/patch_size)
# Flatten: (B, num_patches, d_model)
```

**Step 3: [CLS] 토큰 추가**

NLP의 BERT에서 영감을 받아, 시퀀스의 맨 앞에 학습 가능한 `[CLS]` 토큰을 추가함:

```
시퀀스: [CLS, Patch_1, Patch_2, ..., Patch_196]
길이: 1 + 196 = 197개 토큰
```

Transformer를 거친 후 `[CLS]` 토큰의 출력이 이미지 전체를 대표하는 벡터로 사용됨. 이 벡터가 CLIP에서 이미지 임베딩이 됨.

> 참고: 일부 모델(예: SigLIP)은 [CLS] 토큰 없이 모든 패치 토큰을 평균(Global Average Pooling)하여 이미지 표현을 만들기도 함.

**Step 4: Positional Embedding 추가**

Self-Attention은 순서 정보가 없으므로, 각 토큰에 위치 정보를 더해줌:

```python
# 학습 가능한 1D Positional Embedding (가장 일반적)
pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, d_model))
# +1은 [CLS] 토큰 위치

x = patch_embeddings + pos_embed  # 위치 정보 주입
```

1D vs 2D Positional Embedding:
- **1D**: 패치를 왼쪽 위부터 래스터 스캔 순서로 번호를 매김. 단순하지만 이미지의 2D 구조를 완전히 반영하지 못함
- **2D**: 행(row)과 열(column) 위치를 각각 임베딩해 더하거나 합침. 더 자연스럽지만 복잡성 증가
- **RoPE (Rotary Position Embedding)**: 2D 확장이 자연스럽고, Qwen2-VL 등에서 사용 (8.5절 참조)

---

## 3. CLIP: Contrastive Language-Image Pretraining

2021년 OpenAI가 발표한 CLIP은 VLM 분야의 패러다임을 바꾼 모델. 이미지와 텍스트를 **공유 임베딩 공간(Shared Embedding Space)**에 매핑하는 방법을 대규모 웹 데이터로 학습함.

### 3.1 Dual Encoder 구조

CLIP은 두 개의 독립적인 인코더로 구성됨:

```
이미지 → [Image Encoder (ViT 또는 ResNet)] → image_embedding (d차원)
텍스트 → [Text Encoder (Transformer)]      → text_embedding  (d차원)
```

두 임베딩을 같은 차원의 공간으로 매핑한 뒤, **코사인 유사도**로 이미지-텍스트 쌍의 연관성을 측정함.

학습이 끝나면:
- 동일한 개념의 이미지와 텍스트는 임베딩 공간에서 **가까이** 위치
- 서로 다른 개념은 **멀리** 위치

### 3.2 대규모 Noisy Web 데이터

CLIP은 인터넷에서 수집한 **4억 개(400M)의 이미지-텍스트 쌍**으로 학습됨. 이 데이터는 큐레이션되지 않은 "잡음(Noisy)" 데이터. 예를 들어:

- 이미지: 어떤 개의 사진
- 텍스트: 해당 웹페이지의 alt 텍스트 → "My dog Max playing in the park" (관련 있음)
- 또는: "Click here for more cute animal photos" (느슨하게 관련)

이런 잡음 많은 데이터를 대량으로 사용해도, Contrastive Loss를 통해 의미있는 표현을 학습할 수 있다는 것이 CLIP의 핵심 발견.

### 3.3 CLIP Embedding Space의 특성

잘 학습된 CLIP 임베딩 공간은 강력한 특성을 가짐:

- **Zero-shot Transfer**: 한 번도 보지 못한 카테고리도 텍스트 프롬프트만으로 분류 가능
- **Semantic Similarity**: 의미적으로 유사한 이미지와 텍스트는 유사한 위치에 존재
- **Compositionality (부분적)**: "red car"와 "blue car"를 어느 정도 구분 가능 (완벽하지는 않음)

### 3.4 Zero-shot Classification

CLIP의 가장 혁신적인 능력 중 하나는 **Zero-shot Classification**. 별도의 파인튜닝 없이 텍스트 프롬프트만으로 이미지를 분류할 수 있음.

```
분류 방법:
1. 각 클래스를 텍스트 프롬프트로 변환:
   "a photo of a {class_name}"
   예: "a photo of a cat", "a photo of a dog", "a photo of a car"

2. 모든 텍스트 프롬프트를 Text Encoder로 임베딩

3. 입력 이미지를 Image Encoder로 임베딩

4. 이미지 임베딩과 각 텍스트 임베딩 간의 코사인 유사도 계산

5. 가장 높은 유사도의 클래스 선택
```

프롬프트 엔지니어링도 중요함. 단순히 클래스명("cat")보다 "a photo of a cat"처럼 문맥을 추가하면 성능이 향상됨. 앙상블 기법으로 여러 프롬프트를 사용하기도 함:

```
"a photo of a {class_name}"
"a blurry photo of a {class_name}"
"a photo of many {class_name}"
"a photo of the large {class_name}"
→ 이 모든 텍스트 임베딩을 평균내어 사용
```

---

## 4. Contrastive Pretraining: CLIP의 학습 메커니즘

### 4.1 Positive Pairs vs Negative Pairs

Contrastive Learning의 핵심은 **매칭되는 쌍과 매칭되지 않는 쌍을 구분**하는 것:

- **Positive Pair**: 같은 데이터 포인트에서 나온 이미지-텍스트 쌍 (실제로 연관된 쌍)
- **Negative Pair**: 서로 다른 데이터 포인트에서 나온 이미지-텍스트 쌍 (연관 없는 쌍)

미니배치 크기가 N이면, N개의 Positive Pair와 N²-N개의 Negative Pair가 생성됨:

```
배치 내 이미지: I_1, I_2, ..., I_N
배치 내 텍스트: T_1, T_2, ..., T_N

Positive Pairs: (I_1, T_1), (I_2, T_2), ..., (I_N, T_N) → N개
Negative Pairs: (I_1, T_2), (I_1, T_3), ..., (I_N, T_{N-1}) → N²-N개
```

이를 N×N 유사도 행렬로 시각화할 수 있음:

```
        T_1   T_2   T_3  ...  T_N
I_1  [ [+]   [-]   [-]  ...  [-] ]
I_2  [ [-]   [+]   [-]  ...  [-] ]
I_3  [ [-]   [-]   [+]  ...  [-] ]
...
I_N  [ [-]   [-]   [-]  ...  [+] ]

[+] = Positive (높은 유사도 목표)
[-] = Negative (낮은 유사도 목표)
```

### 4.2 InfoNCE Loss

CLIP은 **InfoNCE (Noise Contrastive Estimation) Loss**를 사용함. 이는 각 이미지에 대해 올바른 텍스트를 찾는 분류 문제(및 그 역방향)로 정의됨:

$$L_{i \rightarrow t} = -\frac{1}{N} \sum_{i=1}^{N} \log \frac{\exp(\text{sim}(v_i, t_i) / \tau)}{\sum_{j=1}^{N} \exp(\text{sim}(v_i, t_j) / \tau)}$$

$$L_{t \rightarrow i} = -\frac{1}{N} \sum_{i=1}^{N} \log \frac{\exp(\text{sim}(t_i, v_i) / \tau)}{\sum_{j=1}^{N} \exp(\text{sim}(t_i, v_j) / \tau)}$$

$$L_{CLIP} = \frac{L_{i \rightarrow t} + L_{t \rightarrow i}}{2}$$

여기서:
- $v_i$: i번째 이미지의 임베딩 (L2 정규화된)
- $t_i$: i번째 텍스트의 임베딩 (L2 정규화된)
- $\text{sim}(u, v) = u \cdot v$ (정규화되었으므로 코사인 유사도)
- $\tau$: Temperature Parameter (학습 가능한 스칼라)

직관적으로, 이 Loss는 다음을 의미함:
- 이미지 $v_i$가 주어졌을 때, 배치 내 모든 텍스트 중 $t_i$가 가장 유사해야 함 (Softmax의 분자를 최대화)
- 텍스트 $t_i$가 주어졌을 때, 배치 내 모든 이미지 중 $v_i$가 가장 유사해야 함

### 4.3 Temperature Parameter (τ)

Temperature는 Softmax의 뾰족함을 조절하는 중요한 하이퍼파라미터:

- **τ가 작으면**: Softmax가 더 뾰족해짐 → 모델이 강하게 하나의 답을 선택 → Hard Negatives에 민감
- **τ가 크면**: Softmax가 더 완만해짐 → 덜 차별적인 학습

CLIP에서 $\tau$는 학습 가능한 파라미터로, 초기값을 0.07로 설정함. 학습 중 자동으로 최적값으로 수렴함.

### 4.4 배치 크기(Batch Size)의 중요성

InfoNCE Loss에서 배치 크기는 매우 중요함:

- **배치 크기 = N** → **N-1개의 Negative Pair** (이미지당)
- 배치가 클수록 더 많은 Negative가 생성되고, 더 어려운 대조 학습 문제가 됩니다
- CLIP은 **32,768 크기의 대규모 배치**를 사용 (512 GPU에 걸쳐 분산)
- 배치가 너무 작으면 Easy Negatives만 많고, Hard Negatives가 부족해 학습이 비효율적

이것이 CLIP 학습에 엄청난 컴퓨팅 자원이 필요한 이유 중 하나.

---

## 5. SigLIP: Sigmoid Loss for Language-Image Pre-training

Google이 2023년에 발표한 SigLIP은 CLIP의 Softmax Contrastive Loss를 **Sigmoid Binary Classification Loss**로 대체한 변형.

### 5.1 핵심 아이디어: Softmax → Sigmoid

CLIP의 InfoNCE Loss는 배치 내 모든 쌍에 대해 Softmax를 계산함. 이는 배치 전체가 하나의 분류 문제가 되어 **배치 크기에 의존적**임.

SigLIP은 각 이미지-텍스트 쌍을 **독립적인 Binary Classification 문제**로 처리함:

$$L_{SigLIP} = -\frac{1}{N^2} \sum_{i=1}^{N} \sum_{j=1}^{N} \log \frac{1}{1 + \exp(-z_{ij} \cdot (\text{sim}(v_i, t_j) / \tau - b))}$$

여기서 $z_{ij} = +1$ (i=j인 Positive Pair), $z_{ij} = -1$ (i≠j인 Negative Pair), $b$는 학습 가능한 바이어스.

각 쌍에 대해 독립적으로 Sigmoid를 적용하여:
- Positive Pair → 높은 유사도 점수 (Sigmoid(large positive) ≈ 1)
- Negative Pair → 낮은 유사도 점수 (Sigmoid(large negative) ≈ 0)

### 5.2 SigLIP의 장점

| 특성 | CLIP (InfoNCE/Softmax) | SigLIP (Sigmoid) |
|------|----------------------|-----------------|
| 배치 크기 의존성 | 높음 (배치 전체가 분모) | 낮음 (쌍별 독립 계산) |
| 작은 배치 성능 | 나쁨 | 좋음 |
| 계산 방식 | Softmax (전역 정규화) | Sigmoid (지역 이진 분류) |
| VLM Visual Encoder 성능 | 좋음 | 더 좋음 |

**작은 배치 크기에서도 효과적인 이유**: Softmax는 배치 내 모든 Negative의 유사도 합으로 정규화하기 때문에, 배치가 작으면 Negative가 너무 적어 학습 신호가 약해짐. Sigmoid는 각 쌍을 독립적으로 판별하므로 이런 문제가 없음.

### 5.3 VLM에서 SigLIP이 더 좋은 이유

SigLIP으로 학습된 Visual Encoder는 VLM의 Visual Encoder로 사용할 때 CLIP보다 일관되게 우수한 성능을 보임. 이는 VLM 학습에서 선호되는 더 풍부한 Feature를 제공하기 때문으로 추정됨. 현재 많은 최신 VLM(LLaVA-NeXT, InternVL 등)이 SigLIP으로 사전학습된 ViT를 Visual Encoder로 채택하고 있음.

---

## 6. 정리: Vision Encoder의 전체 그림

```
원본 이미지
    │
    ▼
[Patch Embedding]
  - 16×16 또는 14×14 패치로 분할
  - Flatten → Linear Projection → d_model 차원
  - [CLS] 토큰 추가
  - Positional Embedding 추가
    │
    ▼
[ViT Transformer Encoder]
  - Multi-Head Self-Attention (모든 패치 간 전역적 관계 포착)
  - Feed-Forward Network
  - Layer Normalization
  - 수십 개의 Transformer 레이어 반복
    │
    ▼
[Contrastive Pretraining (CLIP/SigLIP)]
  - 이미지 임베딩 ↔ 텍스트 임베딩 정렬
  - InfoNCE (CLIP) 또는 Sigmoid Loss (SigLIP)
    │
    ▼
[Vision Encoder 출력]
  - 옵션 A: [CLS] 토큰 임베딩 → 이미지 전체 표현 (단일 벡터)
  - 옵션 B: 모든 패치 토큰 → 공간 정보 보존 (시퀀스)
    │
    ▼
VLM Connector/Projector로 전달 (8.2절 참조)
```

Vision Encoder의 선택과 품질은 VLM 전체 성능에 결정적인 영향을 미침. 현재 VLM 연구의 표준은 **SigLIP 또는 CLIP으로 사전학습된 ViT-L/14 이상**을 Visual Encoder로 사용하는 것.

---

## 핵심 용어 정리

| 용어 | 설명 |
|------|------|
| **CNN** | Convolutional Neural Network, 지역 필터 기반 이미지 처리 |
| **ViT** | Vision Transformer, 이미지를 패치 시퀀스로 처리 |
| **Patch Embedding** | 이미지 패치를 벡터로 변환하는 과정 |
| **[CLS] Token** | 이미지 전체를 대표하는 학습 가능한 토큰 |
| **Positional Embedding** | 패치의 위치 정보를 인코딩하는 벡터 |
| **CLIP** | Contrastive Language-Image Pretraining, 이미지-텍스트 대조 학습 |
| **SigLIP** | Sigmoid Loss 기반 이미지-텍스트 대조 학습 |
| **InfoNCE Loss** | CLIP에서 사용하는 대조 손실 함수 |
| **Dual Encoder** | 이미지 인코더 + 텍스트 인코더의 이중 구조 |
| **Zero-shot Classification** | 추가 학습 없이 텍스트 프롬프트만으로 분류 |
| **Temperature (τ)** | Contrastive Loss의 날카로움을 조절하는 파라미터 |
| **Translation Equivariance** | CNN의 이동 등변성 특성 |
| **Inductive Bias** | 모델 구조에 내재된 학습 가정 |
