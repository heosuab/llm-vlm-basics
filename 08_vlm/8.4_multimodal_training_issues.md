# 8.4 Multimodal Training Issues: 멀티모달 학습의 핵심 문제들

Vision-Text Alignment, Hallucination, Grounding, 멀티이미지/비디오 처리, 그리고 Catastrophic Forgetting 등등 VLM 학습의 핵심 문제들.

---

## 1. Vision-Text Alignment: 두 세계를 연결하기

### 1.1 왜 Alignment가 어려운가?

Vision Encoder(예: CLIP ViT)와 LLM(예: LLaMA)은 완전히 **다른 방식으로 사전학습**됨:

- **Vision Encoder**: 이미지-텍스트 대조 학습 → 이미지와 텍스트를 임베딩 공간에서 정렬
- **LLM**: 다음 토큰 예측(Causal Language Modeling) → 텍스트 시퀀스의 분포를 모델링

이 두 공간은 완전히 다른 특성을 가지고 있음. 시각 특징을 LLM의 토큰 임베딩 공간으로 매핑하는 것이 바로 **Alignment**의 핵심.

### 1.2 2단계 학습 파이프라인 (Two-Stage Training Pipeline)

현재 가장 검증된 방법은 LLaVA가 확립한 **2단계 학습 전략**:

---

**Stage 1: Pre-training (사전 정렬 단계)**

목표: Vision Encoder의 출력을 LLM 토큰 공간과 정렬

```
학습 구성:
  - Vision Encoder: 완전 동결 (Frozen)
  - LLM: 완전 동결 (Frozen)
  - Projector (MLP/Linear): 학습 가능 ✓

학습 데이터:
  - 대규모 이미지-캡션 쌍 (예: CC3M, LAION, ShareGPT4V)
  - 600K ~ 수백만 샘플

학습 태스크:
  - Image Captioning: 이미지 → 텍스트 설명 생성
  - 단순하지만 Alignment에 핵심적인 태스크
```

이 단계에서 Projector는 "시각 특징을 어떻게 LLM이 이해하는 형태로 변환할지"를 학습함. 마치 번역가처럼 시각 언어를 텍스트 언어로 번역하는 능력을 키우는 것.

---

**Stage 2: Instruction Fine-tuning (지시 학습 단계)**

목표: 다양한 멀티모달 태스크를 따르는 능력 학습

```
학습 구성:
  - Vision Encoder: 동결 (일부 모델은 낮은 LR로 파인튜닝)
  - LLM: 학습 가능 ✓ (또는 LoRA로 효율적 학습)
  - Projector: 학습 가능 ✓

학습 데이터:
  - 다양한 멀티모달 Instruction 데이터
    - VQA (Visual Question Answering)
    - 이미지 설명 (Detailed Description)
    - 추론 (Reasoning)
    - OCR, 차트 이해
  - 약 600K ~ 수백만 샘플

학습 태스크:
  - 사용자 지시에 따라 이미지를 분석하고 응답 생성
  - Chat 형식의 멀티턴 대화
```

---

**왜 2단계인가?**

Stage 1에서 LLM을 동결하는 이유:
1. LLM이 아직 시각 토큰을 이해하지 못하므로 함께 학습하면 비효율적
2. Projector가 빠르게 Vision-LLM 공간 연결을 학습할 수 있음
3. 이미 검증된 LLM 능력(추론, 언어 이해)을 보존

Stage 2에서 LLM을 함께 학습하는 이유:
1. Stage 1에서 정렬된 표현을 기반으로 LLM이 시각 정보를 더 깊이 통합
2. 복잡한 추론, 지시 따르기 능력을 멀티모달로 확장

```
Stage 1 학습 예시 데이터:
입력: [이미지: 해변 사진] "이 이미지를 설명해주세요."
출력: "A sandy beach with gentle waves and blue sky."

Stage 2 학습 예시 데이터:
입력: [이미지: 복잡한 차트] "2022년 대비 2023년 수익 변화율을 계산해주세요."
출력: "차트를 보면 2022년 수익은 $1.2M이고 2023년은 $1.5M입니다. 따라서 변화율은 (1.5-1.2)/1.2 × 100 = 25% 증가입니다."
```

---

## 2. Hallucination in VLMs: 보이지 않는 것을 말하다

### 2.1 Hallucination이란?

VLM의 Hallucination은 **이미지에 없거나 부정확한 정보를 사실인 것처럼 생성**하는 문제. LLM의 Hallucination과 비슷하지만, 이미지라는 추가적인 그라운딩 소스가 있음에도 틀린 정보를 생성한다는 점에서 더 심각함.

### 2.2 Object Hallucination (객체 환각)

모델이 이미지에 **존재하지 않는 물체를 있다고 주장**하는 경우:

```
이미지: 빈 탁자 사진 (물건 없음)
질문: "탁자 위에 무엇이 있나요?"

좋은 응답: "탁자가 비어 있습니다."
Hallucination: "탁자 위에 컵과 책이 있습니다."
             (실제로 없는 물체를 만들어냄)
```

**원인**:
1. **Language Prior Bias**: 학습 데이터에서 "탁자"가 나오면 "컵, 책" 등이 함께 나오는 경우가 많아 LLM이 패턴을 학습
2. **시각 신호의 약한 영향**: 이미지 정보보다 언어 패턴이 응답을 더 강하게 지배
3. **부족한 Negative Training**: "없다"고 말하는 예시가 상대적으로 적음

### 2.3 Attribute Hallucination (속성 환각)

물체는 맞게 인식하지만 **색상, 크기, 개수 등의 속성을 잘못 설명**하는 경우:

```
이미지: 빨간 차 사진
질문: "차의 색깔은?"
Hallucination: "파란색 차입니다."

이미지: 사과 3개 사진
질문: "사과가 몇 개인가요?"
Hallucination: "5개의 사과가 있습니다."
```

### 2.4 Over-reliance on Language Priors

Language Prior에 지나치게 의존하는 전형적인 패턴:

```
이미지: 의사가 청진기 없이 진료하는 사진
질문: "의사가 무엇을 들고 있나요?"
Language Prior Hallucination: "청진기를 들고 있습니다."
(의사 → 청진기 라는 강한 언어 연상)
```

### 2.5 POPE Benchmark (Polling-based Object Probing Evaluation)

POPE는 VLM의 Object Hallucination을 측정하는 표준 벤치마크:

```
평가 방식:
- Yes/No 질문 형식: "이미지에 [물체]가 있나요?"
- Positive 샘플: 실제 존재하는 물체에 대한 질문
- Negative 샘플: 존재하지 않는 물체에 대한 질문 (3가지 종류)

Negative 샘플 구성:
1. Random: 완전 무작위로 선택한 물체
2. Popular: 자주 등장하는 물체 (모델이 쉽게 환각할 수 있는)
3. Adversarial: 이미지의 물체와 자주 함께 등장하는 물체
                (예: 이미지에 컵이 있으면 → "접시가 있나요?" 질문)
```

Adversarial POPE가 가장 어려움. 모델이 Language Prior에 의존한다면 "컵이 있으니 접시도 있을 것"이라는 환각을 일으킴.

### 2.6 Hallucination 완화 방법

1. **더 나은 학습 데이터**: 부정 표현("~이 없습니다")을 포함한 다양한 데이터
2. **RLHF/RLAIF**: 사람 또는 AI 피드백으로 환각 응답에 페널티
3. **Visual Grounding 강화**: 응답을 이미지의 특정 영역에 연결
4. **Contrastive Decoding**: 이미지 있을 때와 없을 때의 로짓 차이를 활용
5. **Instruction Tuning Data 다양화**: 부정 답변, 속성 세분화 등 다양한 패턴 포함

---

## 3. Grounding: 언어와 이미지 영역 연결하기

### 3.1 Grounding이란?

Grounding은 **언어 표현(Language Expression)을 이미지의 특정 영역(Region)**에 연결하는 능력. 단순히 이미지를 설명하는 것을 넘어, "어디에" 있는지를 파악함.

### 3.2 Referring Expression Comprehension (REC)

자연어 표현으로 지시된 물체의 위치(Bounding Box)를 찾는 태스크:

```
이미지: 여러 사람이 있는 파티 사진
질문: "빨간 드레스를 입은 오른쪽 여성을 찾아주세요."
출력: Bounding Box 좌표 [x1=0.65, y1=0.20, x2=0.85, y2=0.90]

또는 모델이 직접 좌표를 텍스트로 생성:
"<bbox>0.65, 0.20, 0.85, 0.90</bbox>"
```

### 3.3 Grounded Captioning

이미지를 설명할 때 각 설명이 이미지의 어느 부분과 연결되는지를 명시함:

```
일반 Captioning:
"A woman in a red dress is standing next to a table with flowers."

Grounded Captioning:
"A woman <ref>오른쪽 인물<box>[0.65, 0.2, 0.85, 0.9]</box></ref>
 in a red dress <ref>빨간 드레스<box>[0.66, 0.4, 0.84, 0.8]</box></ref>
 is standing next to a table
 with flowers <ref>꽃 테이블<box>[0.3, 0.5, 0.6, 0.9]</box></ref>."
```

### 3.4 Grounding 학습 방법

**Grounding 데이터 포맷**: 이미지 + 바운딩 박스 좌표 + 텍스트 레이블

```python
# 학습 데이터 예시
{
    "image": "party_photo.jpg",
    "conversations": [
        {"role": "user", "content": "빨간 드레스를 입은 여성의 위치는?"},
        {"role": "assistant", "content": "빨간 드레스를 입은 여성은 이미지 오른쪽에 위치합니다. <box>[0.65, 0.20, 0.85, 0.90]</box>"}
    ]
}
```

**Coordinate Representation 방식**:
- **텍스트 토큰**: 좌표를 "<box>x1, y1, x2, y2</box>" 형태 텍스트로 생성 (대부분의 모델)
- **특수 토큰**: `<loc_xxx>` 같은 특수 토큰을 어휘에 추가
- **정규화 좌표**: 0~1 사이로 정규화하거나 0~999 사이의 정수로 표현

---

## 4. Multi-image Input: 여러 이미지 처리

### 4.1 멀티이미지의 필요성

많은 실제 사용 사례가 여러 이미지를 동시에 처리해야 함:

- "이 두 제품 사진을 비교해줘" (2장 비교)
- "이 전후 사진들의 변화를 설명해줘" (Before/After 비교)
- "4장의 옵션 중 가장 적합한 것을 골라줘" (다중 선택)
- 대화 중에 여러 이미지가 순차적으로 등장

### 4.2 구현 방법

멀티이미지 처리는 개념적으로 단순함: 각 이미지를 독립적으로 Vision Encoder로 처리한 뒤, 시각 토큰들을 텍스트 시퀀스에서 각자의 자리에 삽입함:

```
대화 예시:
"[이미지1] 이 사진의 사람은 누구인가요?
 [이미지2] 그리고 이 사진의 사람은요?"

처리된 시퀀스:
[V1_1, V1_2, ..., V1_576,   ← 이미지1의 시각 토큰
 텍스트: "이 사진의 사람은 누구인가요?",
 V2_1, V2_2, ..., V2_576,   ← 이미지2의 시각 토큰
 텍스트: "그리고 이 사진의 사람은요?"]
```

### 4.3 Position IDs across Images

중요한 기술적 문제는 **여러 이미지에 걸친 위치 인코딩**:

**옵션 1: 연속 Position IDs**
```
이미지1 토큰: position 0, 1, 2, ..., 575
텍스트 토큰: position 576, 577, ...
이미지2 토큰: position 600, 601, ..., 1175
```
문제: 이미지2의 패치 위치 정보가 상대적 공간 위치가 아닌 절대 순서로 인코딩됨

**옵션 2: 이미지별 독립적 Position IDs + 특수 구분자**
```
이미지1 토큰: position 0~575 (내부적으로 독립)
<image_sep> 토큰
이미지2 토큰: position 0~575 (독립적으로 재시작)
```
각 이미지가 독립적인 공간 정보를 유지

**Qwen2-VL의 M-RoPE**: 이미지에는 2D 위치(행, 열), 텍스트에는 1D 위치를 각각 인코딩하여 혼용하는 방식으로 이 문제를 우아하게 해결함.

### 4.4 실용적 제한

현재 대부분의 VLM이 멀티이미지를 지원하지만 실용적 제한이 있음:

- **이미지당 토큰 수 × 이미지 수 = 전체 시각 토큰 수** → Context Length 제한
- 예: 이미지당 576 토큰, Context 4096 → 약 7장의 이미지만 처리 가능
- 고해상도 + 멀티이미지 = 빠른 Context Length 소진

---

## 5. Video Tokens: 시간 차원 추가

### 5.1 비디오 처리의 기본 접근

비디오는 기본적으로 **시간 순서로 나열된 이미지(Frame)**의 시퀀스:

```
비디오 (30fps, 10초)
    ↓ Frame Sampling
선택된 프레임들: [Frame_1, Frame_3, Frame_6, ..., Frame_30]
    ↓ 각 프레임을 Vision Encoder로 처리
[V1_1...V1_576, V3_1...V3_576, V6_1...V6_576, ...]
    ↓ LLM에 시퀀스로 입력
시간 순서로 이해 가능
```

### 5.2 Frame Sampling 전략

모든 프레임을 처리하는 것은 불가능하므로, 적절한 프레임을 선택해야 함:

**Uniform Sampling (균일 샘플링)**:
```
10초 비디오에서 8개 프레임 선택:
0초, 1.4초, 2.8초, 4.2초, 5.6초, 7.0초, 8.4초, 10초
→ 시간적으로 균일하게 분산
장점: 구현 단순, 전체 내용 커버
단점: 중요한 순간과 중요하지 않은 순간을 동일 취급
```

**Keyframe Extraction (키프레임 추출)**:
```
변화가 큰 순간의 프레임만 선택:
- 씬 전환 감지 (연속 프레임 간 픽셀 차이가 큰 순간)
- 움직임이 활발한 구간
- 오디오 변화가 큰 순간 (말이 시작/종료되는 시점)
장점: 중요한 순간에 집중
단점: 정적인 씬에서는 과도하게 샘플링
```

**Adaptive Sampling**:
```
내용에 따라 샘플링 밀도를 조절:
- 빠른 변화 → 높은 샘플링 밀도
- 느린 변화 → 낮은 샘플링 밀도
장점: 효율적인 정보 커버리지
단점: 구현 복잡, 추가 계산 비용
```

### 5.3 Token Count Explosion Problem

비디오 처리에서 가장 큰 기술적 도전:

```
비디오 처리 예시:
- 비디오: 1분, 1fps 샘플링 = 60 프레임
- 프레임당 토큰: 576개 (336×336)
- 총 시각 토큰: 60 × 576 = 34,560 토큰
- 일반 LLM Context Length: 4,096~32,768 토큰
→ 단순 접근으로는 1분 비디오도 처리 불가!
```

**해결책**:

1. **Temporal Pooling (시간 풀링)**:
   ```
   연속 N개 프레임의 토큰을 평균 풀링
   예: 4개 프레임을 1개로 → 토큰 수 75% 감소
   ```

2. **Per-frame Token Compression**:
   ```
   각 프레임의 패치 토큰을 풀링으로 압축
   576 토큰/프레임 → 144 토큰/프레임 (4배 압축)
   60 프레임 × 144 = 8,640 토큰 (관리 가능한 수준)
   ```

3. **3D Patch Embedding**:
   ```
   여러 연속 프레임을 3D 볼륨으로 처리
   시간 × 높이 × 너비 → 단일 3D 패치 임베딩
   ```

4. **Long Context LLM 사용**:
   ```
   Context Length가 128K~1M인 모델 사용
   (예: Gemini 1.5 Pro의 1M Context)
   장기 비디오도 처리 가능
   ```

### 5.4 시간 정보 인코딩

프레임의 시간적 순서 정보도 인코딩해야 합니다:

```python
# Temporal Position Embedding 추가
for frame_idx, frame_tokens in enumerate(all_frame_tokens):
    # 공간적 위치 임베딩
    spatial_pos_embed = get_2d_position_embedding(frame_tokens)
    # 시간적 위치 임베딩
    temporal_pos_embed = get_temporal_embedding(frame_idx)
    # 결합
    frame_tokens = frame_tokens + spatial_pos_embed + temporal_pos_embed
```

---

## 6. Catastrophic Forgetting: 새 지식이 예전 것을 지운다

### 6.1 현상 설명

**Catastrophic Forgetting**은 신경망이 새로운 태스크를 학습할 때 이전에 학습한 능력을 잊어버리는 현상. VLM에서는 특히 두 가지 형태로 나타남:

**1) LLM의 텍스트 능력 저하**:
```
VLM 파인튜닝 전:
  "다음 수식을 풀어주세요: 2x + 5 = 11"
  → "x = 3" (정확)

VLM 파인튜닝 후 (과도한 파인튜닝):
  "다음 수식을 풀어주세요: 2x + 5 = 11"
  → "이미지를 제공해주세요" 또는 엉뚱한 응답
  (텍스트 전용 추론 능력이 손상됨)
```

**2) Vision Encoder의 표현 품질 저하 (Encoder Drift)**:
```
CLIP Vision Encoder는 일반적인 이미지 표현을 잘 학습함
    ↓
VLM 학습 시 Vision Encoder가 파인튜닝됨
    ↓
VLM 학습 데이터에 편향된 표현으로 drift
    ↓
분포 밖의 이미지에 대한 일반화 능력 저하
```

### 6.2 Vision Encoder Drift

Vision Encoder를 VLM 학습 중에 파인튜닝할 때 발생하는 Drift는 특히 위험합니다:

**왜 Drift가 발생하는가?**
- CLIP은 수억 개의 다양한 이미지-텍스트 쌍으로 일반적인 시각 표현 학습
- VLM 파인튜닝 데이터는 상대적으로 소규모이고 편향됨 (VQA, 지시 따르기 등)
- 강한 Gradient가 인코더 표현을 VLM 태스크 방향으로 무리하게 변경

```
CLIP 표현 공간 (원래):
- "강아지" 이미지 ↔ "dog" 텍스트: 코사인 유사도 0.85
- "바나나" 이미지 ↔ "banana" 텍스트: 코사인 유사도 0.88

Drift 발생 후:
- "강아지" 이미지 ↔ "dog" 텍스트: 코사인 유사도 0.60 (저하)
- VLM 학습 데이터에 강아지 사진이 적었다면 더 심각
```

### 6.3 해결 방법들

**1) 낮은 Learning Rate 사용**

Vision Encoder 파인튜닝 시 매우 낮은 학습률 적용:

```python
optimizer = AdamW([
    # Projector: 정상 학습률
    {"params": projector.parameters(), "lr": 1e-3},
    # LLM: 중간 학습률
    {"params": llm.parameters(), "lr": 2e-5},
    # Vision Encoder: 매우 낮은 학습률
    {"params": vision_encoder.parameters(), "lr": 2e-6},  # 10배~100배 낮음
])
```

**2) Selective Freezing (선택적 동결)**

Vision Encoder의 일부만 파인튜닝:

```
동결: Vision Encoder 하위 레이어 (저수준 특징 보존)
학습: Vision Encoder 상위 레이어 (고수준 의미 조정)

예시 (24레이어 ViT-L):
- 레이어 1~12: Frozen (기본 엣지, 텍스처 특징)
- 레이어 13~24: Trainable (의미적 표현 조정)
```

**3) Learning Rate Warmup + Scheduling**

급격한 표현 변화를 방지하기 위한 Warmup:

```python
# Warmup: 처음 몇 스텝은 매우 낮은 LR
# 이후 선형 또는 코사인으로 목표 LR까지 증가
# VLM 파인튜닝 후반부에는 다시 감소 (Cosine Decay)
```

**4) 텍스트 전용 데이터 혼합**

멀티모달 데이터와 텍스트 전용 데이터를 혼합해 학습:

```
학습 배치 구성:
- 80%: 멀티모달 데이터 (이미지 + 텍스트)
- 20%: 텍스트 전용 데이터 (기존 LLM 학습 데이터)

→ LLM이 텍스트 능력을 유지하면서 시각 능력도 학습
```

**5) EWC (Elastic Weight Consolidation)**

이전 태스크에서 중요한 파라미터를 정규화로 보호:

```python
# Fisher Information Matrix를 이용해 각 파라미터의 중요도를 계산
# 중요한 파라미터에 강한 정규화(페널티)를 적용
# → 중요한 파라미터가 크게 변하지 않도록 제약
```

---

## 7. 학습 안정성을 위한 추가 기법들

### 7.1 Gradient Clipping

VLM 학습에서 Gradient가 폭발적으로 커지는 것을 방지:

```python
# 학습 루프에서
loss.backward()
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
optimizer.step()
```

### 7.2 Mixed Precision Training

메모리 효율과 속도를 위한 BF16/FP16 학습:

```python
from torch.cuda.amp import autocast, GradScaler
scaler = GradScaler()

with autocast(dtype=torch.bfloat16):
    output = model(images, input_ids)
    loss = criterion(output, labels)

scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```

### 7.3 데이터 품질이 전략보다 중요

실제 VLM 연구에서 종종 발견되는 교훈:
- 고품질 데이터 10만 개 > 저품질 데이터 100만 개
- 텍스트 설명의 품질, 이미지-텍스트 정합성이 핵심
- LLaVA-1.5가 상대적으로 적은 데이터로 우수한 성능을 낸 비결도 데이터 품질

---

## 8. 핵심 용어 정리

| 용어 | 설명 |
|------|------|
| **Vision-Text Alignment** | 시각 특징과 언어 특징을 같은 공간에서 정렬하는 과정 |
| **Two-Stage Training** | Stage 1(정렬) + Stage 2(지시 학습)의 2단계 학습 파이프라인 |
| **Hallucination** | 이미지에 없는 정보를 있다고 생성하는 현상 |
| **Object Hallucination** | 존재하지 않는 물체를 있다고 주장 |
| **Attribute Hallucination** | 물체의 속성(색상, 크기 등)을 잘못 설명 |
| **Language Prior** | 언어 모델이 학습한 단어 간 통계적 패턴 |
| **POPE** | Polling-based Object Probing Evaluation, Hallucination 평가 벤치마크 |
| **Grounding** | 언어 표현을 이미지의 특정 영역과 연결 |
| **REC** | Referring Expression Comprehension, 자연어로 지시된 물체 위치 찾기 |
| **Bounding Box** | 물체의 위치를 나타내는 직사각형 좌표 |
| **Frame Sampling** | 비디오에서 처리할 프레임을 선택하는 전략 |
| **Temporal Pooling** | 연속 프레임의 토큰을 시간 축으로 압축 |
| **Catastrophic Forgetting** | 새로운 학습이 기존 능력을 덮어쓰는 현상 |
| **Vision Encoder Drift** | 파인튜닝으로 인한 사전학습 표현 품질 저하 |
| **Selective Freezing** | 모델의 특정 레이어만 선택적으로 학습 |
| **Gradient Clipping** | Gradient 크기를 제한해 학습 안정성 확보 |
