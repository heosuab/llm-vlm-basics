# 1.4 Positional Encoding

> Transformer는 순서를 모르는 구조이므로, 위치 정보를 별도로 주입해야 합니다. 다양한 방법과 그 장단점을 다룹니다.

---

## 왜 Positional Encoding이 필요한가?

Self-attention은 "어떤 위치에 있는지"에 관계없이 token 간의 관계만 계산합니다.
즉, "The dog bit the man"과 "The man bit the dog"을 동일하게 처리합니다.

위치 정보를 주입해야 순서를 학습할 수 있습니다.

---

## Sinusoidal Positional Encoding

원래 "Attention Is All You Need" 논문에서 제안된 방법입니다.

```
PE(pos, 2i)   = sin( pos / 10000^(2i / d_model) )
PE(pos, 2i+1) = cos( pos / 10000^(2i / d_model) )
```

- pos: 위치 인덱스 (0, 1, 2, ...)
- i: 차원 인덱스 (0, 1, ..., d_model/2 - 1)
- 짝수 차원: sin, 홀수 차원: cos

**직관**: 각 차원이 서로 다른 주파수의 sin/cos 파형으로 위치를 인코딩합니다.
작은 i일수록 파장이 짧고 (세밀한 위치 구분), 큰 i일수록 파장이 깁니다 (대략적 위치).

**장점**: 학습 없이 계산 가능, 이론적으로 학습 시 최대 길이 이상으로 extrapolation 가능
**단점**: 실제 extrapolation 성능이 좋지 않음, 학습 시 최대 길이에서 성능이 떨어짐

---

## Learned Positional Encoding

각 위치에 대한 임베딩을 직접 학습합니다:

```
PE = EmbeddingLayer(position_ids)  # (max_len, d_model)
input = token_embedding + PE
```

BERT, GPT-2에서 사용합니다.

**장점**: 단순, 데이터에서 직접 학습
**단점**: 학습 시 최대 길이 이상으로 extrapolation 불가 (그 이상 길이는 처음 보는 것이므로)

---

## RoPE (Rotary Position Embedding)

현대 LLM의 표준 positional encoding입니다. LLaMA, Mistral, Qwen, Gemma 등이 모두 사용합니다.

### 핵심 아이디어

위치 정보를 embedding에 더하는 것이 아니라, **Q와 K 벡터를 위치에 따라 회전**시킵니다.
그 결과, Q와 K의 내적(attention score)이 **상대 위치(relative position)에 의존**하게 됩니다.

```
position m의 query vector q에 rotation matrix R_m 적용:
q_m = R_m · q

position n의 key vector k에 rotation matrix R_n 적용:
k_n = R_n · k

attention score:
q_m · k_n = (R_m · q)^T · (R_n · k) = q^T · R_m^T · R_n · k = q^T · R_{m-n} · k
```

즉, attention score가 m-n(상대 위치)에만 의존하게 됩니다.

### 2D 회전으로 구현

d_k 차원을 2개씩 묶어 2D 회전으로 구현:
```
[x₁, x₂] → [x₁cos θ - x₂sin θ, x₁sin θ + x₂cos θ]

각 pair i에 대해: θᵢ = pos / 10000^(2i / d_k)
```

**장점**:
- 상대 위치 정보가 자연스럽게 인코딩됨
- Extrapolation이 sinusoidal보다 훨씬 나음
- Attention 연산과 통합되어 추가 파라미터 없음

---

## ALiBi (Attention with Linear Biases)

position encoding 대신 **attention score에 선형 페널티를 직접 추가**합니다.

```
Attention score_ij = QᵢKⱼ^T / √d_k - m · |i - j|
```

- m: 각 head마다 다른 고정 slope (학습 없음)
- |i - j|: 두 token 사이의 거리
- 멀수록 attention score에 더 큰 페널티

**장점**:
- 학습 가능한 파라미터 없음
- 학습 시 길이보다 긴 시퀀스에서도 잘 동작 (강한 extrapolation)
- 구현이 매우 단순

**단점**: 상대적으로 제한된 positional information (거리만 고려, 방향 무시)

MPT, BLOOM 등에서 사용합니다.

---

## NTK Scaling

RoPE의 "Neural Tangent Kernel" 관점에서 base frequency를 조정하여 context length를 늘리는 방법입니다.

```
RoPE base를 10000에서 더 큰 값으로 변경:
θᵢ = pos / base^(2i / d_k)

base를 키우면 → θ가 작아짐 → 더 긴 주기 → 더 긴 context에서도 위치 구분 가능
```

**NTK-aware scaling**:
```
new_base = base * (new_len / train_len)^(d_k / (d_k - 2))
```

장점: 재학습 없이 추론 시에만 적용 가능 (few-shot adaptation 가능)

---

## Long-Context RoPE Interpolation

학습 시 최대 길이(예: 4K) 이상에서 RoPE를 사용하는 방법들입니다.

### Position Interpolation (PI)
긴 위치를 그냥 쓰는 대신, 기존 범위 내로 **압축(interpolate)**합니다:
```
position = pos * (train_len / new_len)
```
예: train_len=4096, new_len=32768이면, position을 8배 압축.
소량의 fine-tuning으로 좋은 성능을 냅니다.

### YaRN (Yet another RoPE extensioN)
NTK scaling + 고주파 성분 보존을 결합:
- 고주파 성분 (가까운 거리): 변경 없음 (이미 잘 작동)
- 저주파 성분 (먼 거리): interpolation 적용
LLaMA-2의 long context 버전에 사용되었습니다.

---

## 2D Positional Encoding (Vision)

이미지 patch를 sequence로 처리할 때, 1D PE를 2D로 확장합니다.

### 방법 1: 분리 가능(Factorized) 2D PE
```
PE(row, col) = PE_row(row) + PE_col(col)
```
행과 열을 독립적으로 인코딩하고 더합니다.

### 방법 2: 2D Sinusoidal PE
2D 좌표를 각각 sin/cos로 인코딩하여 concat합니다.

### 방법 3: Learned 2D PE
각 (row, col) 쌍에 대한 임베딩을 직접 학습합니다 (ViT 기본 방식).

---

## Multimodal RoPE (M-RoPE)

Qwen2-VL에서 도입된 방법으로, 텍스트와 이미지 token에 **별도의 position ID**를 할당합니다.

```
텍스트 token: (t, t, t) - 동일한 1D position을 3개 복제
이미지 patch: (t, h, w) - 시간 t, 행 h, 열 w를 독립적으로 인코딩
```

RoPE의 d_k 차원을 3등분하여:
- 1/3은 temporal position
- 1/3은 height position
- 1/3은 width position

으로 사용합니다.

**장점**:
- 이미지의 2D 공간 구조가 position encoding에 반영됨
- 비디오에서 시간 차원도 자연스럽게 확장
- 텍스트 token의 1D sequential 특성도 유지
