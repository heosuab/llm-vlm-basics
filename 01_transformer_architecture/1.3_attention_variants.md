# 1.3 Attention Variants

> 표준 Multi-Head Attention(MHA)의 메모리·속도 문제를 해결하기 위한 다양한 변형들.

---

## Multi-Query Attention (MQA)

### 아이디어

표준 MHA에서 h개의 head가 각각 독립적인 K, V projection을 가짐.
MQA는 **모든 head가 단 하나의 K와 V를 공유**.

```
# MHA
Q: (batch, h, seq, d_k)
K: (batch, h, seq, d_k)   ← h개 독립
V: (batch, h, seq, d_v)   ← h개 독립

# MQA
Q: (batch, h, seq, d_k)
K: (batch, 1, seq, d_k)   ← 1개 공유
V: (batch, 1, seq, d_v)   ← 1개 공유
```

**KV-cache 메모리 절감**: h배 줄어듭니다 (예: 32 heads → 32배 절감)
**모델 품질**: 약간의 성능 저하 있음 (하지만 실용적으로 허용 가능)

PaLM이 처음 도입했고, Falcon 등에서 사용.

---

## Grouped Query Attention (GQA)

MHA(각 head 독립)와 MQA(단 1개 공유) 사이의 **중간점**.
G개의 그룹으로 나눠서, 그룹 내에서 K, V를 공유.

```
# GQA (G=4, h=32 예시)
Q: (batch, 32, seq, d_k)   ← 32 heads
K: (batch,  4, seq, d_k)   ← 4 groups
V: (batch,  4, seq, d_v)   ← 4 groups

# 각 8개의 Q-head가 1개의 K, V 그룹을 공유
```

**장점**: MQA만큼 빠르면서 MHA에 가까운 품질

**채택 모델들**:
- LLaMA-2 (G = h/8)
- LLaMA-3
- Mistral 7B (G = h/4)
- Gemma, Qwen2 등

GQA는 현재 decoder-only LLM의 사실상 표준이 된 방법.

---

## Sparse Attention

모든 n² 쌍을 계산하지 않고 **선택된 쌍만** 계산.

### 패턴 유형

**Local (window) attention**: 각 token은 주변 w개의 token만 봄
```
token i attends to: [i-w/2, ..., i+w/2]  (bidirectional)
                또는: [i-w, ..., i]         (causal)
```

**Strided attention**: 규칙적인 간격으로 떨어진 token들을 봄
```
token i attends to: i, i-s, i-2s, ...
```

**Global tokens**: 특정 token(예: [CLS])은 모든 token과 attention
→ Longformer: local + global token 조합

**복잡도**: O(n·w) where w ≪ n → 사실상 O(n)

---

## Sliding Window Attention

Local attention의 실용적 구현. 각 token이 자신을 중심으로 **w개의 window 내의 token들만** attend.

```
Mistral 7B 설정:
  window_size = 4096
  context_length = 32768

각 token: 최대 4096개의 이전 token과 attention
→ O(n·w) 복잡도
```

**장점**: 긴 시퀀스에서도 메모리가 O(n·w)로 제한됨
**단점**: window를 벗어난 정보는 직접 attend 불가 (여러 layer를 거쳐 간접적으로 전달)

---

## Linear Attention (Performer 등)

Softmax attention을 **kernel function으로 근사**하여 O(n²)을 O(n)으로 줄임.

### 핵심 아이디어

```
Attention(Q, K, V) = softmax(QK^T / √d) V

softmax(QK^T) ≈ φ(Q) · φ(K)^T  (kernel trick)
→ (φ(Q) · φ(K)^T) · V = φ(Q) · (φ(K)^T · V)
```

행렬 곱셈 순서를 바꾸면: φ(K)^T · V 를 먼저 계산 (n × n이 아니라 d × d 크기)
→ O(nd) 복잡도 달성

**한계**:
- 근사 오차 존재 (exact softmax가 아님)
- 실제 학습 품질이 softmax attention보다 낮은 경우 많음
- Causal masking과의 호환이 복잡함

현재는 연구 단계이며, 실제 프로덕션 LLM에는 아직 거의 사용되지 않음.

---

## FlashAttention (v1/v2/v3)

FlashAttention은 attention 연산 자체를 바꾸지 않고 **IO를 최적화**하여 빠르게 만듬.

### 문제: GPU 메모리 계층

```
GPU 메모리 계층:
  SRAM (on-chip):  ~20 MB, 매우 빠름 (19 TB/s)
  HBM (off-chip): ~80 GB, 상대적으로 느림 (2 TB/s)
```

표준 attention 구현:
1. QK^T 계산 → HBM에 저장 (n×n matrix)
2. Softmax 계산 → HBM에서 읽고, 결과 저장
3. × V 계산 → HBM에서 읽음
→ HBM 접근이 O(n²)번 → **bottleneck**

### FlashAttention의 해결책: Tiling + Recomputation

```
1. Q, K, V를 작은 tile로 나눔 (SRAM에 맞게)
2. 각 tile을 SRAM으로 로드
3. SRAM 내에서 partial attention 계산
4. Incremental softmax normalization (online softmax)
5. 중간 n×n 행렬을 HBM에 저장하지 않음
6. Backward pass: 중간값 재계산 (HBM 저장 대신)
```

**결과**:
- HBM 접근 O(n²) → O(n) 수준으로 감소
- 속도: A100에서 2~4배 빠름
- 메모리: O(n²) → O(n)

**버전별 발전**:
| 버전 | 특징 |
|------|------|
| FlashAttention-1 (2022) | Tiling, recomputation 도입 |
| FlashAttention-2 (2023) | 병렬화 개선, sequence parallelism 지원, 2배 더 빠름 |
| FlashAttention-3 (2024) | H100 최적화, FP8 지원, asynchronous execution |

FlashAttention은 현재 LLM 학습·추론의 표준 구현.

---

## Memory-Efficient Attention Kernels

FlashAttention 외에도 (대표적인) 메모리 효율적인 attention 구현들.

**xformers (Meta)**:
- FlashAttention과 유사한 접근 + 추가 최적화
- 다양한 attention 패턴 지원 (local, global 등)
- PyTorch에 통합되기 시작

**Memory-Efficient Attention (Jax/TPU 버전)**:
- Google의 구현, TPU에 최적화
- Ring attention: sequence를 여러 장치에 분산

**실용 팁**: PyTorch 2.0+에서는 `torch.nn.functional.scaled_dot_product_attention()`이 자동으로 FlashAttention을 사용.
