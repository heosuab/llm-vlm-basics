# 1.2 Attention Details

> Attention 메커니즘의 수식과 구현 세부사항을 깊게 파헤칩니다.

---

## Scaled Dot-Product Attention

Transformer의 핵심 연산입니다. 전체 수식:

```
Attention(Q, K, V) = softmax( QK^T / √d_k ) · V
```

각 요소별 설명:

| 요소 | 크기 | 의미 |
|------|------|------|
| Q | (n, d_k) | Query: "내가 찾는 정보" |
| K | (m, d_k) | Key: "각 위치가 갖고 있는 정보의 레이블" |
| V | (m, d_v) | Value: "실제로 전달할 내용" |
| QK^T | (n, m) | 각 query-key 쌍의 유사도 (attention score) |
| softmax(...) | (n, m) | 정규화된 attention weight |
| output | (n, d_v) | value들의 weighted sum |

n = query 길이, m = key/value 길이 (self-attention에서는 n = m)

---

### 왜 √d_k로 나누는가?

d_k 차원의 벡터 Q, K가 각 성분이 평균 0, 분산 1을 가지는 경우, 내적 Q·K의 분산은 **d_k**가 됩니다.
→ d_k가 클수록 내적 값이 매우 커짐
→ softmax 입력이 크면 gradient가 매우 작아짐 (softmax 포화, vanishing gradient)
→ √d_k로 나눠서 분산을 1로 유지합니다.

```python
# 예시: d_k = 64이면
scores = (Q @ K.T) / math.sqrt(64)  # = (Q @ K.T) / 8
```

---

## Attention Masking

### Causal Mask (Decoder / Autoregressive LM)

Decoder-only LM(GPT 계열)은 future token을 볼 수 없어야 합니다. 이를 위해 upper triangular 부분을 `-inf`로 마스킹합니다.

**적용 위치**: QK^T 이후, softmax **이전**에 더합니다.

```
scores = QK^T / √d_k           # (n, n) attention scores
scores = scores + causal_mask   # -inf for future positions
attn_weights = softmax(scores)  # -inf → 0 (softmax 후)
output = attn_weights @ V
```

causal_mask의 생김새 (4×4 예시):
```
[[0,   -inf, -inf, -inf],
 [0,    0,   -inf, -inf],
 [0,    0,    0,   -inf],
 [0,    0,    0,    0  ]]
```

위치 0은 자기 자신만, 위치 1은 0과 1, ... 이런 식으로 과거만 볼 수 있습니다.

---

### Padding Mask

Batch 내의 시퀀스 길이가 다를 때, 짧은 시퀀스는 `[PAD]` token으로 채웁니다. 이 padding position은 attention에 포함되지 않아야 하므로 마스킹합니다.

```python
# padding_mask: (batch, 1, 1, seq_len)
# True (또는 -inf) where token is [PAD]
scores = scores + padding_mask
```

**Causal mask vs Padding mask 차이**:
| | Causal Mask | Padding Mask |
|--|-------------|--------------|
| 목적 | Future 토큰 차단 | PAD 토큰 차단 |
| 형태 | Upper triangular | 각 sequence의 PAD 위치 |
| Decoder-only | 필수 | 필요 시 |
| Encoder | 불필요 | 필요 시 |

---

## Time Complexity O(n²)

Attention의 계산 복잡도가 왜 O(n²)인지:

```
QK^T: (n × d_k) × (d_k × n) = n² 번의 곱셈
attn_weights @ V: (n × n) × (n × d_v) = n² 번의 곱셈
```

**메모리**: attention matrix가 (n × n) 크기이므로 O(n²) 메모리 필요

**실제 영향**:
- n = 512: 512² = 262K → 문제없음
- n = 4096: 4096² = 16.7M → 관리 가능
- n = 32768: 32768² = 1.07B → GPU 메모리 부족 시작
- n = 131072 (128K): 131072² = 17.2B → FlashAttention 등 필수

이것이 Long Context를 다루는 것이 어려운 근본 이유입니다.

---

## KV-Cache Mechanics

Autoregressive generation에서 매 step마다 전체 시퀀스에 대한 attention을 다시 계산하면 비효율적입니다. KV-cache는 이를 해결합니다.

### KV-Cache 없을 때

token을 n번째까지 생성했다면:
- Step n에서 attention 계산: O(n²) (모든 이전 token의 K, V를 다시 계산)
- 전체 생성 과정: O(n³) → **매우 비효율적**

### KV-Cache 있을 때

이전 step에서 계산한 Key, Value를 cache에 저장합니다:

```python
# Prefill 단계: 전체 prompt를 한 번에 처리
K_cache, V_cache = compute_KV(prompt)

# Decode 단계: 매 step마다 새 token의 K, V만 계산
new_K = compute_K(new_token)
new_V = compute_V(new_token)

K_cache = concat(K_cache, new_K)  # cache에 추가
V_cache = concat(V_cache, new_V)

# Attention: 새 query와 전체 K_cache로 계산
output = attention(new_Q, K_cache, V_cache)
```

**Decode 단계의 복잡도**: O(n) per step (n개의 cached K, V와 1개의 새 Q만 계산)

**메모리 트레이드오프**:
```
KV-cache 메모리 = 2 · n_layers · n_heads · d_head · seq_len · precision
```
예: LLaMA-3 70B, seq_len=8192, BF16 기준 ≈ 약 64GB

---

## Query/Key/Value Projection

Input hidden state `x` (d_model 차원)에서 Q, K, V를 생성하는 learned projection:

```
Q = x · W_Q    W_Q ∈ ℝ^(d_model × d_model)
K = x · W_K    W_K ∈ ℝ^(d_model × d_k)
V = x · W_V    W_V ∈ ℝ^(d_model × d_v)
```

Multi-head의 경우 각 head마다 별도의 projection이 있지만,
보통 하나의 큰 행렬로 통합하여 효율적으로 계산합니다:

```
# h heads, each with d_k = d_model / h
W_Q: (d_model, h * d_k) = (d_model, d_model)
```

**계산 순서** (실제 구현):
```
Q_full = x @ W_Q                          # (seq, d_model)
Q = Q_full.reshape(seq, h, d_k)           # per-head queries
Q = Q.transpose(1, 2)                     # (h, seq, d_k)
# K, V도 동일
```

attention 이후 output:
```
output = MultiHead(Q,K,V).reshape(seq, d_model)  # (h, seq, d_v) → (seq, d_model)
output = output @ W_O                             # final linear projection
```
