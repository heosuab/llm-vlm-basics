# 1.7 Alternative Architectures (Non-Transformer)

> Transformer의 O(n²) 복잡도 한계를 극복하기 위한 대안 아키텍처들.

---

## Why Non-Transformer?

Transformer의 근본적인 제약:

| 문제 | 원인 | 결과 |
|------|------|------|
| O(n²) 계산 복잡도 | n×n attention matrix | 긴 시퀀스에서 극도로 느림 |
| O(n²) 메모리 | KV-cache: O(n × d_model) | 매우 긴 시퀀스에서 GPU 메모리 초과 |
| 고정 context window | positional encoding 한계 | 무한 길이 처리 불가 |
| 추론 시 순차적 생성 | autoregressive decoding | decode 속도 병목 |

대안 아키텍처들은 이 문제들을 O(n) 복잡도로 해결하려 함.

---

## State Space Models (SSM)

### 개념

SSM은 연속 시간 동적 시스템(continuous-time dynamical system)에서 유래한 모델.

```
연속 시간:
  h'(t) = A·h(t) + B·x(t)   # 상태 업데이트
  y(t)  = C·h(t) + D·x(t)   # 출력

이산 시간 (실제 구현):
  h_t = Ā·h_{t-1} + B̄·x_t
  y_t = C·h_t
```

- h(t): hidden state (고정 크기 d_state)
- x(t): 입력
- y(t): 출력
- A, B, C: 학습 가능한 행렬

**핵심 특성**:
- 학습 시: parallel convolution으로 계산 (efficient)
- 추론 시: recurrent computation (O(1) per step, 고정 메모리)

### HiPPO Initialization

A 행렬을 랜덤이 아닌 수학적으로 의미있는 초기값으로 설정함.
"High-order Polynomial Projection Operators"에서 유래하며, 긴 range의 의존성을 포착하는 데 유리함.

---

## Mamba / Mamba-2 (Selective SSM)

2023년 Albert Gu, Tri Dao가 제안. 현재 가장 널리 사용되는 non-transformer 아키텍처.

### 기존 SSM의 문제

고전 SSM은 A, B, C가 입력과 무관한 고정값. → **모든 입력에 동일한 방식으로 반응**, 중요한 정보에 집중하지 못함

### Mamba의 혁신: Selective State Space

**A, B, C를 입력 x에 따라 동적으로 결정함:**

```
B_t = Linear(x_t)  ← 입력에 의존
C_t = Linear(x_t)  ← 입력에 의존
Δ_t = softplus(Linear(x_t))  ← step size도 입력에 의존
```

이를 통해 "관련 있는 정보는 기억하고, 관련 없는 정보는 잊는" 선택적 처리가 가능함.

### Parallel Scan (학습 효율성)

순차 RNN이지만, **associative scan** 알고리즘으로 병렬 계산 가능:

```
h_t = A_t · h_{t-1} + B_t · x_t

→ Parallel prefix scan으로 O(n log n) 또는 CUDA 최적화로 O(n) 달성
```

### Mamba Block 구조

```
Input x
  → [Linear projection × 2] → x_ssm, x_gate
  → x_ssm: [Conv1D → SSM (selective)] → y_ssm
  → y = y_ssm ⊙ SiLU(x_gate)  ← gating
  → Linear projection → output
```

### 복잡도 비교

| 작업 | Transformer | Mamba |
|------|-------------|-------|
| 학습 (시퀀스 처리) | O(n²) | O(n) |
| 추론 (per token) | O(n) (KV-cache 있어도) | O(1) |
| 추론 메모리 | O(n) (KV-cache) | O(d_state) (고정!) |

### Mamba-2

Mamba의 이론적 기반을 강화하고 더 큰 state dimension을 효율적으로 처리함.
Structured State Space Duality (SSD): SSM과 linear attention의 수학적 동등성을 보임.

### 한계

- In-context learning이 Transformer보다 약함 (attention의 exact copy 능력 부재)
- 매우 긴 context에서의 recall이 Transformer보다 떨어지는 경우 있음
- FlashAttention 같은 성숙한 최적화 인프라 아직 부족

---

## RWKV (Receptance Weighted Key Value)

Transformer와 RNN의 장점을 결합한 "RNN-Transformer hybrid" 모델.

### 핵심 구조

**Time-mixing** (attention 역할):
```
wkv_t = (Σ_{i≤t} e^{-(t-i)w+k_i} · v_i) / (Σ_{i≤t} e^{-(t-i)w+k_i})
output = σ(r_t) ⊙ wkv_t
```

- w: 위치에 따라 감소하는 decay (멀수록 적게 참고)
- r, k, v: input에서 projection된 Receptance, Key, Value

**Channel-mixing** (FFN 역할):
```
output = σ(r_t) ⊙ (W_V · GELU(W_K · x_t))
```

### 학습과 추론

- **학습 시**: 행렬 곱으로 병렬 계산 가능 (Transformer처럼)
- **추론 시**: 완전한 RNN으로 변환 → O(1) per step

### 특징

- RWKV-4 ~ RWKV-6까지 발전
- 14B 모델까지 오픈소스 공개
- 메모리 요구량이 매우 낮아 edge device에 적합

---

## Hybrid Architectures

순수 SSM 모델의 한계를 극복하기 위해 **SSM layer + Attention layer를 혼합**함.

### Jamba (AI21 Labs, 2024)

- Mamba + Transformer 레이어를 교차 배치
- Attention 레이어는 전체의 1/8 정도만 사용
- MoE도 결합 → Jamba: SSM + Attention + MoE

```
Layer 1: Mamba block
Layer 2: Mamba block
...
Layer 7: Mamba block
Layer 8: Transformer Attention block  ← 특정 위치에만
```

**장점**: SSM의 효율 + Attention의 정확한 recall 능력

### Zamba (Zyphra, 2024)

- Mamba-2와 Attention을 결합
- Shared attention block (여러 SSM layer가 하나의 attention 블록 공유)
- 1~7B 규모의 효율적인 모델

### Falcon Mamba (TII, 2024)

- 순수 Mamba 기반 7B 모델
- 같은 크기 Transformer와 경쟁적 성능

---

## 정리: 언제 어떤 아키텍처를?

| 상황 | 추천 |
|------|------|
| 일반 LLM (reasoning, QA) | Transformer (검증된 성능) |
| 매우 긴 시퀀스 (100K+) | Mamba or Hybrid |
| Edge/모바일 디바이스 | RWKV or Mamba (고정 메모리) |
| 실시간 스트리밍 | RWKV/Mamba (O(1) per step) |
| 연구/탐색 | Hybrid (두 장점 결합) |

Transformer가 여전히 지배적이지만, 긴 시퀀스와 효율적인 추론이 중요해지면서 non-transformer 아키텍처의 중요성이 계속 높아지고 있음.
