# 1.1 Transformer Block

> Transformer의 핵심 구성 요소들에 대한 Basics.

---

## Self-Attention

Self-attention은 시퀀스 내의 각 token이 다른 모든 token과 얼마나 관련 있는지를 계산하여, 그에 따라 정보를 모으는 메커니즘.

**직관**: "I saw the bank by the river" 에서 "bank"의 의미를 파악하려면 "river"를 봐야 하며, self-attention은 각 token이 필요한 context를 능동적으로 가져오게 하는 방법.

**계산 과정**:
```
Q = X · W_Q   (Query: 내가 찾고 싶은 것)
K = X · W_K   (Key: 내가 가진 정보의 레이블)
V = X · W_V   (Value: 실제 전달할 내용)

Attention(Q, K, V) = softmax( QK^T / √d_k ) · V
```

- Q와 K의 내적 → 각 token 쌍의 "관련도" (attention score)
- `/ √d_k` → 내적 값이 커지는 것을 방지 (scaling, 아래에서 설명)
- softmax → 합이 1이 되도록 정규화 (attention weight)
- `· V` → weight에 따라 value를 weighted sum

---

## Multi-Head Attention (MHA)

단일 attention으로는 "하나의 관점"만 포착할 수 있음. Multi-head attention은 여러 관점(head)에서 동시에 attention을 계산.

```
head_i = Attention(Q·W_Q^i, K·W_K^i, V·W_V^i)
MultiHead(Q,K,V) = Concat(head_1, ..., head_h) · W_O
```

**왜 여러 head가 필요한가?**
- head 1: 문법적 관계 (subject-verb agreement)
- head 2: 의미적 관계 (co-reference)
- head 3: 위치 관계 (proximity)

각 head는 d_k = d_model / h 차원으로 독립적으로 학습. 총 계산량은 single-head와 비슷하지만, 더 다양한 정보를 포착.

---

## Cross-Attention

Cross-attention은 두 개의 다른 시퀀스 사이에서 attention을 계산.
Self-attention이 같은 시퀀스 내에서 정보를 모은다면, cross-attention은 **다른 시퀀스에서** 정보를 가져옴.

```
Q = decoder_hidden · W_Q   (decoder의 현재 상태)
K = encoder_output · W_K   (encoder의 전체 출력)
V = encoder_output · W_V   (encoder의 전체 출력)
```

**사용처**: Encoder-decoder 구조(T5, BART)에서 decoder가 encoder 출력을 참조할 때 사용됨. 번역 모델에서 decoder의 각 token이 source sentence의 어떤 부분에 attention해야 하는지 계산.

---

## Residual Connections

```
output = LayerNorm(x + Sublayer(x))
```

입력 x를 그대로 출력에 더해줌. 이 "shortcut"이 없으면 gradient가 깊은 레이어를 거치면서 소실되기 때문.

**왜 중요한가?**
- Gradient highway: gradient가 residual connection을 통해 직접 흐를 수 있음
- Identity mapping: sublayer가 아무것도 안 해도 (zero output) x는 그대로 전달됨
- Depth scaling: 이 덕분에 100+ layer도 학습 가능

---

## LayerNorm vs RMSNorm

### Layer Normalization

각 token의 feature dimension에 걸쳐 정규화:

```
LayerNorm(x) = γ · (x - μ) / √(σ² + ε) + β
```
- μ: 평균, σ²: 분산 (feature dimension에 걸쳐 계산)
- γ, β: 학습 가능한 scale/shift 파라미터

### RMSNorm (Root Mean Square Normalization)

평균 제거 없이 RMS(Root Mean Square)로만 정규화:

```
RMSNorm(x) = γ · x / RMS(x)
RMS(x) = √(1/d · Σ xᵢ²)
```

**LayerNorm vs RMSNorm 비교**:
| 항목 | LayerNorm | RMSNorm |
|------|----------|---------|
| 연산 | 평균 + 분산 계산 | RMS만 계산 |
| 속도 | 상대적으로 느림 | ~10-15% 빠름 |
| 성능 | 거의 동등 | 거의 동등 |
| 사용 모델 | BERT, GPT-2 | LLaMA, Mistral, Qwen |

현대 LLM(LLaMA, Mistral, Gemma 등)은 거의 모두 RMSNorm을 사용.

---

## Pre-LN vs Post-LN

### Post-LN (원래 Transformer 구조)
```
x = LayerNorm(x + Attention(x))
x = LayerNorm(x + FFN(x))
```
LayerNorm이 sublayer **이후**에 옴. 깊은 네트워크에서 학습이 불안정해질 수 있음 (gradient 크기가 레이어마다 달라짐).

### Pre-LN (현대 표준)
```
x = x + Attention(LayerNorm(x))
x = x + FFN(LayerNorm(x))
```
LayerNorm이 sublayer **이전**에 옴.

**Pre-LN의 장점**:
- 학습 안정성이 훨씬 높음 (gradient가 residual path를 통해 깨끗하게 흐름)
- Warmup 없이도 학습 가능한 경우도 있음
- GPT-3 이후 대부분의 LLM이 Pre-LN 채택

---

## Feedforward Network (FFN)

Transformer block의 두 번째 sublayer. 각 token의 representation을 독립적으로 변환 (position-wise FFN).

```
FFN(x) = W₂ · activation(W₁ · x + b₁) + b₂
```

- W₁: d_model → d_ff (보통 4×d_model로 확장)
- W₂: d_ff → d_model (다시 축소)

**Expansion ratio (4×)의 이유**: attention이 "어디서 정보를 가져올지"를 결정한다면, FFN은 "그 정보를 어떻게 처리할지"를 담당. 충분한 capacity를 위해 넓게 확장했다가 다시 압축.

---

## SwiGLU / GeGLU

현대 LLM들은 일반 FFN 대신 **Gated Linear Unit(GLU)** 기반의 변형을 사용.

### SwiGLU (LLaMA, PaLM 등에서 사용)
```
SwiGLU(x) = (W₁x) ⊙ Swish(W₃x)
output = W₂ · SwiGLU(x)
```
- Swish(x) = x · σ(x) (sigmoid-weighted linear unit)
- ⊙: element-wise multiplication (gating)
- 게이트가 어떤 정보를 통과시킬지 학습함

### GeGLU
```
GeGLU(x) = (W₁x) ⊙ GELU(W₃x)
```
SwiGLU와 유사하나 activation으로 GELU를 사용.

**GLU 기반이 일반 FFN보다 좋은 이유**: 게이팅 메커니즘이 불필요한 정보를 억제하여 더 effective한 representation을 학습. 보통 W₁, W₂, W₃ 세 개의 행렬을 사용하므로, 동일 파라미터 수를 유지하려면 d_ff를 약 2/3로 줄임.

---

## Activation Functions

### GELU (Gaussian Error Linear Unit)
```
GELU(x) = x · Φ(x) ≈ x · σ(1.702x)
```
Φ(x)는 표준 정규분포의 CDF.
ReLU의 smooth한 근사로, 음수 입력에도 작은 gradient가 흐름. BERT, GPT-2에서 사용.

### SiLU (Sigmoid Linear Unit, = Swish)
```
SiLU(x) = x · σ(x)
```
Self-gated activation으로, LLaMA가 FFN의 gating에 사용.

---

## Dropout

학습 중 무작위로 일부 unit을 0으로 만들어 regularization 효과를 줌:

```
output = x · Bernoulli(1 - p) / (1 - p)  [training]
output = x                                  [inference]
```

**Transformer에서의 위치**:
- Attention weight 이후 (attention dropout)
- FFN 내부의 activation 이후
- Residual connection 이후

**LLM에서의 추세**: 대규모 LLM 사전학습에서는 dropout을 거의 사용하지 않는 추세. 데이터가 충분히 많고 training time이 짧아 overfitting이 문제가 되지 않기 때문다.

---

## Weight Tying

Input embedding matrix와 output projection(LM head)의 파라미터를 **공유**하는 기법.

```
logits = hidden_state · E^T   (E: embedding matrix)
```

**왜 작동하는가?**
- Input embedding: "이 token은 어떤 vector로 표현되는가"
- Output projection: "이 vector가 어떤 token에 가까운가"
두 역할이 본질적으로 같은 mapping이므로 공유해도 성능 저하가 없음.

**장점**:
- 파라미터 수 감소: vocab_size × d_model만큼 절약 (32K × 4096 ≈ 130M params)
- 학습 신호가 embedding에 두 배로 전달

GPT-2, BERT 등이 사용. LLaMA-3 같은 최신 모델들은 모델이 워낙 커서 weight tying을 사용하지 않는 경우도 있음.
