# 1.5 Tokenization

> 텍스트를 token 시퀀스로 변환하는 방법들과 각각의 특성들.

---

## BPE (Byte Pair Encoding)

GPT 계열(GPT-2, GPT-3, GPT-4, LLaMA)에서 사용하는 방법.

### 알고리즘

1. 모든 단어를 character 단위로 분리하고 각 character를 vocab에 추가
2. 가장 자주 등장하는 **인접한 두 symbol 쌍**을 하나로 merge
3. 2번을 vocab 크기가 목표에 달할 때까지 반복

**예시**:
```
초기:  "low low low lower lower newest newest widest"
      → l o w _, l o w e r _, n e w e s t _, w i d e s t _

Step 1: "e s" 쌍이 가장 빈번 → "es" 로 merge
      → l o w _, l o w e r _, n e w "es" t _, w i d "es" t _

Step 2: "es t" 쌍 merge → "est"
      ...

최종: "low", "lower", "newest", "widest" 각각 적절한 token들로 분리됨
```

**장점**: 자주 쓰이는 단어는 통으로 하나의 token, 드문 단어는 subword로 분리

---

## WordPiece

BERT에서 사용하는 방법. BPE와 유사하지만, merge 기준이 다름.

**BPE**: 빈도가 가장 높은 쌍을 merge
**WordPiece**: likelihood를 가장 많이 높이는 쌍을 merge

```
score(A, B) = count(AB) / (count(A) × count(B))
```

즉, A와 B가 개별적으로 많이 나오더라도 AB가 독립적으로 쓰일 때보다 더 자주 같이 나오면 merge함.

**특징**: 접두사가 아닌 subword 앞에 `##`를 붙임
```
"unaffordable" → "un", "##afford", "##able"
```

---

## SentencePiece

Google이 개발한 언어에 독립적인(language-agnostic) tokenizer. T5, LLaMA, Gemma 등에서 사용함.

**핵심 차이점**:
1. **공백 처리**: 텍스트를 공백으로 분리하지 않고 raw text를 직접 처리 → 언어에 독립적
2. **공백을 특수 문자(`▁`)로 표현**: "Hello world" → `▁Hello`, `▁world`
3. BPE 또는 Unigram 언어 모델 기반으로 학습

**장점**:
- 한국어, 중국어, 일본어 등 공백이 없는 언어에서도 잘 동작
- 동일한 코드로 모든 언어 처리 가능
- 텍스트 정규화를 tokenizer 내에 포함 가능

---

## Byte-Level Tokenization

모든 가능한 byte(0~255)를 기본 vocab으로 사용함. GPT-2가 도입했고 LLaMA-2까지 사용함.

```
vocab 시작: 256개의 byte token
+ BPE merge 반복 → 최종 vocab size (예: 50,257 for GPT-2)
```

**핵심 장점**: `<UNK>` token이 없음. 어떤 텍스트든(이모지, 특수문자, 코드 등) 반드시 표현 가능.

```
"Hello 🌍" → no UNK, 이모지도 byte 단위로 분리
```

---

## Vocabulary Size Tradeoffs

vocab 크기는 성능에 큰 영향을 미칩니다.

| 항목 | 작은 Vocab (32K) | 큰 Vocab (128K+) |
|------|-----------------|-----------------|
| Embedding 파라미터 | 작음 | 큼 |
| 단어당 평균 token 수 | 많음 | 적음 |
| 시퀀스 길이 | 길어짐 | 짧아짐 |
| 희귀어 처리 | subword로 분리 | 통으로 처리 가능 |
| 다국어 지원 | 각 언어 token 수 불균형 | 더 균형적 |

**트렌드**: 최신 모델들은 vocab을 키우는 추세
- GPT-2: 50,257
- LLaMA-2: 32,000
- LLaMA-3: 128,256 (코드, 다국어 강화)
- Qwen2: 151,936 (대규모 다국어)

Vocab이 크면 같은 텍스트를 더 적은 token으로 표현 → context window를 더 효율적으로 사용 → 긴 문서 처리에 유리함.

---

## Special Tokens Design

모델의 동작을 제어하는 특수 token들. 설계가 잘못되면 학습이 불안정해짐.

### 기본 Special Tokens

| Token | 역할 |
|-------|------|
| `<BOS>` / `<s>` | Beginning of Sequence — 시퀀스 시작 |
| `<EOS>` / `</s>` | End of Sequence — 생성 종료 신호 |
| `<PAD>` | Padding — 짧은 시퀀스를 채울 때 (attention mask로 무시) |
| `<UNK>` | Unknown — byte-level에서는 불필요 |
| `<SEP>` | Separator — BERT 등에서 두 시퀀스 구분 |

### Chat Template Special Tokens

Instruction-tuned 모델들은 대화 구조를 위한 special token을 사용함.

**LLaMA-2 Chat 형식**:
```
<s>[INST] System: You are helpful. [/INST]
User: Hello [INST] Assistant: Hi there! [/INST]
```

**LLaMA-3 형식**:
```
<|begin_of_text|>
<|start_header_id|>system<|end_header_id|>
You are helpful.
<|eot_id|>
<|start_header_id|>user<|end_header_id|>
Hello
<|eot_id|>
<|start_header_id|>assistant<|end_header_id|>
```

**ChatML 형식** (GPT-4, Qwen 등):
```
<|im_start|>system
You are helpful.<|im_end|>
<|im_start|>user
Hello<|im_end|>
<|im_start|>assistant
```

### SFT에서의 Loss Masking

Fine-tuning 시 user turn의 token에는 loss를 계산하지 않고, **assistant 응답 부분에만** loss를 계산함:

```python
# user message: loss_mask = 0 (gradient 없음)
# assistant response: loss_mask = 1 (gradient 흐름)
loss = cross_entropy(logits, labels, reduction='none') * loss_mask
```

이렇게 해야 모델이 "user처럼 말하는 것"이 아니라 "assistant처럼 응답하는 것"을 학습함.
