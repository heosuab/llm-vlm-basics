# 1.6 Model Architecture Types

> LLM의 대표적인 아키텍처 유형들과 각각의 특성, 적합한 사용 사례.

---

## Encoder-Only

**대표 모델**: BERT, RoBERTa, DeBERTa, ELECTRA

### 구조

입력 시퀀스 전체를 **양방향(bidirectional)**으로 처리함.
모든 token이 왼쪽과 오른쪽 모두를 볼 수 있음.

```
Input:  [CLS] The dog bit the man [SEP]
           ↕    ↕   ↕   ↕   ↕   ↕
Output: h_CLS h_The h_dog h_bit h_the h_man
```

각 token의 output이 해당 위치의 문맥 표현(contextual representation).

### Pretraining: Masked Language Modeling (MLM)

```
Input:  "The [MASK] sat on the mat"
Target: "The cat sat on the mat"
```

전체 token의 ~15%를 랜덤으로 masking하고 예측함.
양방향 context를 활용하여 더 풍부한 표현 학습이 가능함.

### 적합한 Task

- 텍스트 분류 (sentiment, topic)
- Named Entity Recognition (NER)
- Question Answering (extractive)
- Semantic similarity, entailment
- Embedding 생성

### 한계

**생성 불가**: autoregressive generation에 적합하지 않음.
LLM의 주류가 decoder-only로 넘어가면서 사용이 줄었지만, 임베딩/분류 task에서는 여전히 활용됨.

---

## Decoder-Only

**대표 모델**: GPT 계열, LLaMA, Mistral, Qwen, Gemma, Claude

### 구조

Causal (left-to-right) attention만 사용함.
각 token은 자신과 이전 token들만 볼 수 있음.

```
Input:  The dog bit
Predict:    dog bit the
                bit the man
```

### Pretraining: Causal Language Modeling (CLM)

```
input:  ["The", "dog", "bit", "the"]
labels: ["dog",  "bit", "the", "man"]
```

모든 position에서 next token을 예측함.
하나의 시퀀스가 n개의 학습 예시를 제공함 → 데이터 효율적

### 왜 Decoder-Only가 LLM의 표준이 되었는가?

1. **Generation에 자연스러움**: autoregressive하게 token을 생성
2. **In-context learning**: few-shot examples를 자연스럽게 처리
3. **Scalability**: 단순한 구조로 대규모 학습이 안정적
4. **Flexibility**: generation, classification, embedding 모두 가능

---

## Encoder-Decoder

**대표 모델**: T5, BART, mT5, UL2

### 구조

Encoder가 입력을 처리하고, Decoder가 출력을 생성함.
Decoder는 encoder output을 cross-attention으로 참조함.

```
Encoder: 입력 전체를 양방향으로 처리 → context representation
           ↓ cross-attention
Decoder: 출력을 autoregressive하게 생성
```

### Pretraining

**T5의 Span Corruption**:
```
Input:  "The X sat Y mat"  (X, Y는 sentinel token)
Target: "<X> cat <Y> on the <Z>"
```
연속된 span을 masking하고 sentinel token으로 대체, decoder가 원래 내용 복원.

**BART**: Encoder는 손상된 텍스트, Decoder는 원본 복원

### 적합한 Task

- Machine Translation
- Summarization
- Question Generation
- Dialogue Systems

### 현재 위치

LLM 시대에서는 GPT 계열에 밀렸지만, 번역·요약 등 특정 seq2seq task에서는 여전히 강점이 있음. T5는 instruction tuning 연구(FLAN-T5)에 많이 활용됨.

---

## Prefix LM

**대표 모델**: PaLM, GLM, UniLM

### 구조

입력의 "prefix" 부분은 **양방향** attention, 이후 생성 부분은 **단방향** attention으로 처리함.

```
Prefix (양방향):  "Translate to French:"
Continuation (단방향): "The cat sat on the mat" → autoregressive generation
```

Prefix 내에서는 모든 token이 서로를 볼 수 있어 더 풍부한 표현을 얻을 수 있고, 이후 생성 부분은 일반 decoder처럼 작동함.

**장점**: Encoder-Decoder의 풍부한 입력 처리 + Decoder의 생성 능력을 동시에
**단점**: 두 mode의 경계 처리가 복잡, 성능상 이점이 항상 명확하지 않음

---

## Mixture-of-Experts (MoE)

**대표 모델**: Mixtral, DeepSeek-V2/V3, Qwen2-MoE, Switch Transformer

### 핵심 아이디어

FFN 레이어를 여러 개의 "expert" FFN으로 교체하고, 각 token에 대해 **상위 k개의 expert만 활성화**함.

```
일반 FFN:
  output = FFN(x)      # 항상 같은 파라미터 사용

MoE FFN:
  gate_scores = softmax(x · W_gate)   # expert 선택 점수
  top_k_experts = topk(gate_scores, k=2)  # 상위 2개 선택
  output = Σᵢ gate_scores[i] · Expert_i(x)  # weighted sum
```

### Total vs Active Parameters

```
Mixtral 8x7B:
  Total parameters:  46.7B (8개 expert × 각 7B)
  Active parameters: 12.9B (각 token마다 2개 expert만 사용)
```

같은 추론 비용으로 4배 더 많은 파라미터를 갖는 셈.
→ "MoE는 학습 비용은 작게, 파라미터는 많게"

---

## Routing Mechanisms

어떤 expert를 선택할지 결정하는 메커니즘.

### Top-k Gating

```python
gate_logits = x @ W_gate      # (batch, seq, num_experts)
gate_probs = softmax(gate_logits)
top_k_probs, top_k_indices = topk(gate_probs, k=2)
top_k_probs = top_k_probs / top_k_probs.sum(dim=-1)  # normalize
```

k=1: 항상 1개 expert (완전 sparse), 효율적이지만 gradient 불안정
k=2: 2개 expert weighted sum, 현재 표준

### Load Balancing Loss

특정 expert에만 token이 몰리는 "expert collapse" 현상을 방지함:

```
L_balance = α · Σᵢ f_i · p_i
```
- f_i: expert i에 실제로 routing된 token 비율
- p_i: router가 expert i를 선택한 평균 확률
- α: balancing loss weight

이 loss를 메인 loss에 더해서 모든 expert가 균등하게 사용되도록 유도함.

---

## Capacity Factor

각 expert가 처리할 수 있는 최대 token 수를 제한하는 파라미터.

```
capacity = (num_tokens / num_experts) × capacity_factor
```

**capacity_factor = 1.0**: 완벽히 균등 분배될 때 딱 맞음
**capacity_factor = 1.25**: 조금 여유를 둠 (불균등 허용)

capacity를 초과한 token은 처리 못하고 그냥 통과되거나(dropped), 다른 expert로 보내집니다.
Capacity factor가 너무 작으면 token drop이 많이 발생하고, 너무 크면 메모리 낭비가 됨.

**DeepSeek-V2의 혁신**: Top-k routing에서 expert group을 만들어 inter-node communication을 최소화하는 효율적인 MoE routing을 도입함.
