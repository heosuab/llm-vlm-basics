# 10.2 VLM Benchmarks

> Vision-LLM의 능력을 평가하는 주요 벤치마크들.

---

## MMMU (Massive Multitask Multimodal Understanding)

### 개요

MMLU의 multimodal 버전. 대학교 수준의 학문적 질문에 이미지가 포함됨.

**포함 과목**: 예술, 비즈니스, 의학, 공학, 사회과학 등 30개 과목
**이미지 유형**: 도표, 그래프, 수식, 의료 이미지, 지도, 화학 구조식 등

```
Example:
  [Image: ECG 그래프]
  Q: "What cardiac rhythm is shown in this ECG?"
  (A) Normal sinus rhythm
  (B) Atrial fibrillation
  (C) Ventricular tachycardia
  (D) Complete heart block
```

**특징**: 단순 이미지 인식이 아닌 **domain knowledge + visual reasoning** 조합이 필요함.

**현재 SOTA**: ~75-80% (GPT-4o, Gemini Ultra, Claude 3.5)

---

## MMBench & MMStar

### MMBench

VLM의 능력을 20개 세부 항목으로 나눠 평가함:

- Object Recognition, Object Localization
- Attribute Reasoning (색깔, 크기, 모양)
- Scene Understanding
- Spatial Reasoning (위/아래/왼/오른)
- Commonsense Reasoning

**형식**: 4-choice multiple choice
**특징**: 능력별 breakdown이 제공되어 모델의 강약점 분석에 유용함.

### MMStar

**오염 방지** 설계 원구로, 이미지 없이 텍스트만으로도 답할 수 있는 문제는 제외함:

```
오염된 문제 예시 (제외):
  [이미지 없어도 답 가능]: "In the image, what color is typically associated with stop signs?"
  → 이미지 없이도 "Red"라고 답할 수 있음 → 제외

진정한 visual reasoning 문제:
  [이미지의 특정 세부사항 필요]: "How many red objects are in the second row?"
  → 이미지를 봐야만 답할 수 있음 → 포함
```

---

## TextVQA & DocVQA

### TextVQA

**이미지 내의 텍스트를 읽고 이해**하는 능력을 테스트함:

```
[이미지: 간판이 있는 거리 사진]
Q: "What is the name of the store on the corner?"
A: "Springfield Hardware"
```

OCR(Optical Character Recognition)과 visual reasoning을 조합함.

**현재 SOTA**: 80%+ (GPT-4V, Qwen2-VL 등)

### DocVQA

**문서 이미지**(스캔된 문서, 양식, 인보이스 등)에서 정보를 추출함:

```
[이미지: 스캔된 청구서]
Q: "What is the total amount due?"
A: "$1,234.56"
```

**도전 요소**:
- 다양한 폰트와 레이아웃
- 표, 체크박스, 서명 등
- 손으로 쓴 텍스트

**현재 SOTA**: 90%+ ANLS (Average Normalized Levenshtein Similarity)

---

## ChartQA

**차트와 그래프를 이해**하고 질문에 답함:

```
[이미지: 막대 그래프 — 연도별 전기차 판매량]
Q: "Which year had the highest EV sales?"
A: "2023"

Q: "By what percentage did sales increase from 2021 to 2023?"
A: "47.3%"  ← 차트에서 값을 읽고 계산해야 함
```

**유형**:
- **Extractive**: 차트에 있는 값 직접 읽기
- **Reasoning**: 여러 값을 조합하거나 계산

**평가 지표**: Relaxed accuracy (숫자는 5% 오차 허용)

---

## POPE (Polling-based Object Probing Evaluation)

### 목적

VLM의 **Object Hallucination**을 평가함.

### 방법

이미지에 **없는 물체**에 대해 "이 이미지에 X가 있나요?" 라고 물음:

```
[이미지: 사과와 바나나가 있는 테이블]

Q: "Is there a dog in the image?"  → 정답: No
Q: "Is there an apple in the image?" → 정답: Yes

모델이 없는 물체를 있다고 하면 hallucination
```

### 세 가지 세팅

- **Random**: 랜덤하게 없는 물체 선택
- **Popular**: 자주 등장하는 물체 선택 (language prior가 강함)
- **Adversarial**: 이미지에 있는 물체와 공존하기 쉬운 물체 선택

**의의**: POPE 점수가 낮으면 모델이 이미지를 제대로 보지 않고 언어 prior에 의존한다는 의미.

---

## VLM 벤치마크 비교

| 벤치마크 | 평가 능력 | 난이도 | 주요 특징 |
|---------|---------|------|---------|
| MMMU | 학문적 추론 | 높음 | domain knowledge 필요 |
| MMBench | 종합 VQA | 중간 | 항목별 breakdown |
| MMStar | 종합 VQA | 중간 | 오염 방지 설계 |
| TextVQA | OCR + VQA | 중간 | 이미지 내 텍스트 이해 |
| DocVQA | 문서 이해 | 높음 | 복잡한 레이아웃 |
| ChartQA | 차트 이해 | 중간 | 수치 추출 + 계산 |
| POPE | Hallucination | 낮음 | 환각 평가 특화 |
