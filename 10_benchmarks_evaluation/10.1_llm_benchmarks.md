# 10.1 LLM Benchmarks

> LLM의 능력을 평가하는 주요 벤치마크들을 소개합니다. 각 벤치마크가 무엇을 측정하고, 어떻게 설계되어 있는지를 이해하는 것이 중요합니다.

---

## MMLU (Massive Multitask Language Understanding)

### 개요

57개 academic subject에 걸친 4-choice multiple choice 문제입니다.

**포함 과목**: 고등학교/대학교 수준의 수학, 과학, 법학, 의학, 인문학, 사회과학 등

### 포맷

```
Question: What is the primary function of the mitochondria?
(A) Protein synthesis
(B) Cellular respiration and ATP production
(C) DNA replication
(D) Lipid storage

Answer: (B)
```

### 중요성

- GPT-4 출시 시 주요 성능 지표로 사용
- 언어 모델의 world knowledge와 reasoning의 proxy
- **한계**: 암기(memorization)를 측정하는 측면이 강함, 포화(saturation) 시작

**현재 SOTA**: 90%+ (GPT-4o, Claude 3.5 Sonnet, Gemini Ultra 등)

---

## HellaSwag & WinoGrande

### HellaSwag (Commonsense NLI)

주어진 상황의 자연스러운 다음 문장을 고릅니다:

```
Context: "She poured water into the pot and put it on the stove."
(A) She then added salt and pasta to cook dinner.      ← 정답
(B) She then drove to the grocery store.
(C) She folded the laundry and put it away.
(D) She painted the walls blue.
```

물리적 상식, 일상 행동 시퀀스에 대한 이해를 측정합니다.

### WinoGrande (Winograd schema)

대명사 참조 해석을 통해 상식 추론을 테스트합니다:

```
"The trophy didn't fit in the suitcase because it was too [large/small]."
"it"이 무엇을 가리키는가?
→ large: trophy / small: suitcase
```

GPT-4 등 대형 모델은 두 벤치마크 모두 거의 saturation 상태입니다.

---

## HumanEval & MBPP

### HumanEval (OpenAI)

164개의 Python 코딩 문제입니다. 함수 docstring이 주어지고 구현을 작성합니다.

```python
def has_close_elements(numbers: List[float], threshold: float) -> bool:
    """ Check if in given list of numbers, are any two numbers closer to each
    other than given threshold.
    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)
    False
    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)
    True
    """
    # 모델이 구현해야 함
```

**평가 지표**: pass@k — k번의 시도 중 적어도 한 번 통과하는 비율

### MBPP (Mostly Basic Python Problems)

974개의 crowdsourced Python 프로그래밍 문제입니다. 함수 설명 + input/output 예시가 주어집니다.

**현재 SOTA**: pass@1 기준 90%+ (GPT-4o, Claude 3.5 Sonnet)

---

## GSM8K & MATH

### GSM8K (Grade School Math)

초등학교 수준의 수학 word problem입니다 (8,500개).

```
Question: Janet's ducks lay 16 eggs per day. She eats 3 for breakfast every
morning and bakes muffins for her friends every day with 4. She sells the
remainder at the farmers' market daily for $2 per fresh duck egg. How much
in dollars does she make every day at the farmers' market?

Answer: 16 - 3 - 4 = 9 eggs remaining
9 × $2 = $18
```

Chain-of-Thought 능력의 핵심 벤치마크였으나, 이제 거의 포화 상태입니다.
**현재 SOTA**: ~97% (거의 모든 frontier model)

### MATH (Hendrycks et al.)

고등학교~대학교 수준의 수학 문제입니다 (12,500개).
대수학, 기하학, 정수론, 미적분학, 확률 등을 포함합니다.

```
Problem: Find all real solutions to x^4 - 2x^3 - x^2 + 2x + 1 = 0.

Solution: [LaTeX-format detailed solution]
```

**난이도**: 5단계 (1-5), 레벨 5는 수학 올림피아드 수준

**현재 SOTA**: ~90% (GPT-4o, o1, DeepSeek-R1)
Reasoning 모델(o1, R1)의 등장으로 급격히 향상되었습니다.

---

## GPQA (Graduate-Level Google-Proof Q&A)

### 개요

박사 과정 수준의 생물학, 화학, 물리학 문제입니다. "Google-proof" — 구글로 검색해도 쉽게 찾을 수 없는 심화 문제입니다.

```
Difficulty: 전문가도 어려운 문제
Expert accuracy: ~65% (해당 분야 전문가)
GPT-4 accuracy: ~39% (2023 기준)
o1 accuracy: ~78% (2024, 전문가 수준 초과)
```

o1, o3, DeepSeek-R1 같은 reasoning 모델의 등장으로 benchmark의 의미가 바뀌고 있습니다.

---

## LiveCodeBench (Contamination-free Coding)

### 동기

HumanEval, MBPP는 학습 데이터에 포함될 가능성이 높습니다 (benchmark contamination).

### 방법

**지속적으로 업데이트**되는 코딩 문제를 사용합니다:
- LeetCode, Codeforces, AtCoder에서 최근 출제된 문제
- 모델 학습 이후에 나온 문제만 사용
- Knowledge cutoff 이후 문제 → 오염 불가능

**평가 항목**:
- Code generation
- Test output prediction
- Code repair (버그 수정)

**의미**: 공정한 코딩 능력 평가의 새로운 표준이 되고 있습니다.

---

## 벤치마크 선택 기준

| 목적 | 추천 벤치마크 |
|------|-------------|
| 일반 지식/추론 | MMLU, GPQA |
| 수학 능력 | MATH, AIME (olympiad) |
| 코딩 능력 | HumanEval, LiveCodeBench |
| 상식 | HellaSwag, WinoGrande |
| 포괄적 평가 | Open LLM Leaderboard (MMLU + HellaSwag + TruthfulQA + ...) |
