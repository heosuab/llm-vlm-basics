# 10.4 Evaluation Issues

> 벤치마크를 올바르게 해석하기 위해 알아야 할 중요한 문제들.

---

## Benchmark Contamination (Data Contamination)

### 문제

모델의 학습 데이터에 벤치마크의 test set이 포함되어 있으면, 모델이 답을 "암기"한 것인지 "이해"한 것인지 구분할 수 없음.

```
벤치마크: MMLU question "What is the capital of France?"
학습 데이터에 포함: "Q: What is the capital of France? A: Paris."

→ 모델이 reasoning으로 맞춘 것인가? 암기한 것인가?
```

**특히 문제가 되는 곳**:
- 웹에서 수집한 데이터(Common Crawl): 많은 벤치마크 문제가 웹에 공개되어 있음
- GitHub: HumanEval 문제들이 GitHub에 indexed
- Reddit, Stack Overflow: 많은 QA 데이터가 공개

### 탐지 방법

**n-gram overlap 체크**: 학습 데이터와 벤치마크 문제 사이의 n-gram 겹침 계산
**min-k% prob**: 모델이 test example을 훈련 데이터에서 본 적 있는지 확률로 추정
**GPT-4 prompt**: "이 문제를 본 적 있는가?" 직접 질문

### 완화 방법

- **Time-based filtering**: 모델 학습 데이터 cutoff 이후의 문제만 사용 (LiveCodeBench)
- **Procedurally generated**: 프로그램으로 매번 새로운 문제 생성
- **Private test set**: test set을 공개하지 않음 (제출 기반 평가)
- **Multiple benchmark**: 여러 벤치마크에서 일관된 결과 확인

---

## Evaluation Harness

### lm-evaluation-harness (EleutherAI)

가장 널리 사용되는 LLM evaluation 프레임워크.

**특징**:
- 60개 이상의 벤치마크 지원
- 표준화된 평가 방법 제공
- 재현 가능한(reproducible) 결과
- HuggingFace models와 통합

```bash
# 사용 예시
lm_eval --model hf \
  --model_args pretrained=meta-llama/Llama-3.1-8B-Instruct \
  --tasks mmlu,hellaswag,gsm8k \
  --batch_size 8
```

**중요성**: 같은 모델도 평가 방법(shot 수, prompt 형식, normalization)에 따라 점수가 다르게 나옴. 표준 harness를 사용해야 비교가 공정함.

### 평가 방법 차이의 영향

```
같은 LLaMA-3 8B, 다른 평가 설정:
  0-shot MMLU: 62.3%
  5-shot MMLU: 66.6%  ← 동일 모델, 4%p 차이!

→ shot 수, prompt 형식을 명시하지 않으면 비교 불가
```

---

## LLM-as-a-Judge의 한계

LLM을 사용하여 다른 LLM의 출력을 평가하는 방법. 비용이 낮고 확장 가능하지만, 여러 편향이 있음.

### Position Bias (순서 편향)

같은 두 응답이어도 A/B 순서에 따라 다르게 판단함:

```
[Response A first]: 70% of the time selects A
[Response B first]: 70% of the time selects B

동일한 두 응답인데 순서에 따라 결과가 달라짐!
```

**완화**: 두 가지 순서로 평가하고 평균 냄

### Verbosity Bias (길이 편향)

더 긴 응답을 더 좋다고 판단하는 경향이 있음:

```
짧고 정확한 답 vs 길지만 중복된 답
→ LLM judge가 긴 답을 선호하는 경우 많음
```

### Self-Enhancement Bias (자기 선호)

모델이 자신이 생성한 스타일의 응답을 선호함:

```
GPT-4가 judge할 때 → GPT-4 스타일 응답 선호
Claude가 judge할 때 → Claude 스타일 응답 선호
```

**완화**: 여러 모델을 judge로 사용, ensemble

### Sycophancy (아첨)

사용자가 이미 의견을 갖고 있으면 그에 동조하는 경향:

```
"I think Response A is better. Which do you prefer?"
→ LLM이 A를 선호한다고 답할 가능성 높음 (진짜 비교 없이)
```

---

## Benchmark Saturation 문제

### 포화의 의미

모델들이 벤치마크의 "ceiling"에 근접하여, 더 이상 성능 차이가 구분되지 않음.

```
2023년 MMLU:
  GPT-3.5: 70%
  GPT-4:   86%
  → 명확한 차이

2025년 MMLU:
  GPT-4o:     87%
  Claude 3.5: 88%
  Gemini Pro: 87%
  → 차이가 거의 없음 → 이 벤치마크로는 모델 구분 불가
```

### 결과

- 연구자들이 더 어려운 벤치마크를 만들어야 함
- MMLU → MMLU-Pro (더 어려운 문제)
- GSM8K → MATH → AIME (점점 어렵게)
- 벤치마크 생명주기가 짧아짐

### AIME 등 Hard Benchmarks

AIME (American Invitational Mathematics Examination): 미국 수학 올림피아드 예선

```
2024 기준:
  GPT-4: 2/30 (random에 가까움)
  o1: 12/30 → 18/30 (re-ranking)
  o3: 25.2/30 (전문 수학자 수준)
```

frontier model의 새 기준이 되고 있음.

---

## 좋은 평가 설계 원칙

Research engineer로서 자체 평가를 설계할 때:

1. **오염 확인**: 사용하는 데이터가 학습에 포함되지 않았는지 확인
2. **재현성**: 동일 조건에서 여러 번 실행해도 같은 결과가 나오는지
3. **Statistical significance**: 충분한 샘플 수, confidence interval 보고
4. **Multiple metrics**: 하나의 수치에 의존하지 말고 여러 관점에서 평가
5. **Evaluation harness 사용**: 표준 프레임워크로 비교 가능성 확보
6. **Ablation과 분리**: 무엇이 성능에 기여하는지 구분하는 ablation 설계
