# 10.3 Long-Context Benchmarks

> 모델이 긴 입력을 제대로 처리하는지 평가하는 벤치마크들입니다.

---

## NIAH (Needle-In-A-Haystack)

### 개념

매우 긴 텍스트("haystack") 중간에 숨겨진 짧은 정보("needle")를 찾아내는 능력을 테스트합니다.

### 설계

```
Haystack: 랜덤하게 선택된 긴 텍스트 (예: Paul Graham 에세이들)
Needle: 임의로 삽입된 하나의 사실
  예: "The best pizza in New York is at Domino's on 42nd Street."

Needle 위치: 텍스트의 어디서든 (처음, 중간, 끝)
Context length: 1K ~ 128K tokens

Question: "What is the best pizza in New York?"
Expected: "Domino's on 42nd Street"
```

### 평가 방법

- X축: Context length (1K, 4K, 8K, 16K, ... 128K)
- Y축: Needle position (0%=처음, 50%=중간, 100%=끝)
- Z축(색상): Accuracy

좋은 모델은 **모든 context length, 모든 위치에서 100%** 가까운 점수를 냅니다.

### 발견

초기 long-context 모델들에서 **"Lost in the Middle"** 현상이 발견되었습니다:
- 처음과 끝 부분에 needle이 있으면 잘 찾음
- **중간 부분에 needle이 있으면 자주 놓침**

이후 모델 개선(Gemini 1.5, LLaMA-3.1 등)으로 대부분의 위치에서 높은 성능을 보입니다.

### Multi-Needle NIAH

하나가 아닌 여러 개의 needle을 동시에 찾는 더 어려운 버전입니다.

---

## RULER

### 동기

NIAH는 단순 recall만 테스트합니다. **RULER (Realistic Understanding for Long-context Evaluation Resources)**는 더 다양하고 현실적인 long-context 능력을 평가합니다.

### 테스트 항목

**1. NIAH 변형들**:
- Single NIAH (기본)
- Multi-key NIAH (여러 다른 needle들)
- Multi-value NIAH (같은 key, 여러 value)
- Multi-query NIAH (여러 질문)

**2. Variable Tracking (VT)**:
긴 텍스트에서 변수가 여러 번 업데이트될 때 최종 값을 추적합니다:

```
"x = 5" ... (많은 텍스트) ... "x = 12" ... (많은 텍스트) ... "x = 8"
Question: "What is the final value of x?"
Answer: 8  ← 가장 최근 값
```

**3. Common Words Extraction (CWE)**:
긴 텍스트에서 가장 자주 등장하는 단어들을 찾습니다 — 통계적 집계 능력 테스트.

**4. Frequent Words Extraction (FWE)**:
여러 목록에서 공통으로 등장하는 항목들을 찾습니다.

**5. Question Answering (QA)**:
여러 문서에 분산된 정보를 종합하여 답변합니다 (multi-hop reasoning).

### 의의

RULER는 모델이 long context를 **실제로 처리하는지** (진짜 long-context understanding) vs **단순히 처음/끝을 보는지**를 구분합니다.

---

## Other Long-Context Evaluations

### LongBench

실제 downstream task들에 대한 long-context 평가입니다:
- Single/Multi-document QA
- Summarization (긴 문서)
- Few-shot learning (긴 context 내의 예시들)
- Code completion (긴 코드베이스)

Chinese + English 이중 언어 버전 존재.

### SCROLLS

긴 문서에 대한 NLP task 모음:
- Summarization (GovReport, SummScreen, QMSum)
- QA (NarrativeQA, QASPER)
- NLI (ContractNLI)

### Infinite Bench

100K+ token의 극단적으로 긴 context에 대한 평가입니다. 기존 벤치마크가 포화될 때 더 어려운 테스트를 제공합니다.

---

## 결론: Long-Context 평가의 주의점

```
NIAH 100% ≠ 진정한 long-context 이해

모델이 NIAH를 쉽게 통과해도:
1. 긴 문서에서의 복잡한 추론은 여전히 어려울 수 있음
2. 여러 문서에서 정보를 종합하는 것은 다른 능력
3. Lost-in-the-Middle이 더 복잡한 task에서는 여전히 문제일 수 있음
```

Long-context 능력은 **용량**(얼마나 긴 context를 처리할 수 있는가)과 **품질**(그 context를 얼마나 잘 이해하고 활용하는가) 두 가지를 모두 고려해야 합니다.
