# Section 6.1: Reasoning Techniques

> **학습 목표**: LLM이 복잡한 수학/논리/사고 문제를 더 잘 풀 수 있도록 만드는 다양한 prompting 기법들.

---

## 6.1.1 Chain-of-Thought (CoT)

### 핵심 직관: "Scratchpad(계산용 종이)" 은유

수학 시험을 풀 때, 머릿속으로만 계산하는 것보다 **과정을 종이에 써가며 계산하면** 훨씬 정확함. CoT는 LLM에게 이 "계산 종이"를 제공하는 아이디어.

CoT 이전 (Direct Prompting):
```
질문: Roger는 5개의 테니스 공을 가지고 있다.
      그는 2캔의 테니스 공을 더 샀다. 각 캔에는 3개의 공이 들어있다.
      Roger는 총 몇 개의 테니스 공을 가지고 있는가?

모델 출력: 11
```

CoT 이후:
```
질문: [위와 동일]

모델 출력:
Roger는 처음에 5개의 테니스 공을 가지고 있다.
2캔을 샀고, 각 캔에는 3개의 공이 있으므로 2 × 3 = 6개를 새로 샀다.
따라서 총 5 + 6 = 11개의 테니스 공을 가지고 있다.
답: 11
```

표면상 최종 답은 같지만, 복잡한 문제에서는 **중간 추론 단계가 정답률을 극적으로 향상**시킴.

Google의 2022년 논문 "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models" (Wei et al.)에서 처음 체계적으로 연구됨.

### Why CoT Works: 이론적 이유

1. **중간 계산 오류 방지**: 복잡한 계산을 단계별로 나누면 각 단계의 오류를 분리할 수 있음
2. **Decomposition**: 복잡한 문제를 서브 문제들로 분해할 기회를 제공
3. **Attention 분산**: 관련 정보에 순차적으로 집중할 수 있게 함
4. **검증 가능성**: 중간 단계를 사람이 검토하고 오류를 발견할 수 있음
5. **Token Budget**: 더 많은 토큰을 "생각"에 사용할 수 있음 → 모델의 표현력 최대 활용

수학적으로 보면, Transformer는 **constant-depth** 계산 그래프. 직접 답을 내려면 단 몇 번의 attention layer 만으로 복잡한 계산을 해야 하지만, CoT를 통해 여러 forward pass에 해당하는 계산을 **sequential하게 토큰으로 펼칠 수** 있음.

### Few-Shot CoT

몇 가지 예시(demonstrations)와 함께 풀이 과정을 보여준다:

```
Q: 15개의 나무가 있다. 조경사가 나무를 더 심어 21개가 되었다.
   조경사는 몇 그루를 심었나?
A: 처음 15개, 나중에 21개가 되었으므로 21 - 15 = 6그루를 심었다.
   답: 6

Q: 주차장에 3개의 차가 있었다. 5개가 더 왔다. 주차장에는 몇 대가 있나?
A: 처음 3대, 5대가 더 오면 3 + 5 = 8대가 된다.
   답: 8

Q: [실제 질문]
A: [모델이 위 형식에 따라 단계별 풀이 생성]
```

### Zero-Shot CoT

단 한 가지 마법 같은 문구:

```
질문: [어려운 문제]

"Let's think step by step."  ← 이 한 마디가 CoT를 유발
```

Kojima et al. (2022)의 발견: "Let's think step by step"이라는 단순한 지시만으로도 few-shot CoT에 준하는 성능을 낼 수 있음.

왜 이게 작동하는가? 학습 데이터에서 "Let's think step by step" 이후에는 단계별 추론 내용이 오는 패턴이 많았기 때문에, 모델이 이 패턴을 학습한 것으로 추정됨.

```python
# Zero-shot CoT 구현 예시
def zero_shot_cot(question, llm):
    # 1단계: 추론 생성
    reasoning_prompt = f"{question}\n\nLet's think step by step."
    reasoning = llm.generate(reasoning_prompt)

    # 2단계: 최종 답 추출
    answer_prompt = f"{question}\n\n{reasoning}\n\nTherefore, the answer is:"
    answer = llm.generate(answer_prompt)

    return reasoning, answer
```

### CoT 성능에 대한 규모 의존성

중요한 관찰: CoT는 **충분히 큰 모델** (~100B 파라미터 이상)에서만 효과적. 작은 모델에서는 CoT가 오히려 성능을 낮출 수 있음.

```
모델 크기 vs GSM8K 정확도 (예시):

                        CoT prompting ───────────────────────●
성능                                                   ●
↑                                               ●
|                                        ●
|                                 ●
|                          ●
|   Standard ──────────────────────────●─────────────●──●
|
+────────────────────────────────────────────────────────→
   1B          10B          100B        500B         1T
```

이 "emergence"는 대형 모델만이 충분히 복잡한 추론 체인을 생성/평가할 수 있기 때문.

---

## 6.1.2 Self-Consistency

### 문제: 단일 CoT의 취약성

CoT를 greedy decoding (temperature=0)으로 실행하면, 하나의 추론 경로만 생성됨. 이 경로에 오류가 있으면 틀린 답이 나옴.

```
질문: [수학 문제]
CoT 경로 1 (greedy): ... → 잘못된 중간 단계 → 틀린 답 "42"
```

### Self-Consistency의 아이디어

**여러 개의 다양한 추론 경로를 샘플링하고, 최종 답에 대해 다수결 투표**를 한다:

```
질문: [수학 문제]

Temperature > 0으로 N번 샘플링:
CoT 경로 1:  ... 경로 A → 답: "11"
CoT 경로 2:  ... 경로 B → 답: "11"   ← 다수
CoT 경로 3:  ... 경로 C → 답: "9"
CoT 경로 4:  ... 경로 D → 답: "11"
CoT 경로 5:  ... 경로 E → 답: "9"

Majority Vote: "11" (3/5)
최종 답: "11"
```

Wang et al. (2022) "Self-Consistency Improves Chain of Thought Reasoning in Language Models":
- GSM8K에서 CoT 단독 대비 **+17%** 이상 성능 향상
- SVAMP, AQuA 등 다양한 수학 benchmark에서 일관된 향상

### 수식으로 보는 Self-Consistency

N개의 reasoning chain을 샘플링할 때의 최종 답 `a*`:

```
a* = argmax_a  Σ_{i=1}^{N} 1[f(r_i) = a]
             ───────────────────────────
                          N

r_i: i번째 reasoning chain
f(r_i): r_i에서 추출된 최종 답
```

즉, 각 가능한 답 `a`에 대해 그 답이 나온 chain의 비율을 계산하고, 가장 높은 비율의 답을 선택함.

### Accuracy vs N (샘플 수) 관계

```
정확도
 ↑
 |                              ●─────────────────
 |                         ●──
 |                    ●──
 |               ●──
 |          ●──
 |     ●──
 |──●
 +────────────────────────────────────────────→ N (샘플 수)
   1    5   10   20  40   80

(수익 체감 법칙: N이 커질수록 추가적인 개선 폭이 줄어듦)
```

실용적으로는 **N = 10~40** 정도에서 좋은 성능-비용 균형을 보임.

### Self-Consistency의 전제 조건

1. **정답이 명확히 추출 가능해야 함**: 주관식이 아닌 수학 문제, 다지선다 등
2. **답이 일관된 형식이어야 함**: "답: 42"처럼 규칙적이어야 투표가 가능
3. **충분한 다양성**: Temperature가 너무 낮으면 모든 sample이 같은 경로를 탐색

---

## 6.1.3 Tree-of-Thought (ToT)

### CoT의 한계

CoT는 **선형적(linear)** 추론 경로만 탐색함. 중간에 잘못된 방향으로 갔을 때 **되돌아올 수 없음**.

```
CoT (선형):
A → B → C (잘못됨) → D (회복 불가)
```

실제 인간의 문제 해결 과정은 더 복잡함:
- 여러 접근법을 시도해보고
- 막히면 뒤로 돌아가고 (backtrack)
- 유망한 방향을 더 깊이 탐색함

### ToT: 추론을 트리로 모델링

**Tree-of-Thought (ToT)** (Yao et al., 2023)는 각 추론 단계를 **트리의 노드**로 모델링함:

```
                     [문제]
                    /   |   \
                   /    |    \
             [접근 A] [접근 B] [접근 C]
              /  \       |      /  \
             /    \      |     /    \
          [A1]  [A2]  [B1]  [C1]  [C2]
    (막힘)        |   (막힘)  |   (유망)
               [A2a]       [C1a]
             (최종 답)    (더 탐색)
```

ToT의 구성 요소:
1. **Thought Generation (생성)**: 현재 상태에서 가능한 다음 단계들을 여러 개 생성
2. **State Evaluation (평가)**: 각 상태의 유망함(promise)을 LLM으로 평가
3. **Search Algorithm (탐색)**: BFS, DFS, 또는 beam search로 트리 탐색
4. **Backtracking (되돌아가기)**: 막히면 이전 상태로 복귀

```python
class TreeOfThought:
    def __init__(self, llm, max_steps=4, n_branches=3, n_evaluations=3):
        self.llm = llm
        self.max_steps = max_steps
        self.n_branches = n_branches
        self.n_evaluations = n_evaluations

    def generate_thoughts(self, problem, current_state):
        """현재 상태에서 가능한 다음 단계 생성"""
        prompt = f"""
        문제: {problem}
        현재까지의 추론: {current_state}

        다음으로 가능한 추론 단계를 {self.n_branches}가지 제안하세요.
        각 단계를 구분하여 작성하세요.
        """
        thoughts = self.llm.generate(prompt)
        return self.parse_thoughts(thoughts)

    def evaluate_state(self, problem, state):
        """현재 상태의 유망함을 평가 (sure / likely / impossible)"""
        prompt = f"""
        문제: {problem}
        추론 상태: {state}

        이 추론이 올바른 답으로 이어질 가능성을 평가하세요.
        응답: sure / likely / impossible 중 하나
        """
        votes = [self.llm.generate(prompt) for _ in range(self.n_evaluations)]
        return self.aggregate_votes(votes)

    def solve_bfs(self, problem, beam_width=3):
        """BFS + Beam Search로 ToT 탐색"""
        # 초기 상태
        current_states = [{"state": "", "score": 1.0}]

        for step in range(self.max_steps):
            all_candidates = []

            for node in current_states:
                # 각 상태에서 가능한 다음 단계 생성
                thoughts = self.generate_thoughts(problem, node["state"])

                for thought in thoughts:
                    new_state = node["state"] + "\n" + thought
                    score = self.evaluate_state(problem, new_state)
                    all_candidates.append({"state": new_state, "score": score})

            # 상위 beam_width개 상태만 유지
            current_states = sorted(
                all_candidates, key=lambda x: x["score"], reverse=True
            )[:beam_width]

        return current_states[0]  # 가장 유망한 최종 상태
```

### ToT가 효과적인 문제 유형

ToT는 특히 **global planning**이 필요한 문제에서 탁월함:
- 퍼즐 (Game of 24: 4개 숫자로 24 만들기)
- 글쓰기 (여러 개요 중 가장 좋은 것 선택)
- 코드 디버깅 (여러 수정 방안 비교)

Game of 24 예시 성능 비교:
```
방법            | 정확도
──────────────────────────
CoT (greedy)   | 4%
CoT (best-of-1)| 9%
ToT (BFS b=5)  | 74%
```

### ToT vs CoT vs Self-Consistency

```
방법               탐색 유형       비용      유망한 문제 유형
──────────────────────────────────────────────────────────────
CoT (greedy)    선형, 단일 경로   낮음      단순 추론
CoT + SC        선형, 다중 경로   중간      수학, 분류
ToT (BFS/DFS)   트리, 구조적     높음      퍼즐, 계획, 다단계
```

---

## 6.1.4 Reflection (자기 반성)

### LLM이 자신의 출력을 비판하고 개선하기

**Reflection**은 LLM이 자신의 출력을 다시 검토하고, 문제를 발견하면 스스로 수정하는 능력.

```
[원래 답변 생성]
  ↓
[자기 비판 (critique)]
  "이 답변에 오류가 있는지 검토해보겠습니다...
   3번째 단계에서 계산 실수가 있었습니다: 7 × 8 = 56이 아니라 56이 맞지만
   그 전 단계의 7을 잘못 계산했습니다..."
  ↓
[답변 재생성 (refinement)]
  "수정된 답변: ..."
```

```python
def reflection_generate(question, llm, n_iterations=2):
    # 초기 답변 생성
    response = llm.generate(f"질문: {question}\n답변:")

    for i in range(n_iterations):
        # 자기 비판
        critique_prompt = f"""
        질문: {question}
        이전 답변: {response}

        이 답변을 면밀히 검토하고 다음을 확인하세요:
        1. 사실적 오류가 있는가?
        2. 논리적 오류가 있는가?
        3. 중요한 정보가 누락되었는가?

        비판:
        """
        critique = llm.generate(critique_prompt)

        # 개선된 답변 생성
        refine_prompt = f"""
        질문: {question}
        이전 답변: {response}
        비판: {critique}

        비판을 바탕으로 개선된 답변을 작성하세요:
        """
        response = llm.generate(refine_prompt)

    return response
```

### Reflexion 프레임워크

Shinn et al. (2023)의 **Reflexion**은 에이전트 설정에서 reflection을 활용함:

```
[시도 1]
  에이전트 행동 → 환경 반응 → 실패
  ↓
  "왜 실패했는가?" → Verbal Reflection 생성 → 장기 기억에 저장

[시도 2]
  이전 reflection을 context로 사용 → 개선된 행동
  에이전트 행동 → 환경 반응 → 더 나은 결과
```

ALFWorld (텍스트 게임), HotPotQA 등에서 단순 반복 시도 대비 큰 성능 향상을 보였음.

### ReAct: Reason + Act + Observe

**ReAct** (Yao et al., 2022)는 추론과 행동을 번갈아 수행하는 프레임워크:

```
Thought 1: "파리의 현재 인구를 알아야 한다."
Act 1:     Search("파리 현재 인구 2024")
Obs 1:     "파리의 2024년 인구는 약 215만 명이다."

Thought 2: "뉴욕과 비교해야 한다. 뉴욕 인구도 검색하자."
Act 2:     Search("뉴욕 현재 인구 2024")
Obs 2:     "뉴욕의 2024년 인구는 약 826만 명이다."

Thought 3: "826만 / 215만 ≈ 3.84. 뉴욕이 약 3.8배 더 크다."
Act 3:     Finish("뉴욕의 인구는 파리보다 약 3.8배 많다.")
```

ReAct의 강점:
- **Grounded reasoning**: 실제 검색 결과를 기반으로 추론 → hallucination 방지
- **Interpretability**: 각 단계가 명시적으로 기록됨
- **Error recovery**: Observation이 예상과 다르면 Thought를 수정 가능

```python
def react_agent(question, llm, tools, max_steps=10):
    trajectory = []
    context = f"질문: {question}\n\n"

    for step in range(max_steps):
        # Thought 생성
        thought_prompt = context + "Thought:"
        thought = llm.generate(thought_prompt)
        trajectory.append(f"Thought: {thought}")

        # Action 생성
        act_prompt = context + f"Thought: {thought}\nAct:"
        action_str = llm.generate(act_prompt)

        if "Finish" in action_str:
            # 답변 완료
            final_answer = extract_finish_arg(action_str)
            return final_answer, trajectory

        # 도구 실행
        tool_name, tool_arg = parse_action(action_str)
        observation = tools[tool_name](tool_arg)

        trajectory.append(f"Act: {action_str}")
        trajectory.append(f"Obs: {observation}")
        context += f"Thought: {thought}\nAct: {action_str}\nObs: {observation}\n\n"

    return None, trajectory  # max_steps 초과
```

---

## 6.1.5 Program-of-Thought (PoT)

### 산술 계산의 취약점

LLM은 언어 이해에는 탁월하지만 **정확한 수치 계산에는 취약**함:

```
질문: "세금 15%를 포함한 총액이 $345.00일 때, 세전 금액은?"

LLM 직접 답: "$298.70"  (틀림: 345 / 1.15 = 300.00)
```

이 오류는 LLM이 산술 연산을 토큰 예측으로 수행하기 때문. 숫자 크기가 커지거나 연산이 복잡해질수록 오류가 급증함.

### PoT: 코드를 통한 계산 분리

**Program-of-Thought (PoT)** (Chen et al., 2022)는 LLM에게 직접 계산하는 대신 **Python 코드를 생성**하게 하고, 코드를 실행하여 정확한 결과를 얻음:

```python
# PoT 예시
질문: "세금 15%를 포함한 총액이 $345.00일 때, 세전 금액은?"

LLM 생성 코드:
total_with_tax = 345.00
tax_rate = 0.15
pre_tax = total_with_tax / (1 + tax_rate)
print(f"세전 금액: ${pre_tax:.2f}")

실행 결과: "세전 금액: $300.00"  ← 정확!
```

PoT의 핵심 통찰: **언어 이해(추론)** 와 **수치 계산** 을 분리하라. LLM은 추론과 코드 작성을 담당하고, 정확한 계산은 Python 인터프리터에게 맡겨라.

```python
import subprocess

def program_of_thought(question, llm):
    # LLM이 Python 코드 생성
    code_prompt = f"""
    다음 문제를 Python 코드로 풀어주세요.
    계산 결과는 print()로 출력하세요.
    변수명과 주석을 통해 추론 과정을 명확히 하세요.

    문제: {question}

    ```python
    """
    generated_code = llm.generate(code_prompt)
    generated_code = extract_code(generated_code)  # ```python...``` 추출

    # 코드 실행
    try:
        result = subprocess.run(
            ["python", "-c", generated_code],
            capture_output=True, text=True, timeout=10
        )
        output = result.stdout.strip()
    except Exception as e:
        output = f"Error: {e}"

    return generated_code, output
```

### PoT의 장점 분석

| 능력 | Direct LLM | CoT | PoT |
|---|---|---|---|
| 복잡한 산술 | 취약 | 보통 | 우수 |
| 대형 수 계산 | 취약 | 취약 | 우수 |
| 조건 분기 | 보통 | 보통 | 우수 |
| 루프 및 반복 | 취약 | 취약 | 우수 |
| 언어 추론 | 우수 | 우수 | 보통 |
| 실행 오류 처리 | N/A | N/A | 추가 처리 필요 |

### PoT가 특히 효과적인 분야

**재무 계산:**
```python
# 복리 이자 계산
principal = 10000       # 원금
annual_rate = 0.05      # 연이율 5%
n_compounding = 12      # 월 복리
years = 30

final_amount = principal * (1 + annual_rate/n_compounding) ** (n_compounding * years)
print(f"30년 후 금액: ${final_amount:,.2f}")
# 출력: 30년 후 금액: $43,098.01
```

**통계/데이터 분석:**
```python
import statistics

scores = [85, 92, 78, 96, 88, 73, 91, 84, 77, 89]
mean = statistics.mean(scores)
std_dev = statistics.stdev(scores)
median = statistics.median(scores)

print(f"평균: {mean:.1f}")
print(f"표준편차: {std_dev:.1f}")
print(f"중앙값: {median}")
```

**조합/순열:**
```python
from math import comb, factorial

# "10명 중 3명 선발 방법의 수"
combinations = comb(10, 3)
print(f"10C3 = {combinations}")  # 120
```

### PoT의 한계 및 대응

**한계 1: 실행 불가능한 코드**
LLM이 문법 오류가 있는 코드를 생성할 수 있음.
→ **Self-debugging**: 오류 메시지를 context에 추가하고 수정 요청

```python
def pot_with_retry(question, llm, max_retries=3):
    code = llm.generate_code(question)

    for attempt in range(max_retries):
        result, error = execute_code(code)

        if error:
            # 오류를 context에 추가하고 수정 요청
            fix_prompt = f"""
            코드:
            {code}

            오류 메시지:
            {error}

            오류를 수정한 코드를 작성하세요:
            """
            code = llm.generate(fix_prompt)
        else:
            return result

    return None  # 최대 재시도 초과
```

**한계 2: 언어적 추론 + 계산의 혼합**
단순 계산이 아닌 복잡한 언어 이해가 필요한 경우 PoT만으로는 부족함.
→ **CoT + PoT 하이브리드**: 언어 이해에는 CoT, 계산에는 PoT를 함께 사용

---

## 6.1.6 기법들의 종합 비교

### 언제 어떤 기법을 사용해야 하는가?

```
문제 유형 판단:

[단순 사실 질문]
  → 직접 답변 (추가 기법 불필요)

[단순~중간 수학/논리]
  → Zero-shot CoT ("Let's think step by step")
  → 중요한 태스크라면 + Self-Consistency (N=10~20)

[복잡한 수치 계산]
  → Program-of-Thought (PoT)
  → 계산 + 추론 혼합이면 CoT + PoT 하이브리드

[복잡한 다단계 계획/퍼즐]
  → Tree-of-Thought (ToT) (비용은 높음)

[에이전트 태스크 (검색, 도구 사용)]
  → ReAct

[반복적인 개선이 필요한 작문/코딩]
  → Reflection / Reflexion
```

### 성능-비용 트레이드오프

```
                     성능
                      ↑
              ToT ────●
                   ●  (SC + CoT)
             ●         (CoT)
         ●                (Zero-shot CoT)
     ●                         (Direct)
     +────────────────────────────────────→
   낮음                                높음
                     비용 (LLM 호출 수)
```

이 모든 기법은 **inference-time compute**를 늘리는 방식으로 성능을 향상시킨다는 공통점이 있음. 이것이 Section 6.2에서 다루는 "Test-Time Compute Scaling"의 핵심 아이디어.

---

*다음 파일: 6.2 Test-Time Compute — PRM, RLVR, Best-of-N, MCTS-style Search*
