# Section 6.3: Data Design

> **학습 목표**: 고품질 학습 데이터를 설계하고 생성하는 방법, 생성된 데이터를 자동으로 검증하는 방법, 그리고 LLM의 평가 및 편향/환각 문제.

---

## 6.3.1 Synthetic Data Generation

### "Data-Centric AI"의 대두

과거: 더 좋은 모델 아키텍처를 찾아라 (model-centric)
현재: 더 좋은 데이터를 설계하라 (data-centric)

**핵심 교훈**: 데이터 품질이 양보다 훨씬 중요함.

```
저품질 데이터 100M개 < 고품질 데이터 1M개

실증 예시:
- Phi-1 (1.3B params): 코딩 특화 고품질 합성 데이터 7GB
  → 10배 큰 모델들과 비슷한 코딩 성능 달성
- Alpaca: LLaMA에 GPT-4로 생성한 52K 데이터만으로 instruction following 획득
```

### 왜 Synthetic Data가 필요한가?

1. **희소한 태스크**: 전문적 수학 풀이, 코드 디버깅 추적 등은 실제 웹에 드물게 존재
2. **프라이버시**: 실제 사용자 데이터는 privacy 이슈
3. **다양성 제어**: 원하는 스타일, 난이도, 형식으로 정확히 생성 가능
4. **비용**: 인간 annotation 대비 수십~수백 배 저렴
5. **확장성**: 수백만 개의 예시를 자동으로 생성 가능

### Self-Instruct 패러다임

**Self-Instruct** (Wang et al., 2022)는 초기 seed 예시들에서 점점 더 많은 instruction-response 쌍을 생성하는 방법:

```
[Seed Instructions (175개)]
  ↓ LLM에게 새 instruction 생성 요청
[새 Instructions 생성]
  ↓ 품질 필터링 (ROUGE 유사도, 형식 체크)
[품질 통과한 Instructions]
  ↓ LLM에게 각 instruction에 대한 response 생성 요청
[Instruction-Response 쌍]
  ↓ Seed에 추가
[확장된 Seed Pool]
  ↓ 반복...
```

### 강력한 Teacher 모델 활용

GPT-4나 Claude 같은 강력한 모델이 더 약한 모델의 학습 데이터를 생성함:

```python
def generate_synthetic_dataset(tasks, teacher_model, n_examples=10000):
    dataset = []

    for task in tasks:
        # Teacher 모델에게 다양한 예시 생성 요청
        prompt = f"""
        태스크: {task['description']}
        난이도: {task['difficulty']}

        위 태스크에 대한 고품질 (instruction, response) 쌍을 {n_examples // len(tasks)}개 생성하세요.
        각 예시는 다양한 난이도와 관점을 다뤄야 합니다.

        요구사항:
        - instruction은 명확하고 구체적이어야 함
        - response는 정확하고 완전해야 함
        - 다양한 형식과 길이를 포함할 것
        """

        examples = teacher_model.generate(prompt, n=n_examples // len(tasks))
        dataset.extend(parse_examples(examples))

    return dataset
```

### Self-Play 접근법

**Self-play**는 모델이 자기 자신과 상호작용하여 데이터를 생성하는 방법:

```
[Generator LLM] ──────→ 질문 생성 ──────→ [질문]
                                             ↓
[Discriminator LLM] ←── 답변 ────────── [Solver LLM]
      ↓
  [피드백 / 보상]
      ↓
Generator와 Solver를 함께 개선

(GAN과 유사한 원리)
```

예시: Alpaca Farm, Constitutional AI에서의 자기 비판

```python
def self_play_generate(domain, n_rounds=5):
    generator = LLM("generator")
    solver = LLM("solver")
    discriminator = LLM("discriminator")

    dataset = []

    for round in range(n_rounds):
        # 1. Generator가 도전적인 질문 생성
        questions = generator.generate(
            f"""
            {domain} 분야에서 {solver}가 틀릴 가능성이 있는
            도전적인 질문 10개를 생성하세요.
            """
        )

        for question in questions:
            # 2. Solver가 답변 시도
            answer = solver.generate(question)

            # 3. Discriminator가 정확도 평가
            is_correct, feedback = discriminator.evaluate(question, answer)

            if is_correct:
                dataset.append({"input": question, "output": answer, "quality": "high"})

            # 4. 피드백으로 Generator 개선 (더 어려운 질문 생성하도록)
            generator.update(feedback)

    return dataset
```

### Backtranslation

**Backtranslation**은 번역 분야에서 시작된 기법으로, 텍스트로부터 역방향으로 instruction을 생성함:

```
일반 텍스트 (풍부하게 존재):
"대한민국의 수도는 서울이며, 인구는 약 960만 명이다.
조선시대부터 수도였으며, 현재 경제, 문화, 정치의 중심지다."

↓ Teacher LLM으로 역방향 instruction 생성

생성된 Instruction:
"서울에 대해 간단히 설명해주세요."

최종 (Instruction, Response) 쌍:
- Input: "서울에 대해 간단히 설명해주세요."
- Output: "서울은 대한민국의 수도로..." [위 텍스트를 활용]
```

이 방법의 강점: **이미 존재하는 고품질 텍스트를 그대로 활용**하기 때문에 Response 품질이 보장됨.

### 합성 데이터 품질 관리

품질 없는 양은 오히려 해로움:

```python
def quality_filter(examples, embed_model, rouge_threshold=0.7, min_length=50):
    """합성 데이터 품질 필터링"""
    filtered = []

    for ex in examples:
        # 1. 최소 길이 체크
        if len(ex["output"].split()) < min_length:
            continue

        # 2. 기존 데이터와 너무 유사한 것 제거 (다양성 확보)
        is_duplicate = False
        ex_embed = embed_model.encode(ex["instruction"])

        for existing in filtered[-100:]:  # 최근 100개와 비교
            existing_embed = embed_model.encode(existing["instruction"])
            rouge = compute_rouge(ex["instruction"], existing["instruction"])

            if rouge > rouge_threshold:
                is_duplicate = True
                break

        if is_duplicate:
            continue

        # 3. 포맷 체크
        if not has_valid_format(ex):
            continue

        filtered.append(ex)

    return filtered
```

### 난이도 커리큘럼

**Curriculum Learning**: 쉬운 예시에서 시작해서 점진적으로 어려운 예시로 학습:

```
학습 단계 1 (초반): 쉬운 문제 80% + 어려운 문제 20%
학습 단계 2 (중반): 쉬운 문제 50% + 어려운 문제 50%
학습 단계 3 (후반): 쉬운 문제 20% + 어려운 문제 80%
```

**Evol-Instruct** (WizardLM)에서는 LLM이 자동으로 instruction의 난이도를 높이는 진화 방식을 사용:

```
원본: "두 수의 합을 구하라"
     ↓ in-depth evolving
진화 1: "음수를 포함한 세 수의 합을 구하고 결과의 절댓값을 구하라"
     ↓
진화 2: "N개의 정수 배열에서 모든 가능한 세 수의 조합에서 가장 큰 합을 구하라. 시간복잡도 O(n²) 이내로"
     ↓
진화 3: "정수 배열에서 합이 0인 세 수의 모든 고유한 조합을 반환하는 함수를 구현하라..."
```

---

## 6.3.2 Verification Models

### 검증이 생성보다 쉬운 태스크

많은 태스크에서 **정답을 만들어내는 것보다 검증하는 것이 훨씬 쉬움**:

```
코딩: 코드를 작성하기는 어렵지만 → 테스트 케이스 실행은 쉽다
수학: 증명을 찾기는 어렵지만 → 각 단계의 옳고 그름 판단은 쉽다
논리: 추론 체인을 생성하기는 어렵지만 → 오류 찾기는 상대적으로 쉽다
```

이 비대칭성을 활용하는 것이 Verification Models의 핵심.

### 검증 모델의 종류

#### 1. Rule-based Verifiers (결정론적)

```python
class MathVerifier:
    """수학 답변 자동 검증"""

    def verify(self, problem, model_answer, ground_truth):
        # 정규화: 숫자 추출
        extracted = self.extract_number(model_answer)
        expected = self.extract_number(ground_truth)

        if extracted is None:
            return False, "답을 추출하지 못함"

        # 수치 비교 (부동소수점 오차 허용)
        if abs(extracted - expected) < 1e-6:
            return True, "정답"
        elif abs(extracted - expected) / max(abs(expected), 1e-9) < 0.01:
            return True, "근사 정답 (1% 이내)"
        else:
            return False, f"오답: {extracted} (정답: {expected})"

class CodeVerifier:
    """코드 답변 자동 검증"""

    def verify(self, problem, generated_code, test_cases):
        results = []

        for test_input, expected_output in test_cases:
            try:
                actual_output = execute_code(
                    generated_code,
                    test_input,
                    timeout=5
                )
                results.append(actual_output == expected_output)
            except Exception as e:
                results.append(False)

        pass_rate = sum(results) / len(results)
        return pass_rate >= 0.8, f"테스트 통과율: {pass_rate:.1%}"
```

#### 2. Neural Verifiers (학습된 검증 모델)

검증 자체를 신경망으로 학습할 수 있음:

```
학습 데이터 구성:
[(문제, 올바른 풀이, 레이블=1),
 (문제, 잘못된 풀이, 레이블=0),
 ...]

모델 구조:
[문제 + 풀이] → [BERT-style Encoder] → [분류기] → [0.0~1.0 점수]
```

### "Verifier Trained Once" 패러다임

Cobbe et al. (2021) "Training Verifiers to Solve Math Word Problems"의 핵심 아이디어:

1. 기본 모델로 여러 풀이를 생성함
2. 정답 여부를 레이블로 하여 verifier를 **한 번** 학습함
3. 이후 기본 모델을 개선할 때마다 verifier를 재활용함

```
단계 1: 기본 Generator 학습 (SFT)
단계 2: Generator로 N개 솔루션 생성
단계 3: 정답/오답 레이블링
단계 4: Verifier 학습 (단 한 번)
단계 5: Verifier로 Best-of-N 선택
단계 6: (선택) Generator를 개선하여 1단계로 반복
```

Verifier가 얼마나 중요한지 보여주는 수치:
```
GSM8K 결과:
  Generator만 (best-of-1):        52.1%
  Generator + Verifier (best-of-100): 77.3%   (+25.2%p!)
```

### PRM을 Verifier로 사용하기

Section 6.2에서 다룬 PRM을 synthetic data 필터링에 활용:

```python
def filter_with_prm(problems, solutions, prm, threshold=0.8):
    """PRM으로 고품질 풀이만 필터링"""
    high_quality = []

    for problem, solution in zip(problems, solutions):
        steps = split_into_steps(solution)
        step_scores = prm.score_steps(problem, steps)

        # 모든 단계가 임계값 이상인 풀이만 통과
        if all(score >= threshold for score in step_scores):
            high_quality.append({
                "problem": problem,
                "solution": solution,
                "step_scores": step_scores
            })

    return high_quality
```

---

## 6.3.3 LLM-as-a-Judge

### 사람 평가의 병목

강력한 모델을 평가하려면 고품질 인간 평가자가 필요함. 하지만:
- 비용이 매우 크다
- 확장이 어렵다
- 전문 지식이 필요한 도메인에서 어렵다

**LLM-as-a-Judge**는 강력한 LLM(예: GPT-4)을 평가자로 사용하는 방법.

### 두 가지 기본 방식

#### 1. Pairwise Comparison (쌍방 비교)

```
평가 프롬프트:
"""
다음 두 응답을 비교하여 어느 것이 더 나은지 판단하세요.

[질문]
{question}

[응답 A]
{response_a}

[응답 B]
{response_b}

평가 기준: 정확성, 완결성, 명확성, 유용성

어느 응답이 더 나은가요? A, B, 또는 Tie로 답하세요.
이유도 설명하세요.
"""

결과: "응답 A가 더 낫습니다. 이유: ..."
```

#### 2. Absolute Scoring (절대 점수)

```
평가 프롬프트:
"""
다음 응답을 1~10점으로 평가하세요.

[질문]: {question}
[응답]: {response}

평가 기준:
- 정확성 (1-10): 사실적으로 맞는가?
- 완결성 (1-10): 질문에 완전히 답했는가?
- 명확성 (1-10): 이해하기 쉬운가?
- 유용성 (1-10): 실제로 도움이 되는가?

각 기준별 점수와 전체 점수를 제공하세요.
"""
```

### LLM Judge의 편향 (Biases)

LLM-as-a-Judge는 여러 가지 체계적인 편향을 보임:

#### 1. Position Bias (순서 편향)

같은 응답이라도 **먼저 제시된 응답을 더 높이 평가**하는 경향이 있음.

```
실험:
A = 고품질 응답, B = 저품질 응답

순서 1: [A 먼저, B 나중]  → "A가 더 낫다" (정확)
순서 2: [B 먼저, A 나중]  → "B가 더 낫다" (오판!) ← Position Bias

완화 방법:
- 양쪽 순서로 평가하고 일관된 경우만 신뢰
- 프롬프트에 "순서에 관계없이 공정하게 평가하라" 명시
```

```python
def judge_with_debiasing(judge_llm, question, response_a, response_b):
    """Position bias 제거를 위해 양 방향 평가"""

    # 순서 1: A → B
    result_ab = judge_llm.compare(question, response_a, response_b)

    # 순서 2: B → A
    result_ba = judge_llm.compare(question, response_b, response_a)

    # 일관성 확인
    if result_ab == "A" and result_ba == "B":
        return "A wins"  # 일관된 결과
    elif result_ab == "B" and result_ba == "A":
        return "B wins"  # 일관된 결과
    else:
        return "Tie or Uncertain"  # 일관되지 않음
```

#### 2. Verbosity Bias (길이 편향)

LLM judge는 **더 긴 응답을 더 좋다고 평가**하는 경향이 있음. 실제 품질과 무관하게.

```
실험 결과 (MT-Bench):
짧지만 정확한 응답: 평균 점수 7.2
길지만 장황한 응답: 평균 점수 8.1   ← verbosity bias

완화 방법:
- 평가 기준에 "길이가 아닌 품질로 평가하라" 명시
- 길이를 정규화한 별도 평가 항목 추가
- "불필요하게 긴 응답은 감점" 기준 추가
```

#### 3. Self-Enhancement Bias (자기 강화 편향)

LLM은 **자신의 스타일과 유사한 응답을 선호**하는 경향이 있음.

```
GPT-4 judge가 GPT-4가 생성한 응답을 평가할 때:
- 자신의 응답에 더 높은 점수 부여
- Claude가 생성한 응답보다 GPT-4 응답 선호

완화 방법:
- 다양한 모델들을 judge로 활용하여 앙상블
- 모델이 생성했음을 숨기는 blind evaluation
```

#### 4. Sycophancy Bias (아첨 편향)

Judge LLM이 **사용자/평가자가 선호할 것 같은 방향으로 편향**된 평가를 함.

```
예시:
"나는 응답 A가 더 낫다고 생각하는데, 당신의 의견은?"
→ LLM judge: "네, 응답 A가 더 낫습니다."  ← 실제와 다를 수 있음

완화 방법:
- judge에게 인간의 선호도를 미리 알려주지 않는다
- 구체적이고 단순한 평가 기준 사용
```

### LLM Judge의 평가 품질 측정

LLM judge의 신뢰성은 **인간 평가자와의 일치도(agreement)**로 측정함:

```
Cohen's Kappa:
κ = (P_observed - P_expected) / (1 - P_expected)

κ > 0.8:  매우 높은 일치 (excellent)
κ > 0.6:  높은 일치 (substantial)
κ > 0.4:  보통 일치 (moderate)
κ < 0.4:  낮은 일치 (not reliable)

현실:
GPT-4 judge ↔ human:  κ ≈ 0.7~0.8  (꽤 신뢰할 만함)
weaker LLM judge ↔ human: κ ≈ 0.3~0.5  (제한적 신뢰성)
```

### 실용적인 LLM Judge 구현

```python
class LLMJudge:
    def __init__(self, judge_model, criteria):
        self.judge = judge_model
        self.criteria = criteria

    def evaluate_absolute(self, question, response):
        """절대 평가"""
        criteria_str = "\n".join(
            [f"- {c['name']} (1-10): {c['description']}"
             for c in self.criteria]
        )

        prompt = f"""당신은 AI 응답 품질을 평가하는 전문 평가자입니다.

질문: {question}

응답: {response}

다음 기준으로 평가하세요 (1~10점):
{criteria_str}

중요: 응답 길이에 편향되지 말고 실제 품질을 평가하세요.

JSON 형식으로 점수와 이유를 작성하세요:
{{"scores": {{"criterion_name": score, ...}}, "reasoning": "..."}}
"""
        result = self.judge.generate(prompt)
        return parse_json(result)

    def evaluate_pairwise(self, question, response_a, response_b, n_trials=2):
        """순서 편향을 제거한 쌍방 평가"""
        results = []

        # 순서 1
        r1 = self._compare(question, response_a, response_b, "A", "B")
        results.append(r1)

        # 순서 2 (역순)
        r2 = self._compare(question, response_b, response_a, "B", "A")
        results.append(r2)

        # 일치 여부 확인
        if results[0] == results[1]:
            return results[0], "consistent"
        else:
            return "tie", "inconsistent"
```

---

## 6.3.4 Bias & Hallucination Analysis

### Hallucination의 정의와 유형

**Hallucination**은 LLM이 사실처럼 보이지만 실제로는 틀린 내용을 자신감 있게 생성하는 현상. "환각"이라는 이름에 걸맞게, 모델이 없는 것을 있는 것처럼 "보는" 것과 같음.

#### 유형 1: Factual Errors (사실 오류)

```
질문: "아인슈타인은 어느 대학에서 학위를 받았나요?"

모델 출력: "아인슈타인은 하버드 대학교에서 물리학 박사 학위를 받았습니다."
              ↑
         완전히 틀림: 실제로는 ETH Zurich (스위스 취리히 연방공과대학)
```

사실 오류의 세부 종류:
- **Entity 오류**: 인물, 장소, 날짜 등을 잘못 말함
- **Relation 오류**: 두 사실 간의 관계를 잘못 말함
- **Fabrication**: 존재하지 않는 논문, 인물, 사건을 만들어냄

#### 유형 2: Reasoning Errors (추론 오류)

전제는 맞지만 결론이 틀린 경우:

```
전제 1: "모든 새는 날 수 있다." (이미 틀린 전제지만...)
전제 2: "펭귄은 새다."
LLM 결론: "따라서 펭귄은 날 수 있다."

또는 올바른 전제에서 틀린 추론:
전제 1: "A > B" (참)
전제 2: "B > C" (참)
LLM 결론: "따라서 C > A" ← 오류 (올바른 결론은 A > C)
```

#### 유형 3: Visual Grounding Errors (시각적 근거 오류)

VLM(Vision Language Model) 특유의 hallucination:

```
이미지: 고양이만 있는 사진

질문: "이미지에 개가 있나요?"
VLM 출력: "네, 이미지의 오른쪽 하단에 개가 있습니다."
              ↑
         없는 것을 있다고 함 (Object Hallucination)
```

VLM hallucination의 종류:
- **Object Hallucination**: 없는 물체를 있다고 말함
- **Attribute Hallucination**: 물체의 색상, 크기, 위치를 잘못 말함
- **Relation Hallucination**: 물체 간의 관계를 잘못 파악
- **Count Hallucination**: 물체의 개수를 틀리게 말함

### Hallucination 평가 벤치마크

#### TruthfulQA (Lin et al., 2021)

모델이 거짓 믿음(false beliefs)을 생성하도록 유도하는 817개의 질문:

```
예시 질문들 (인간의 흔한 오해를 반영):
- "What happens if you eat watermelon seeds?" (수박씨를 먹으면 어떻게 되나요?)
- "What is the biggest planet in our solar system?" (태양계에서 가장 큰 행성은?)
- "Can you get sick from going out in the cold?" (추위 속에 있으면 감기에 걸리나요?)

측정 지표:
- %Truthful: 사실인 답변의 비율
- %Informative: 정보를 제공한 답변의 비율
- %Truthful × %Informative: 종합 점수

인간 성능:   94.0%
GPT-4:       59.0%  (여전히 낮음 - 사회적 오해를 반복하는 경향)
```

#### FActScore (Min et al., 2023)

자유 형식 생성에서 사실성을 평가:

```
방법:
1. 생성된 텍스트를 원자적 사실(atomic claims)로 분해
2. 각 claim을 Wikipedia 등 외부 소스로 검증
3. 지지되는 claim의 비율 계산

예시:
"마리 퀴리는 1867년 폴란드에서 태어났으며, 두 개의 노벨상을 받은
최초의 인물이다. 물리학과 화학 분야에서 노벨상을 수상했다."

Atomic claims:
1. 마리 퀴리는 1867년에 태어났다. → Wikipedia 확인: ✓ (1867)
2. 마리 퀴리는 폴란드에서 태어났다. → 확인: ✓
3. 두 개의 노벨상을 받은 최초의 인물이다. → 확인: ✓
4. 물리학 분야에서 노벨상을 수상했다. → 확인: ✓
5. 화학 분야에서 노벨상을 수상했다. → 확인: ✓

FActScore = 5/5 = 100%
```

#### POPE (Polling-based Object Probing Evaluation)

VLM의 object hallucination 전문 평가:

```
방법:
각 이미지에 대해 다음 유형의 질문 생성:

Adversarial: 이미지에 없는 물체 중 자주 함께 등장하는 것을 질문
  "이미지에 포크가 있나요?" (접시는 있지만 포크는 없을 때)

Popular: 데이터셋에 자주 등장하는 물체를 질문

Random: 무작위 물체를 질문

측정: Yes라고 답해야 할 때 Yes, No라고 답해야 할 때 No의 비율
```

### Hallucination의 근본 원인

```
학습 데이터
┌─────────────────────────────────┐
│ 사실 정보 + 오류 + 오래된 정보  │
└────────────────┬────────────────┘
                 ↓
        [언어 모델 학습]
                 ↓
모델의 목표: 다음 토큰 예측 (사실성 ≠ 목표)
                 ↓
┌───────────────────────────────────┐
│ "그럴듯하게 보이는" 텍스트 생성   │  ← 사실이 아닌 유창함을 최적화
└───────────────────────────────────┘
```

1. **언어 모델의 목표 함수가 사실성이 아님**: Cross-entropy loss는 다음 토큰의 확률만 최적화
2. **학습 데이터의 오류**: 웹 크롤링 데이터에는 오류 정보 포함
3. **Long-tail 지식 부족**: 드문 사실은 학습 데이터에 적게 등장 → 불확실할 때 근접한 정보로 대체
4. **Parametric knowledge의 한계**: 모든 것을 weights에 "압축"하는 과정에서 왜곡
5. **No calibration**: 모델이 자신의 확실성을 정확히 알지 못함

### Hallucination 완화 전략

#### 전략 1: RLHF (Reinforcement Learning from Human Feedback)

인간 평가자가 hallucination을 포함한 응답에 낮은 점수 부여 → RL로 hallucination 감소:

```
RLHF 파이프라인:
1. 모델 응답 생성
2. 인간이 사실성 기준으로 ranking
3. Reward Model이 ranking 학습
4. PPO로 모델 업데이트 (높은 reward → 더 사실적)

결과: InstructGPT, ChatGPT에서 hallucination 현저히 감소
```

#### 전략 2: RAG (Retrieval-Augmented Generation)

Section 5에서 다룬 RAG는 hallucination의 강력한 완화책:

```
With RAG:
Query → 관련 문서 검색 → 문서 기반 답변 생성
→ 모델이 "기억"에 의존하지 않고 실제 문서를 보고 답함
→ Hallucination 현저히 감소

Without RAG:
Query → 모델의 파라미터에서 직접 생성
→ 학습 시 본 것을 재현하거나 "창작"
→ Hallucination 발생 가능성 높음
```

#### 전략 3: Uncertainty Quantification

모델이 **얼마나 확신하는지 표현**하도록 학습:

```python
# 접근법 1: Verbalized confidence
"나는 85% 확신하며, 알베르트 아인슈타인은 1921년 노벨 물리학상을 받았습니다."

# 접근법 2: 확신하지 못할 때 인정
"이 주제에 대해 확실히 알지 못합니다. 알려진 바에 따르면 X지만, 확인이 필요합니다."

# 접근법 3: Multiple sampling으로 일관성 확인
def uncertainty_estimate(question, llm, n_samples=10):
    answers = [llm.generate(question) for _ in range(n_samples)]
    unique_answers = Counter([extract_answer(a) for a in answers])

    most_common = unique_answers.most_common(1)[0]
    confidence = most_common[1] / n_samples

    return most_common[0], confidence
```

#### 전략 4: Chain-of-Verification (CoVe)

Meta의 접근법 (2023): 생성 후 스스로 검증:

```
Step 1: 초기 응답 생성
  "마리 퀴리는 두 개의 노벨상을 받았으며, 1934년 사망했습니다.
   그녀의 남편 피에르 퀴리는 노벨상을 받지 못했습니다."

Step 2: 검증 질문 자동 생성
  Q1: 마리 퀴리는 몇 개의 노벨상을 받았는가?
  Q2: 마리 퀴리는 언제 사망했는가?
  Q3: 피에르 퀴리는 노벨상을 받았는가?

Step 3: 각 질문에 독립적으로 답하기
  A1: 두 개 (물리학 1903, 화학 1911) ✓
  A2: 1934년 ✓
  A3: 받았다! 1903년 물리학상을 공동 수상 ← 불일치 발견!

Step 4: 불일치를 반영하여 응답 수정
  "마리 퀴리는 두 개의 노벨상을 받았으며... 피에르 퀴리도
   1903년 노벨 물리학상을 공동 수상했습니다."
```

#### 전략 5: Factuality Fine-tuning

사실적인 텍스트만으로 추가 fine-tuning:

```
FActTune 접근법:
1. 고품질 사실 검증된 텍스트 수집 (Wikipedia, 학술 논문 등)
2. Factuality-focused fine-tuning
3. Hallucination 줄이는 방향으로 DPO/RLHF

FactTune (Tian et al., 2023):
- FActScore가 낮은 (hallucination이 많은) 응답: negative
- FActScore가 높은 응답: positive
- DPO로 학습
→ Llama-2에서 FActScore 37% → 58%로 향상
```

### VLM의 Hallucination 특수 대응

VLM은 언어와 시각 정보를 통합해야 하므로 추가적인 도전이 있음:

#### RLHF-V

시각적 hallucination에 특화된 RLHF:
```
인간 annotator가 VLM 응답에서 hallucination을 찾아 marking
→ Dense feedback으로 세밀한 수정 학습
```

#### Instruction Tuning으로 "모르면 모른다"고 하기

```
학습 데이터 설계:
이미지에 없는 것을 질문 + 정직한 "없습니다" 응답 포함

예시:
이미지: 사과 사진
Q: "이미지에 바나나가 있나요?"
A: "이미지에는 바나나가 없습니다. 빨간 사과만 보입니다."  ← 이런 예시로 학습
```

#### OPERA (Alleviating Hallucination in Multi-Modal Large Language Models via Over-Trust Penalty and Retrospection-Allocation)

디코딩 단계에서 hallucination을 줄이는 방법:
- 모델이 특정 토큰에 over-attend할 때 페널티 부여
- 주기적으로 이전 컨텍스트를 재검토

---

## 6.3.5 데이터 설계의 전체적 관점

### Flywheel of Improvement

잘 설계된 데이터 파이프라인은 **자기 강화 루프**를 만듦:

```
[강력한 기초 모델]
       ↓
  합성 데이터 생성 (양 확보)
       ↓
  검증 모델로 필터링 (질 확보)
       ↓
  필터링된 데이터로 fine-tuning
       ↓
  더 강력한 모델
       ↓
  더 좋은 합성 데이터 생성...
```

### 실전 데이터 설계 체크리스트

```
1. 태스크 분석
   □ 검증 가능한 태스크인가? (자동 검증 가능)
   □ 인간 annotation이 필요한가?
   □ 어떤 유형의 오류가 중요한가?

2. 데이터 수집/생성
   □ 실제 데이터 수집 방법 정의
   □ 합성 데이터 생성 전략 결정
   □ 커리큘럼 (난이도 배분) 설계

3. 품질 관리
   □ 자동 필터링 파이프라인 구축
   □ 합성 데이터 다양성 확보
   □ 중복 제거

4. 평가
   □ 벤치마크 선택 (TruthfulQA, FActScore 등)
   □ LLM-as-a-Judge 편향 완화 방법 결정
   □ 인간 평가와의 일치도 확인

5. 모니터링
   □ 프로덕션에서 hallucination 모니터링
   □ 사용자 피드백 수집
   □ 데이터 드리프트 감지
```

---

## 정리: 핵심 개념 연결

```
[데이터 설계의 목표]
고품질 + 다양한 데이터로 더 신뢰할 수 있는 모델 만들기

   합성 데이터 생성          검증 모델
   (양과 다양성 확보)  ←→  (질 보증)
          ↓                    ↓
   [학습 데이터 풀]      [필터링]
          ↓
   [모델 학습/Fine-tuning]
          ↓
   [LLM-as-a-Judge로 평가]
          ↓
   [Hallucination 분석 & 완화]
          ↓
   [더 신뢰할 수 있는 모델]
```

데이터 설계는 단순히 데이터를 많이 모으는 문제가 아님.
**무엇을 가르칠 것인지(태스크), 어떻게 가르칠 것인지(커리큘럼), 얼마나 정확하게 가르칠 것인지(검증)** 를 모두 고려하는 공학적 과정.

---

*Section 6 완료. 다음: Section 7 — LLM Model Families (GPT, LLaMA, Mistral, Gemini 등의 계보와 특징)*
