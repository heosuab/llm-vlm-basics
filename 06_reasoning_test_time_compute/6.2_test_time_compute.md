# Section 6.2: Test-Time Compute

> **학습 목표**: 모델을 재학습하지 않고도 **추론 시간(inference time)에 더 많은 계산**을 투입하여 성능을 높이는 다양한 기법들을 이해한다. "Thinking harder, not just training harder" 패러다임.

---

## 배경: Training-Time vs Test-Time Compute

전통적인 관점: 더 좋은 모델을 만들려면 → 더 많은 데이터와 더 긴 학습

새로운 관점: 학습된 모델에서 더 좋은 답을 이끌어내려면 → **추론 시간에 더 많이 "생각"**

```
Training-Time Scaling:
  더 많은 데이터 + 더 큰 모델 + 더 긴 학습
  → 학습 비용 수백만 달러
  → 결과: 더 나은 기본 모델

Test-Time Scaling:
  같은 모델 + 더 많은 inference 연산
  → 추론 비용 증가 (쿼리당 10x ~ 1000x)
  → 결과: 어려운 문제에서 드라마틱한 성능 향상
```

---

## 6.2.1 Process Reward Model (PRM) vs Outcome Reward Model (ORM)

### Reward Model의 필요성

Test-time search (Best-of-N, MCTS 등)를 하려면 **어떤 추론 경로가 좋은지 평가하는 기준**이 필요하다. 여기서 Reward Model(RM)이 등장한다.

### ORM (Outcome Reward Model)

**최종 답만 보고 점수를 매기는** 방식이다.

```
추론 과정:
Step 1: x = 5
Step 2: y = x × 3 = 15   ← 이 단계가 잘못됐어도
Step 3: z = y + 2 = 17   ← ...이렇게 수정되어서
Step 4: w = z - 17 = 0   ← 최종 답: 0

ORM 입력: 최종 답 "0"
ORM 출력: [맞음 / 틀림] 또는 [확률 점수]
```

학습 레이블 수집: 쉽다 → 정답을 알고 있으므로 자동으로 레이블링 가능

단점:
- 잘못된 추론 과정도 "행운"으로 맞을 수 있음 → 나쁜 reasoning을 보상
- 중간에 어디서 틀렸는지 알 수 없음
- 실제로 틀린 경우 어느 단계가 문제였는지 피드백 없음

### PRM (Process Reward Model)

**각 추론 단계(step)마다 점수를 매기는** 방식이다.

```
추론 과정:
Step 1: 먼저 총 비용을 계산합니다.
        3개 × $5 = $15          [PRM 점수: +0.95]

Step 2: 세금 10%를 더합니다.
        $15 × 0.1 = $1.5        [PRM 점수: +0.92]

Step 3: 할인 20%를 뺍니다.
        $15 - $20 = -$5         [PRM 점수: -0.88] ← 오류 감지!
        (잘못됨: $15 × 0.2 = $3이 맞음)

Step 4: 최종 금액: -$5 + $1.5 = -$3.5  [PRM 점수: -0.95]
```

PRM의 장점:
- **오류 위치 정확히 파악 가능**
- 잘못된 경로를 조기에 차단 → 더 효율적인 search
- 부분적으로 올바른 reasoning도 적절히 보상

PRM의 단점:
- **레이블 수집이 매우 어렵다**: 각 단계가 맞는지 사람이 하나씩 확인해야 함
- 수학 문제의 경우 Step-level annotation은 ORM보다 훨씬 많은 노동 필요
- 단계를 어떻게 나누냐(granularity)에 따라 성능이 달라짐

### PRM 학습 데이터 수집: MATH Dataset 예시

OpenAI의 "Let's Verify Step by Step" 논문 (Lightman et al., 2023)에서 이 과정을 상세히 설명:

```
MATH 데이터셋 문제:
"2x + 3 = 7일 때 x의 값은?"

모델이 생성한 추론 체인:
Step 1: 양변에서 3을 뺍니다: 2x = 4    ← 사람이 평가: ✓ (맞음)
Step 2: 양변을 2로 나눕니다: x = 2     ← 사람이 평가: ✓ (맞음)
Step 3: 따라서 x = 2입니다.            ← 사람이 평가: ✓ (맞음)

다른 추론 체인:
Step 1: 양변에 3을 더합니다: 2x + 6 = 10  ← 사람이 평가: ✗ (틀림)
```

이처럼 step-level human annotation이 필요하므로 비용이 매우 크다. 이 때문에 **자동화된 PRM 학습** 연구가 활발히 진행되고 있다.

### PRM vs ORM 성능 비교

OpenAI 연구 (MATH 벤치마크, Best-of-N 설정):

```
N (샘플 수)  |  ORM 정확도  |  PRM 정확도
───────────────────────────────────────
     1       |    43.0%    |    43.0%
     4       |    50.2%    |    58.8%
    16       |    54.5%    |    67.4%
    64       |    57.3%    |    73.6%
   256       |    58.9%    |    78.2%
  1860       |    59.4%    |    78.6%
```

**PRM이 일관적으로 ORM을 크게 앞선다.** 특히 N이 클수록 차이가 뚜렷하다.

---

## 6.2.2 RLVR (RL with Verifiable Rewards)

### RLHF의 한계

기존 RLHF (Reinforcement Learning from Human Feedback)의 문제:
- 보상 신호가 **주관적 인간 선호도**에 의존 → 확장성 한계
- **Reward hacking**: 모델이 실제 품질 대신 사람 평가자를 속이는 법을 배울 수 있음
- **데이터 수집 비용**: 대량의 human annotation 필요

### 검증 가능한 태스크의 특수성

수학, 코딩, 논리 퍼즐 등의 태스크에서는 **정답 여부를 자동으로 검증**할 수 있다:

```
수학 문제:
정답: 42
모델 출력: "따라서 x = 42"
자동 검증: extract_number("따라서 x = 42") == 42 → True ✓

코딩 문제:
정답: 주어진 테스트 케이스를 통과하는 코드
모델 출력: def solution(n): return n*(n+1)//2
자동 검증:
  solution(5) == 15 → True ✓
  solution(10) == 55 → True ✓
  solution(100) == 5050 → True ✓
```

### RLVR 파이프라인

```
[사전 학습된 LLM]
        ↓
[정책 모델 (Policy Model)] ← 이 모델을 fine-tuning할 것
        ↓
  문제 풀기 시도
  {q₁, q₂, ..., qₙ} → {a₁, a₂, ..., aₙ}
        ↓
[Verifier (결정론적 검증기)]
  - 수학: 수치 답 비교
  - 코드: 테스트 케이스 실행
  - 체스: 규칙 준수 여부 확인
        ↓
보상 신호 r ∈ {0, 1} (또는 연속값)
        ↓
[RL 업데이트 (PPO, GRPO 등)]
  policy gradients로 correct solutions 확률 높이기
```

### GRPO (Group Relative Policy Optimization)

DeepSeek-R1에서 사용한 방법. PPO 대신 더 간단한 방식으로 RL을 적용한다:

```
같은 문제 q에서 G개의 샘플 생성:
{o₁, o₂, ..., oG}

각 샘플의 보상:
r₁ = 1 (맞음), r₂ = 0 (틀림), r₃ = 1, ..., rG = 0

그룹 내 상대적 어드밴티지 계산:
Aᵢ = (rᵢ - mean(r)) / std(r)

GRPO 목표:
L_GRPO = E[Σᵢ Aᵢ · log π_θ(oᵢ|q)]
```

일반 PPO 대비 장점:
- Value network 불필요 (추가 모델 없음)
- 같은 문제 내에서 상대 비교 → 더 안정적인 학습

### RLVR이 Self-Improvement를 가능하게 하는 이유

RLVR의 핵심 통찰:

1. **무한한 연습 문제 생성 가능**: 수학 문제는 자동 생성 가능
2. **자동 피드백**: 검증기가 즉시, 무한히 피드백 제공
3. **데이터 부트스트랩**: 모델이 생성한 correct solutions이 다음 학습 데이터가 됨

```
초기 모델 M₀
  ↓ 수학 문제 풀기
  정답인 (문제, 풀이) 쌍 수집
  ↓ RL 학습
개선된 모델 M₁
  ↓ 더 어려운 문제 풀기
  더 많은 정답 (문제, 풀이) 쌍 수집
  ↓ RL 학습
더 개선된 모델 M₂
  ...
```

이것이 바로 **자기 개선(self-improvement)** 루프이며, 이론적으로 human annotation 없이 무한히 성능 향상이 가능하다 (물론 실제 한계는 있음).

---

## 6.2.3 Long Chain-of-Thought (Long CoT) Training

### 짧은 CoT의 한계

일반적인 CoT fine-tuning에서 사용하는 reasoning chain은 보통 수십~수백 토큰이다. 매우 어려운 문제 (IMO 수준 수학, 복잡한 코딩)에서는 이 정도 추론으로는 부족하다.

### Long CoT의 핵심 특성

**Long CoT**는 수천 토큰에 달하는 매우 긴 reasoning chain을 통해 모델이 다음을 학습하도록 한다:

1. **탐색(Exploration)**: 여러 접근법을 시도해본다
2. **자기 검증(Self-verification)**: 중간 결과를 스스로 확인한다
3. **역추적(Backtracking)**: 막히면 되돌아가 다른 방법을 시도한다
4. **자기 수정(Self-correction)**: 오류를 발견하면 즉시 수정한다

```
Long CoT 예시 (수학 문제):

"이 문제를 풀려면... 먼저 x에 대해 정리해보자.
2x² + 5x - 3 = 0을 인수분해하면...

시도 1: (2x - 1)(x + 3) = 2x² + 5x - 3?
검증: 2x² + 6x - x - 3 = 2x² + 5x - 3 ✓

잠깐, x값을 구해야 하니까:
2x - 1 = 0 → x = 1/2
x + 3 = 0 → x = -3

확인해보자:
x = 1/2: 2(1/4) + 5(1/2) - 3 = 0.5 + 2.5 - 3 = 0 ✓
x = -3: 2(9) + 5(-3) - 3 = 18 - 15 - 3 = 0 ✓

따라서 x = 1/2 또는 x = -3"
```

### DeepSeek-R1의 접근법

DeepSeek-R1 (2025)은 Long CoT training의 대표적 성공 사례다:

**학습 과정:**
```
Stage 1: RLVR로 "cold start" (긴 CoT 생성 능력 부여)
  - 검증 가능한 수학/코딩 문제에 대해 GRPO 적용
  - 모델이 자연스럽게 길고 탐색적인 추론 체인을 생성하기 시작

Stage 2: Rejection Sampling Fine-tuning
  - Stage 1 모델이 생성한 long CoT 중 맞는 것만 수집
  - 이 데이터로 SFT

Stage 3: 추가 RLVR
  - 더 어려운 문제로 추가 강화
```

**핵심 발견**: 명시적으로 backtracking이나 self-verification을 가르치지 않았지만, **RLVR만으로 모델이 스스로 이런 행동을 발견**했다. "Aha moment" 현상이라고도 부름.

### Long CoT의 "Thinking Tokens" 개념

o1, DeepSeek-R1 등에서는 **생성하지만 사용자에게는 보여주지 않는** 특수한 thinking 토큰 구간을 둔다:

```
사용자 입력: "어려운 수학 문제..."

[<think>]                          ← 내부 사고 시작
"이 문제는 어려워 보인다. 우선..."
"...여러 접근법을 시도..."
"...아, 이 방법은 틀렸네, 다시 시도..."
[</think>]                         ← 내부 사고 끝

사용자에게 보이는 출력:
"풀이: 먼저 x를 정리하면... 답: 42"
```

내부 사고 과정에서는 모델이 매우 자유롭게 탐색할 수 있고, 최종 출력에서는 정제된 답변만 제공한다.

---

## 6.2.4 Best-of-N Sampling

### 가장 단순하고 강력한 Test-Time Scaling

**Best-of-N (BoN)**은 직관적이다:
1. N개의 서로 다른 completion을 샘플링한다
2. Reward model 또는 verifier로 가장 좋은 것을 선택한다

```
질문: [어려운 수학 문제]

샘플 1 (temperature=0.8): 추론... → 답: 42  (RM 점수: 0.95)
샘플 2 (temperature=0.8): 추론... → 답: 38  (RM 점수: 0.71)
샘플 3 (temperature=0.8): 추론... → 답: 42  (RM 점수: 0.88)
...
샘플 N (temperature=0.8): 추론... → 답: 41  (RM 점수: 0.62)

선택: 샘플 1 (RM 점수 0.95로 가장 높음)
최종 출력: 답 42
```

### Best-of-N의 수학적 분석

모델이 각 독립 샘플에서 정답을 낼 확률이 `p`라 하면:

```
N번 샘플 중 적어도 하나가 정답일 확률:
P(Best-of-N 성공) = 1 - (1-p)^N

예시: p = 0.3 (단일 시도 30% 성공률)
N = 1:  1 - 0.7^1  = 0.300 (30%)
N = 5:  1 - 0.7^5  = 0.832 (83%)
N = 10: 1 - 0.7^10 = 0.972 (97%)
N = 20: 1 - 0.7^20 = 0.999 (99.9%)
```

하지만 이건 이상적인 경우다. 실제로는 **완벽한 verifier가 없으므로** RM score가 높은 샘플이 항상 정답인 것은 아니다.

### 검증 방법 선택

| 검증 방법 | 적용 가능 태스크 | 정확도 | 비용 |
|---|---|---|---|
| **Exact Match** (수치/기호 비교) | 수학, 코딩 | 높음 | 매우 낮음 |
| **Code Execution** (테스트 통과) | 코딩 | 높음 | 낮음 |
| **ORM** (Outcome Reward Model) | 수학, 일반 | 중간 | 중간 |
| **PRM** (Process Reward Model) | 수학 | 높음 | 높음 |
| **LLM-as-a-Judge** | 오픈엔드 | 낮음~중간 | 높음 |

### BoN의 한계: Coverage vs Discrimination

BoN의 실제 성능은 두 요소에 의해 제한된다:

```
실제 BoN 정확도 ≤ min(Coverage, Discrimination)

Coverage: 모델이 N번 안에 정답을 생성하는 비율
Discrimination: RM이 정답 sample을 가장 높이 평가하는 비율
```

좋은 RM 없이는 N을 늘려도 성능이 포화된다.

---

## 6.2.5 Inference-Time Search (o1 / DeepSeek-R1 Style)

### "Thinking Tokens" 패러다임

OpenAI의 o1 (2024.09)과 DeepSeek-R1 (2025.01)의 핵심 혁신은 **추론 토큰을 test-time에 충분히 생성하도록 학습**한 것이다. 더 어려운 문제일수록 더 많은 "생각" 토큰을 사용한다.

```
쉬운 문제: "What is 2+2?"
  → [think: 4] → "4"  (수십 토큰)

중간 난이도 문제: "Solve: x² - 5x + 6 = 0"
  → [think: 인수분해... x=2 또는 x=3... 검증...] → "x = 2 or x = 3"  (수백 토큰)

어려운 문제: IMO 2024 Problem 6
  → [think: 긴 탐색... 여러 접근법... 역추적... 검증...] → 답  (수천 토큰)
```

### MCTS (Monte Carlo Tree Search) 스타일의 추론

o1이나 유사 시스템에서 사용되는 것으로 추정되는 MCTS 기반 search:

```
MCTS in Reasoning:

루트 노드: [문제]
   ↓ 여러 다음 단계 생성 (확장)
[Step A] [Step B] [Step C]
  ↓ PRM으로 평가
[가치: 0.9] [가치: 0.3] [가치: 0.7]
  ↓ 가장 유망한 노드 선택 (A)
[A] → [A1] [A2] [A3]
  ↓ 각 노드에서 rollout (끝까지 시뮬레이션)
  ↓ 결과를 역전파하여 노드 가치 업데이트
```

**MCTS의 4단계:**

1. **Selection**: UCB (Upper Confidence Bound) 기반으로 가장 유망한 노드 선택
   ```
   UCB(s) = Q(s) + c · √(ln(N(parent)) / N(s))
   Q(s): 예상 보상 (PRM 또는 rollout 결과)
   N(s): 방문 횟수
   c: 탐색/활용 균형 파라미터
   ```

2. **Expansion**: 선택된 노드에서 가능한 다음 단계들 생성

3. **Simulation (Rollout)**: 현재 상태에서 끝까지 진행하여 결과 얻기

4. **Backpropagation**: 결과를 상위 노드들에 역전파하여 가치 업데이트

### Beam Search over Thought Chains

MCTS보다 단순하지만 효과적인 방법:

```python
def beam_search_reasoning(problem, llm, prm, beam_width=4, max_steps=8):
    """
    Beam Search로 추론 경로 탐색
    beam_width: 동시에 유지하는 후보 경로 수
    """
    # 초기 beam: 빈 추론에서 시작
    beams = [{"steps": [], "score": 0.0}]

    for step in range(max_steps):
        all_candidates = []

        for beam in beams:
            # 현재 경로에서 가능한 다음 단계 생성
            next_steps = llm.generate_next_steps(
                problem,
                current_steps=beam["steps"],
                n_candidates=3
            )

            for next_step in next_steps:
                new_beam = {
                    "steps": beam["steps"] + [next_step],
                    "score": 0.0
                }

                # PRM으로 각 후보 경로 평가
                new_beam["score"] = prm.score(problem, new_beam["steps"])

                # 종료 조건 확인
                if is_terminal(next_step):
                    new_beam["is_complete"] = True

                all_candidates.append(new_beam)

        # 상위 beam_width개 경로만 유지
        beams = sorted(all_candidates, key=lambda x: x["score"], reverse=True)
        beams = beams[:beam_width]

        # 모든 beam이 완료되면 종료
        if all(b.get("is_complete", False) for b in beams):
            break

    # 최고 점수의 완료된 beam 반환
    completed = [b for b in beams if b.get("is_complete", False)]
    return max(completed, key=lambda x: x["score"])
```

### 검색 전략 비교

| 전략 | 탐색 패턴 | 메모리 | 최적성 보장 |
|---|---|---|---|
| **Greedy** | 가장 좋은 단일 경로 | O(d) | 없음 |
| **Beam Search** | 상위 k개 경로 병렬 | O(k·d) | 없음, 하지만 실용적 |
| **MCTS** | 방문 횟수 기반 균형 탐색 | O(전체 트리) | 시뮬레이션 수 → ∞ 시 최적 |
| **Best-of-N** | 독립 샘플 N개 | O(N) | 없음 |

---

## 6.2.6 Test-Time Compute Scaling Laws

### 핵심 질문: "더 많이 생각할수록 더 잘 풀 수 있는가?"

**답: 그렇다, 하지만 수익 체감의 법칙이 적용된다.**

### Scaling with Best-of-N

Best-of-N에서 N과 성능의 관계:

```
이론적 분석 (Snell et al., 2024 "Scaling LLM Test-Time Compute"):

[수학 문제 나이도 기준]

쉬운 문제 (기본 성공률 p = 0.9):
  N=1: 90%, N=4: 99.99%  → 조금만 늘려도 포화

중간 문제 (기본 성공률 p = 0.5):
  N=1: 50%, N=16: 99.9%  → 큰 향상 가능

어려운 문제 (기본 성공률 p = 0.1):
  N=1: 10%, N=64: 99.9%  → 많이 늘려야 효과
```

### Test-Time Compute vs Training-Time Compute

"Scaling LLM Test-Time Compute Optimally" (Snell et al., 2024)의 핵심 결과:

```
발견 1:
  작은 모델 + 많은 test-time compute
  ≈ 큰 모델 + 적은 test-time compute

예시:
  3B 모델 + 256x compute ≈ 34B 모델 + 1x compute
  (특정 수학 벤치마크 기준)
```

이 발견은 매우 중요한 함의를 가진다:
- 항상 더 큰 모델이 필요하지는 않다
- 추론 시간에 유연하게 compute를 조절할 수 있다
- 어려운 문제에는 더 많은 compute를, 쉬운 문제에는 적게

### Token Budget의 역할

o1 스타일 모델에서 **thinking token 수**와 성능의 관계:

```
문제 난이도에 따른 최적 thinking token 수:

AMC (쉬움):    ~100-500 thinking tokens로 충분
AIME (중간):   ~1000-3000 thinking tokens 필요
IMO (어려움):  ~5000-20000+ thinking tokens 필요
```

흥미롭게도, 단순히 thinking token을 강제로 늘리는 것보다는 **모델이 스스로 필요한 만큼 생각하도록** 학습하는 것이 중요하다.

### "Compute-Optimal" 전략

어떤 방법이 주어진 compute budget에서 가장 효율적인가?

```
Budget B tokens가 주어졌을 때:

옵션 1: Best-of-N
  각 샘플에 T/N 토큰, N개 샘플
  → 다양성은 높지만 각 샘플이 얕은 추론

옵션 2: Single Long CoT
  전체 T 토큰을 하나의 긴 추론에 사용
  → 깊은 탐색 가능

옵션 3: Beam Search with PRM
  T 토큰을 beam_width × step_tokens로 분배
  → 중간 수준의 균형
```

**실험적 결과** (Snell et al., 2024):
- 쉬운 문제: Best-of-N이 효율적 (첫 번째 시도가 거의 맞음)
- 어려운 문제: Beam Search + PRM이 Best-of-N보다 효율적

### 미래 방향: Adaptive Compute

이상적인 시스템은 **문제 난이도를 감지하고 compute를 동적으로 배분**한다:

```python
def adaptive_compute_solve(problem, llm, prm, verifier):
    # 1단계: 빠른 시도
    quick_answer = llm.generate(problem, max_tokens=500)

    if verifier.is_correct(quick_answer):
        return quick_answer, "easy"  # 쉬운 문제, 빠르게 해결

    # 2단계: 더 많은 compute 투입
    medium_answer = beam_search_reasoning(problem, llm, prm, beam_width=4)

    if verifier.is_correct(medium_answer["steps"]):
        return medium_answer, "medium"

    # 3단계: 최대 compute 투입
    best_answer = best_of_n(problem, llm, prm, n=64)

    return best_answer, "hard"
```

---

## 정리: Test-Time Compute 전략 선택 가이드

```
목표: 어려운 문제에서 성능 극대화

[검증 가능한 태스크 (수학, 코딩)]
  → RLVR로 모델 학습 (PRM 또는 verifier 기반)
  → 추론 시 Best-of-N (간단) 또는 Beam Search + PRM (효율적)

[검증 불가능한 태스크 (오픈엔드 글쓰기)]
  → LLM-as-a-Judge로 Best-of-N
  → 또는 Self-Consistency (구조화된 태스크인 경우)

[매우 어려운 문제 (IMO, 복잡한 코딩)]
  → Long CoT 학습 (DeepSeek-R1 스타일)
  → MCTS 또는 Beam Search over thought chains

[비용 제약이 있는 경우]
  → 문제 난이도 추정 후 adaptive compute 배분
  → 쉬운 문제에는 direct generation
  → 어려운 문제에만 expensive search 적용
```

---

*다음 파일: 6.3 Data Design — Synthetic Data, Verification Models, LLM-as-a-Judge, Bias & Hallucination*
