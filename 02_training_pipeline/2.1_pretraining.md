# 2.1 Pretraining

> LLM의 기본 언어 능력을 학습하는 Pretraining 단계.

---

## Training Pipeline Overview

```
[LLM Training Pipeline]

1) Data Collection & Curation
   └─ 대규모 텍스트 수집 → 정제 → 토크나이징

2) Pretraining (Self-Supervised Learning)
   └─ 다음 토큰 예측(next-token prediction) 등 언어 모델링 목표로 학습
   └─ 범용 언어 이해·생성 능력 형성

3) Supervised Fine-Tuning (SFT)
   └─ 지시문-응답 데이터로 지도학습
   └─ 지시 수행 능력 및 태스크 적합성 향상

4) Alignment (e.g., RLHF / DPO 등)
   └─ 인간 피드백 기반 선호 최적화
   └─ 안전성·유용성·정렬(alignment) 강화

5) (Optional) Domain / Task Fine-Tuning
   └─ 특정 도메인 데이터로 추가 미세조정


────────────────────────────────────────

[VLM Training Pipeline]

1) Multimodal Data Collection
   └─ 이미지-텍스트 페어 수집 및 정제

2) Modality-Specific Pretraining
   ├─ Vision Encoder 사전학습 (이미지 표현 학습)
   └─ Language Model 사전학습 (텍스트 표현 학습)

3) Multimodal Alignment / Bridging
   └─ 이미지-텍스트 표현 공간 정렬
   └─ Cross-attention 또는 Projection Layer 학습

4) Instruction Tuning (Multimodal SFT)
   └─ 이미지 기반 질의응답, 캡셔닝 등 지도학습

5) Alignment & Safety Tuning
   └─ 인간 피드백 기반 정렬 및 안전성 강화
```

---

## Next Token Prediction (Causal LM)

현대 LLM의 거의 대부분이 사용하는 pretraining objective.

### 목표

주어진 이전 token들로부터 다음 token을 예측함:

```
L = -Σ_t log p_θ(x_t | x_1, ..., x_{t-1})
```

**Teacher forcing**: 학습 시에는 예측한 token이 아닌 **정답 token**을 다음 입력으로 넣음. 이를 통해 각 position에서의 예측을 독립적으로 병렬 계산할 수 있음.

```python
# 하나의 시퀀스: ["The", "cat", "sat", "on", "the", "mat"]
inputs:  ["The", "cat", "sat", "on",  "the"]
labels:  ["cat", "sat", "on",  "the", "mat"]
# → 5개의 학습 예시를 하나의 시퀀스에서 동시에 학습
```

### 왜 Next Token Prediction이 강력한가?

- 레이블이 자동으로 생성됨 (labeled data 불필요)
- 모든 token이 학습 신호를 받음 (데이터 효율적)
- 텍스트 이해와 생성을 동시에 학습
- 세계 지식을 암묵적으로 압축하여 파라미터에 저장

---

## Denoising Objective (Masked LM)

BERT 계열 encoder-only 모델의 pretraining objective.

```
Input:  "The [MASK] sat on the [MASK]"
Target: "The cat  sat on the  mat"
```

**BERT의 마스킹 전략** (token 15% 선택 후):
- 80%: [MASK] token으로 교체
- 10%: 랜덤 token으로 교체 (노이즈)
- 10%: 원래 token 유지 (이미 정답 알고도 예측 학습)

**Next Sentence Prediction (NSP)**: BERT에 포함된 추가 objective였으나, 이후 연구(RoBERTa)에서 별로 도움이 되지 않는다고 밝혀짐.

---

## Span Corruption (T5)

T5(Text-to-Text Transfer Transformer)의 objective.

```
입력: "Thank you [X] me to your party [Y] week."
출력: "[X] for inviting [Y] last <EOS>"
```

연속된 token span을 sentinel token([X], [Y], ...)으로 대체하고, decoder가 원래 span을 복원함.

**CLM vs MLM vs Span Corruption 비교**:
| | CLM | MLM | Span Corruption |
|--|-----|-----|----------------|
| 방향성 | Left-to-right | Bidirectional | Both |
| Generation | ✅ 자연스러움 | ❌ 어색 | ✅ seq2seq |
| 효율 | 모든 token 학습 | 15%만 학습 | ~15% span 학습 |
| 대표 모델 | GPT, LLaMA | BERT | T5 |

---

## Data Deduplication

같은 텍스트가 여러 번 학습되면 모델이 그 내용을 memorization하여 hallucination이 증가하고 일반화 성능이 떨어집니다.

### Exact Deduplication

완전히 동일한 문서를 제거함. URL 기반 중복이나 hash 기반으로 처리함.

### Near-Duplicate Detection

**MinHash + LSH**:
1. 각 문서의 n-gram 집합에서 MinHash signature 계산
2. Locality Sensitive Hashing(LSH)으로 유사한 문서를 빠르게 찾음
3. Jaccard similarity가 임계값(예: 0.8) 이상인 문서 제거

**SimHash**:
문서를 d-bit hash로 변환하고, Hamming distance가 작은 것들을 near-duplicate로 처리함.

### 왜 중요한가?

- 중복이 많은 데이터: 특정 텍스트를 과도하게 memorization
- CC(Common Crawl) 웹 데이터에는 중복이 매우 많음
- 연구에 따르면 dedup만으로 perplexity가 수% 개선됨

---

## Data Filtering

Raw 웹 데이터에는 저품질 텍스트가 많음. 필터링으로 품질을 높임.

### Heuristic Filters

```python
# 예시 필터 조건들
if len(text) < 100:           remove   # 너무 짧음
if symbol_ratio > 0.1:        remove   # 특수문자 과다
if digit_ratio > 0.5:         remove   # 숫자만 가득
if line_dup_ratio > 0.3:      remove   # 중복 줄 과다
if not detect_language(text) == 'en':  remove  # 언어 필터
```

### Perplexity Filtering

높은 perplexity = 기존 언어 모델이 잘 예측하지 못하는 텍스트 → 저품질 또는 비자연어

```python
ppl = language_model.perplexity(text)
if ppl > threshold:  remove  # threshold: 데이터마다 조정
```

KenLM(n-gram LM)으로 빠르게 계산함.

### Classifier-Based Filtering

고품질 데이터(Wikipedia, 책 등)에 학습한 classifier로 각 문서의 품질 점수를 매깁니다.

```python
quality_score = classifier.predict_proba(text)[1]  # P(high quality)
if quality_score < 0.5:  remove
```

**CCNet, FineWeb**: Common Crawl을 이런 방식으로 정제한 대규모 데이터셋.

---

## Curriculum Learning

쉬운 예시에서 시작해 점점 어려운 예시를 학습하는 전략.

### 기본 아이디어

```
초기 학습: 짧고 단순한 문장들
중반부:   더 긴, 다양한 도메인의 텍스트
후반부:   긴 context, 복잡한 reasoning이 필요한 텍스트
```

### 구체적 구현

**길이 기반 커리큘럼**: 학습 초반에는 짧은 시퀀스만, 후반에는 긴 시퀀스도 포함
**도메인 기반 커리큘럼**: 기초 도메인 먼저, 전문 도메인 나중
**Long context warm-up**: 대부분의 학습을 짧은 context로, 마지막에 long context로 fine-tuning

LLaMA-3는 학습 마지막 단계에 long context data를 집중적으로 넣어 128K context를 지원함.

---

## Data Mixture Strategies

여러 도메인의 데이터를 어떤 비율로 섞을지는 모델 성능에 큰 영향을 줍니다.

### 전형적인 데이터 소스와 비율

| 소스 | 비율 (대략) | 특성 |
|------|------------|------|
| Web (CC, RefinedWeb) | 50-80% | 대용량, 중간 품질 |
| Books (Books3, Gutenberg) | 10-20% | 고품질, 긴 context |
| Code (GitHub, Stack) | 5-20% | 구조적, reasoning 향상 |
| Wikipedia | 2-5% | 고품질, 사실 정보 |
| Academic papers | 1-5% | 전문 지식 |
| Math/Science | 1-3% | reasoning 능력 강화 |

### 데이터 비율의 영향

- **코드 비율을 높이면**: reasoning, 수학 능력 향상 (LLaMA-3는 코드 비율 높임)
- **수학 데이터 포함**: GSM8K, MATH 성능 향상
- **다국어 데이터**: 비영어권 성능 향상

### DoReMi (2023)

데이터 도메인 비율을 자동으로 최적화하는 방법. Small proxy model로 각 도메인의 중요도를 추정하여 비율을 조정함.

### Practical Consideration

데이터 혼합은 empirical하게 결정되는 경우가 많습니다. 절대적인 정답이 없으며, target task가 무엇인지에 따라 최적 비율이 달라집니다.
