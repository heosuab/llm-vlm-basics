# 2.5 Optimization & Stability

> LLM 학습을 안정적이고 효율적으로 만드는 다양한 기법들을 정리합니다.

---

## Mixed Precision Training

### 왜 Mixed Precision을 쓰는가?

FP32(32-bit float)로 학습하면 정확하지만 메모리와 연산 비용이 큽니다.
Lower precision을 섞어 쓰면 속도와 메모리를 절약하면서도 품질을 유지할 수 있습니다.

### FP16 (Half-Precision Float)

```
범위: ±65504
정밀도: ~3 decimal digits
장점: FP32 대비 메모리 2배 절약, 2~4배 빠름
단점: overflow (값이 너무 크면) / underflow (값이 너무 작으면) 위험
```

LLM 학습에서 FP16는 gradient가 소실되는 문제가 있어 단독으로 쓰기 어렵습니다.

### BF16 (Brain Float 16)

```
범위: FP32와 동일 (exponent bit가 많음)
정밀도: ~2-3 decimal digits (FP16보다 낮음)
장점: overflow 없음, 현대 LLM 학습의 표준
단점: A100 이상 GPU 필요 (T4, V100 등 미지원)
```

**BF16이 LLM 학습 표준이 된 이유**: Transformer의 gradient는 작은 값보다 큰 범위를 다루는 것이 더 중요하므로, exponent bits가 많은 BF16이 FP16보다 안정적입니다.

### FP8 (8-bit Float)

H100 GPU에서 네이티브 지원합니다. E4M3(0~448 범위, 더 정밀)와 E5M2(넓은 범위)의 두 포맷이 있습니다.

DeepSeek-V3는 FP8 mixed-precision 학습으로 학습 비용을 크게 줄였습니다.

### Master Weights

```
학습 실제 구조:
  - Master weights (FP32): optimizer에서 파라미터 업데이트에 사용
  - Forward/Backward weights (BF16): 실제 forward/backward pass에 사용
  - Optimizer states (FP32): momentum, variance

업데이트 흐름:
  BF16 forward → BF16 gradient → FP32로 변환 → FP32 optimizer update → BF16으로 변환
```

---

## Gradient Clipping

### 문제

학습 중 gradient가 갑자기 폭발(explode)하면 loss가 NaN이 되고 학습이 망가집니다.

### Global Norm Clipping

```python
# 모든 파라미터의 gradient를 모아 전체 norm 계산
total_norm = sqrt(Σᵢ ||gᵢ||²)

# norm이 threshold를 초과하면 scale down
if total_norm > max_norm:
    scale = max_norm / total_norm
    for g in gradients:
        g *= scale
```

**일반적인 threshold**: 1.0 (LLaMA, GPT 계열 기본값)

Gradient clipping은 **방향은 유지하고 크기만 제한**합니다. 즉, 학습 방향은 바꾸지 않습니다.

---

## Gradient Checkpointing (Activation Recomputation)

### 문제

Backward pass를 위해 forward pass의 중간 activation을 모두 저장하면 메모리가 엄청납니다.

```
메모리: O(n_layers × batch × seq_len × d_model)
100 layers, batch=8, seq=2048, d=4096 → 수십 GB
```

### 해결책: 재계산

```
Forward pass:
  - 중간 activation을 저장하지 않음 (또는 일부만 checkpoint)
  - 입력만 저장

Backward pass:
  - 필요한 activation을 그 때그때 재계산 (forward 재실행)
```

**트레이드오프**:
- 메모리: O(n_layers × batch × seq × d) → O(√n_layers × batch × seq × d)로 감소
- 계산: Forward pass를 1.33배 정도 더 수행

큰 모델을 학습할 때는 거의 필수로 사용합니다.

---

## Gradient Accumulation

### 문제

Large batch size가 학습에 유리하지만, 메모리 제한으로 한 번에 큰 batch를 넣을 수 없습니다.

### 해결책

N번의 small batch에 걸쳐 gradient를 누적하고, N번마다 한 번만 parameter update합니다:

```python
optimizer.zero_grad()
for i, batch in enumerate(dataloader):
    loss = model(batch) / accumulation_steps
    loss.backward()  # gradient 누적 (zero_grad 하지 않음)

    if (i + 1) % accumulation_steps == 0:
        optimizer.step()  # N step마다 업데이트
        optimizer.zero_grad()
```

**Effective batch size** = physical_batch_size × accumulation_steps × num_GPUs

```
예시:
  Physical batch per GPU: 4
  Accumulation steps: 8
  GPUs: 64
  → Effective batch size: 4 × 8 × 64 = 2048
```

---

## EMA (Exponential Moving Average)

학습 중 파라미터의 smoothed 버전을 유지합니다:

```
θ_ema_t = α · θ_ema_{t-1} + (1 - α) · θ_t
α = 0.999  (보통)
```

**사용 목적**:
- Inference: EMA weights가 일반적으로 더 안정적인 성능
- Training 불안정성 완화: 갑작스러운 parameter 변화를 smooth하게 표현

EMA model은 별도로 저장하고, evaluation 때만 사용합니다. Gradient update에는 원래 θ_t를 사용합니다.

---

## Weight Decay

파라미터가 너무 커지지 않도록 L2 regularization을 적용합니다:

```
L_total = L_task + λ/2 · Σᵢ θᵢ²
```

AdamW에서는 weight decay가 gradient에 더해지지 않고 **파라미터에 직접 적용** (decoupled):

```
θ_{t+1} = (1 - η·λ) · θ_t - η · m̂_t / (√v̂_t + ε)
```

**중요**: Weight decay를 모든 파라미터에 적용하지 않습니다. **적용하지 않는 파라미터**:
- Bias terms
- LayerNorm/RMSNorm의 γ, β
- Embedding weights (선택적)

---

## Label Smoothing

One-hot 레이블 대신 **softened label**을 사용합니다:

```
y_smooth = (1 - ε) · y_onehot + ε / K

예시 (K=10, ε=0.1):
  일반 label: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
  Smoothed:   [0.01, 0.01, 0.91, 0.01, 0.01, ...]
```

**효과**:
- 모델이 "100% 확신"하지 못하게 함 → overconfidence 방지
- Calibration 개선
- 약한 regularization 효과

**단점**: 모델이 실제로 매우 확신할 수 있는 경우에도 억제됨

LLM에서는 ε=0.1 정도를 사용합니다.

---

## Loss Scaling

FP16 학습에서 gradient underflow를 방지합니다.

### 문제

FP16의 최솟값은 약 6×10⁻⁸입니다. 작은 gradient(예: 10⁻¹⁰)는 0으로 flush됩니다.

### 해결책

Forward pass 전에 loss를 큰 값(예: 2¹⁵)으로 곱하고, backward 후 gradient를 같은 비율로 나눕니다:

```python
loss_scaled = loss * scale_factor        # scale up
loss_scaled.backward()
for p in model.parameters():
    p.grad /= scale_factor               # scale down
# 이제 gradient가 FP16 범위 내에 있음
optimizer.step()
```

### Dynamic Loss Scaling

고정 scale_factor 대신 자동으로 조정:
- Gradient에 inf/nan이 없으면 → scale_factor를 2배로 키움
- inf/nan 발생하면 → scale_factor를 절반으로 줄이고 해당 step skip

BF16을 쓰면 loss scaling이 불필요합니다 (overflow 범위가 FP32와 동일).

---

## Optimizers

### AdamW 복습

현재 LLM 학습의 표준입니다. 자세한 내용은 Section 0.3 참조.

**LLM 학습 typical 설정**:
```
lr = 1e-4 ~ 3e-4 (pretraining), 1e-5 ~ 5e-5 (fine-tuning)
β₁ = 0.9
β₂ = 0.95 (LLaMA는 0.95, 일부 모델은 0.999)
ε = 1e-8
weight_decay = 0.1
```

### Adafactor

AdamW의 메모리를 크게 줄인 optimizer입니다.

**문제**: AdamW는 m_t, v_t를 저장하므로 파라미터 크기의 3배 메모리 필요

**Adafactor 해결책**: v_t를 저장하는 대신 **행렬의 rank-1 근사**를 저장합니다:

```
V_t ∈ ℝ^(d1 × d2) → r_t ∈ ℝ^(d1) + c_t ∈ ℝ^(d2)
메모리: d1 × d2 → d1 + d2 (행 + 열 분리)
```

**사용 사례**: T5, 초대형 모델에서 optimizer 메모리를 아낄 때

**단점**: AdamW보다 약간 불안정할 수 있음. LLaMA 등은 여전히 AdamW 사용.
