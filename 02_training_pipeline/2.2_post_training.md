# 2.2 Post-Training

> Pretraining으로 language ability를 갖춘 base model을 실제로 유용하게 만드는 단계입니다.

---

## Supervised Fine-Tuning (SFT)

### 개념

(instruction, response) 쌍으로 이루어진 데이터로 모델을 fine-tuning합니다.

```
{
  "instruction": "파이썬으로 피보나치 수열을 구하는 함수를 작성해줘",
  "response": "def fibonacci(n):\n    if n <= 1:\n        return n\n    return fibonacci(n-1) + fibonacci(n-2)"
}
```

### Loss Masking (핵심)

SFT에서는 **response 부분에만 loss를 계산**합니다:

```python
# tokens: [system_prompt... user_turn... assistant_response...]
# labels: [-100 ... -100 ... actual_tokens...]
#          ↑ 무시 (loss=0)    ↑ 학습 (loss 계산)

loss = cross_entropy(logits, labels, ignore_index=-100)
```

이렇게 해야 모델이 "user 질문 형태를 모방하는 것"이 아니라 "assistant로서 응답하는 것"을 학습합니다.

### Chat Template

각 모델마다 고유한 chat 형식이 있습니다. SFT 데이터는 반드시 이 형식을 따라야 합니다.

```
# LLaMA-3 형식
<|begin_of_text|>
<|start_header_id|>system<|end_header_id|>
You are a helpful assistant.<|eot_id|>
<|start_header_id|>user<|end_header_id|>
What is 2+2?<|eot_id|>
<|start_header_id|>assistant<|end_header_id|>
4<|eot_id|>
```

---

## Instruction Tuning

SFT의 한 형태로, 다양한 NLP task를 **instruction 형식으로 변환**하여 학습합니다.

```
# 번역 task
Instruction: "Translate the following to French: Hello world"
Response: "Bonjour le monde"

# 요약 task
Instruction: "Summarize this article in 2 sentences: [article]"
Response: "[summary]"
```

### FLAN (Finetuned Language Net)

Google의 접근법으로, 1000개 이상의 task를 instruction 형식으로 변환하여 multi-task 학습합니다.

**핵심 발견**: 다양한 task를 instruction으로 학습하면 **학습에 없던 task도** instruction을 보고 수행할 수 있습니다 (zero-shot generalization).

데이터 개수보다 **task의 다양성**이 더 중요합니다.

---

## Self-Instruct

**아이디어**: LLM 자신을 사용해 instruction data를 자동으로 생성합니다.

```
1. 소수의 seed instruction 작성 (175개)
2. LLM에게 "이와 비슷한 새로운 instruction을 만들어줘" 요청
3. 생성된 instruction 중 품질 낮은 것 필터링
4. LLM에게 각 instruction에 대한 response 생성
5. 다시 필터링
6. 이 데이터로 SFT
```

**대표 사례**:
- Alpaca (Stanford): GPT-3로 52K instruction 생성 → LLaMA fine-tuning
- WizardLM: "Evol-Instruct"로 instruction을 반복적으로 복잡하게 만들어 데이터 품질 향상

**한계**:
- Teacher 모델의 품질이 ceiling이 됨 (student ≤ teacher)
- 최근에는 GPT-4, Claude로 생성한 데이터 사용이 일반적

---

## Distillation

**Knowledge Distillation**: 큰 teacher 모델의 지식을 작은 student 모델로 전달합니다.

### Hard Label (Standard SFT)
```
L_hard = CrossEntropy(student_logits, teacher_hard_labels)
```
teacher의 최종 답변 token만 사용. 정보가 제한적입니다.

### Soft Label Distillation
```
L_soft = KL( softmax(teacher_logits/T) || softmax(student_logits/T) )
```
Temperature T로 softened된 teacher의 **전체 확률 분포**를 타겟으로 합니다.

Soft label이 더 많은 정보를 담습니다: "cat" 60%, "dog" 30%, "bird" 10%처럼 각 후보의 상대적 likelihood도 전달됩니다.

### Sequence-Level Distillation

Teacher가 생성한 output sequence 전체를 SFT 데이터로 사용합니다.
DeepSeek-R1은 자신의 긴 CoT 출력을 소형 모델(LLaMA-8B, Qwen-7B 등)에 distillation하여 작은 "reasoning 모델"을 만들었습니다.

---

## Rejection Sampling

**아이디어**: 모델의 여러 출력 중 가장 좋은 것만 SFT 데이터로 사용합니다.

```
1. 각 instruction에 대해 모델로부터 N개 샘플 생성 (N=10~100)
2. 각 샘플을 reward model 또는 verifier로 점수 매김
3. 가장 높은 점수의 샘플만 SFT 데이터로 선택
4. 이 filtered dataset으로 다시 SFT 학습
```

**Iterative Rejection Sampling**: 위 과정을 반복합니다.
- Step 1: 기본 모델로 샘플 생성 → 필터링 → SFT 학습
- Step 2: 개선된 모델로 샘플 재생성 → 필터링 → SFT 학습
- ...

LLaMA-2의 chat 버전 학습에 extensively 사용된 방법입니다.

---

## Multi-Turn Training

### 형식

여러 번의 대화(turn)로 이루어진 데이터:

```python
conversation = [
    {"role": "user",      "content": "What is Python?"},
    {"role": "assistant", "content": "Python is a programming language..."},
    {"role": "user",      "content": "Give me a simple example."},
    {"role": "assistant", "content": "Here's hello world: print('Hello, World!')"},
]
```

### 두 가지 학습 방식

**방식 1: 마지막 assistant turn만 학습**
```python
# 마지막 응답에만 loss 계산
loss_mask = [0, 0, 0, 0, 0, 1, 1, 1, ...]  # 마지막 assistant만
```

**방식 2: 모든 assistant turn 학습**
```python
# 모든 assistant 응답에 loss 계산
# 더 많은 학습 신호, 하지만 earlier turns가 later context에 영향
```

### 도전 과제

- **길이**: multi-turn 대화는 길어서 context window를 빨리 채움
- **일관성**: 이전 대화 내용을 기억하고 일관되게 응답해야 함
- **Role confusion**: 모델이 가끔 user 턴과 assistant 턴을 혼동
