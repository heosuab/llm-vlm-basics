# 2.3 Alignment / RLHF

> 모델이 사람의 의도와 가치에 맞게 행동하도록 조정하는 기법들을 다룹니다.

---

## Reward Modeling

### 목적

사람의 선호도(preference)를 수치 점수로 예측하는 모델입니다. RLHF 파이프라인의 핵심 구성 요소입니다.

### 학습 데이터: Pairwise Preference

같은 prompt에 대해 두 가지 응답 중 어떤 것이 더 나은지 사람이 판단합니다:

```
Prompt: "수도권 여행 추천해줘"
Chosen:  "경복궁, 남산타워, 인사동을 추천합니다. 각각의 특징은..."  ← 선호
Rejected: "서울 가세요"  ← 비선호
```

### Bradley-Terry Model

두 응답의 선호도 확률을 모델링합니다:

```
P(y_w ≻ y_l | x) = σ(r(x, y_w) - r(x, y_l))
```

- y_w: chosen (winner), y_l: rejected (loser)
- r: reward function

**학습 loss** (쌍간 ranking loss):
```
L_reward = -E[log σ(r(x, y_w) - r(x, y_l))]
```

Reward model은 보통 LLM base에 scalar head를 추가한 형태입니다.

---

## Preference Data Collection

좋은 preference data 없이는 좋은 reward model도, 좋은 RLHF도 없습니다.

### 수집 방식

**사람이 직접 비교 (Human preference)**:
- Annotator가 두 응답 중 하나를 선택
- 고품질이지만 비용이 높음
- InstructGPT, LLaMA-2 Chat에서 사용

**크라우드소싱**: Amazon Mechanical Turk, Scale AI 등 활용

**품질 관리**: annotator agreement (Fleiss' kappa), disagreement 처리

### 선호도 기준 (dimensions)
- Helpfulness: 질문에 얼마나 도움이 되는가
- Harmlessness: 유해하거나 위험한 내용이 없는가
- Honesty: 사실에 기반하고 불확실성을 인정하는가

---

## PPO (Proximal Policy Optimization)

### RLHF with PPO 파이프라인

```
1. Base LLM (pretrained)
   ↓ SFT
2. Policy model π_θ
   ↓ PPO
3. RLHF-tuned model

PPO 루프:
for each batch:
  1. π_θ로 응답 생성 (rollout)
  2. Reward model로 점수 계산
  3. KL penalty 추가: reward -= β · KL(π_θ || π_ref)
  4. PPO update: π_θ 파라미터 업데이트
```

### PPO Objective

```
L_PPO = E[ min(r_t(θ) · Â_t, clip(r_t(θ), 1-ε, 1+ε) · Â_t) ]

r_t(θ) = π_θ(a_t|s_t) / π_old(a_t|s_t)  # probability ratio
Â_t = GAE(rewards, values)  # generalized advantage estimate
ε = 0.2  # clipping 범위
```

**왜 PPO가 복잡한가**:
- Policy model, Reference model, Reward model, Value model → 4개의 모델 필요
- On-policy: 매번 새 샘플 생성 필요 (느림)
- Reward hacking에 취약 (KL penalty로 완화)

---

## DPO (Direct Preference Optimization)

2023년에 제안된 방법으로, **Reward model 없이** preference learning을 수행합니다.

### 핵심 아이디어

Reward model 학습 + PPO 두 단계를 **하나의 loss**로 합칩니다.

RLHF의 최적 policy를 closed-form으로 구하면:
```
r*(x, y) = β · log(π*(y|x) / π_ref(y|x)) + β · log Z(x)
```

이를 Bradley-Terry model에 대입하면:

```
L_DPO = -E[ log σ( β · log(π_θ(y_w|x)/π_ref(y_w|x))
                   - β · log(π_θ(y_l|x)/π_ref(y_l|x)) ) ]
```

**해석**:
- chosen 응답에 대한 log probability 증가
- rejected 응답에 대한 log probability 감소
- 두 변화의 차이를 reward model 없이 직접 계산

### PPO vs DPO

| | PPO | DPO |
|--|-----|-----|
| Reward model | 필요 | 불필요 |
| 모델 수 | 4개 | 2개 (policy + reference) |
| 학습 방식 | On-policy | Offline (static dataset) |
| 구현 복잡도 | 높음 | 낮음 |
| 성능 | 일반적으로 높음 | 충분히 경쟁적 |
| 사용 모델 | GPT 계열, LLaMA-2 | Mistral, Zephyr, Gemma 등 |

---

## GRPO (Group Relative Policy Optimization)

DeepSeek의 R1 학습에 사용된 방법으로, **Value model(critic) 없이** PPO를 근사합니다.

### 핵심 아이디어

각 prompt에 대해 G개의 응답을 샘플링하고, 그 중 상대적인 보상으로 baseline을 추정합니다:

```
For each prompt x:
  1. G개의 응답 {y_1, ..., y_G} 생성
  2. 각 응답의 reward {r_1, ..., r_G} 계산
  3. Advantage = (r_i - mean({r_j})) / std({r_j})

L_GRPO = -E[ Advantage_i · log π_θ(y_i|x) ] + β · KL(π_θ || π_ref)
```

**장점**: Value model(critic) 학습 불필요 → 메모리 절약, 구현 단순화

RLVR(Verifiable Rewards)와 결합: 수학 답변 정확성, 코드 실행 결과 등 binary reward를 사용합니다.

---

## KTO (Kahneman-Tversky Optimization)

**쌍(pair) 없이** 단일 피드백(good/bad)만으로 학습합니다.

```
데이터 형식: (prompt, response, label)
label: "good" or "bad"  (비교 없음)
```

Kahneman-Tversky의 prospect theory에서 영감: 손실이 이득보다 더 크게 느껴짐

```
L_KTO = E_good[ -log σ(r(x,y) - z_ref) ]  # good response: 보상 높이기
       + E_bad[ -log σ(z_ref - r(x,y)) ]   # bad response: 보상 낮추기
```

**장점**: Pair annotation보다 데이터 수집이 훨씬 쉬움

---

## RLAIF (RL from AI Feedback)

사람 대신 **LLM이 preference를 판단**합니다.

```
1. LLM (GPT-4, Claude 등)에게 두 응답 중 어느 것이 더 나은지 물음
2. 이 AI preference로 reward model 학습
3. PPO 또는 DPO로 policy 최적화
```

**장점**: 사람 annotation 비용 없음, 대규모 데이터 생성 가능
**단점**: AI의 편향이 그대로 전달됨, AI 자체의 한계도 전달됨

---

## Constitutional AI (Anthropic)

모델 자신이 헌법(constitution)에 따라 자기 응답을 비판하고 수정합니다.

```
1. 유해한 질문에 응답 생성 (harmful response 허용)
2. "이 응답이 [원칙 X]를 위반하는가?" 자기 비판
3. 원칙에 맞게 응답 수정 (revised response)
4. 원래 vs 수정된 응답으로 preference 학습 (RLAIF)
5. RL로 constitutional하게 행동하도록 최적화
```

Constitution 예시:
- "응답이 인종 차별적인가?"
- "응답이 허위 정보를 포함하는가?"
- "응답이 다른 사람에게 해가 될 수 있는가?"

---

## KL Regularization

Policy가 reference model(SFT model)에서 너무 멀리 벗어나지 못하게 제약합니다:

```
reward_total = r_reward_model(x, y) - β · KL(π_θ(y|x) || π_ref(y|x))
```

**왜 필요한가?**
- Reward hacking 방지: reward model은 불완전하므로, reward를 극대화하려다 보면 reward model을 "속이는" 비정상적인 출력을 학습할 수 있음
- Distribution collapse 방지: 모든 응답이 비슷해지는 현상

β 값:
- 작으면 → policy 자유도 높음, reward 극대화, reward hacking 위험
- 크면 → SFT model에 가깝게 유지, 안전하지만 RLHF 효과 줄어듦

---

## Policy vs Reward Overoptimization

**Goodhart's Law**: 측정치가 목표가 되는 순간, 그것은 더 이상 좋은 측정치가 아닙니다.

PPO training이 너무 많이 진행되면:
- Reward model score는 계속 올라가지만
- 실제 사람이 느끼는 quality는 오히려 떨어짐

**증상들**:
- 응답이 비정상적으로 길어짐 (length hack)
- 과도한 수식어, 과장된 표현
- 실제 도움이 안 되지만 "좋아 보이는" 텍스트 생성

**완화 방법**:
- KL penalty로 drift 제한
- Reward model ensemble (여러 RM의 평균)
- 학습 중 human evaluation으로 모니터링

---

## Calibration & Abstention

### Calibration

모델의 confidence가 실제 정확도와 일치하는 정도입니다.

```
잘 calibrated된 모델:
  "90% 확신" → 실제로 90%의 경우에 맞음

Overconfident 모델:
  "95% 확신" → 실제로는 70%만 맞음
```

**ECE (Expected Calibration Error)**: calibration 품질의 측정치

### Abstention (모른다고 말하기)

모델이 불확실할 때 틀린 답을 자신있게 말하는 것보다 "모른다"고 하는 것이 더 낫습니다.

훈련 방법:
```
# 확실한 경우
"The capital of France is Paris."

# 불확실한 경우
"I'm not certain, but I believe..."
"I don't have reliable information about this."
```

데이터에 명시적으로 abstention 예시를 포함시켜 SFT로 학습합니다.
