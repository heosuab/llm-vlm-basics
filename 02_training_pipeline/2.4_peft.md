# 2.4 Parameter-Efficient Fine-Tuning (PEFT)

> Full fine-tuning의 높은 비용 없이 모델을 특정 task에 적응시키는 방법들.

---

## 왜 PEFT가 필요한가?

7B 모델을 full fine-tuning하려면 약 60GB+ GPU 메모리가 필요함 (파라미터 + gradient + optimizer state).
PEFT는 전체 파라미터의 0.1~1%만 업데이트하면서 full fine-tuning에 준하는 성능을 냄.

| 방법 | 학습 파라미터 비율 | 메모리 절약 |
|------|-----------------|------------|
| Full fine-tuning | 100% | — |
| LoRA | ~0.1~1% | 매우 높음 |
| Adapters | ~0.5~5% | 높음 |
| Prefix tuning | ~0.1% | 높음 |
| Prompt tuning | <0.01% | 매우 높음 |

---

## LoRA (Low-Rank Adaptation)

2022년 Microsoft에서 제안. 현재 가장 널리 사용되는 PEFT 방법.

### 핵심 아이디어

Weight matrix의 변화량 ΔW를 **두 개의 작은 행렬의 곱으로 근사**함:

```
W' = W₀ + ΔW = W₀ + B · A

W₀ ∈ ℝ^(d × k)  ← 원래 weight (frozen)
B  ∈ ℝ^(d × r)  ← 학습
A  ∈ ℝ^(r × k)  ← 학습
r ≪ min(d, k)   ← rank (작은 수)
```

**파라미터 수 비교**:
- 원래: d × k (예: 4096 × 4096 = 16.7M)
- LoRA: (d + k) × r (예: (4096 + 4096) × 8 = 65K) → **256배 적음**

### 초기화

```python
A: kaiming_uniform (random)
B: zeros           ← 초기에 ΔW = B·A = 0 (학습 시작 시 원래 모델과 동일)
```

### 스케일링 팩터

```
output = W₀x + (α/r) · BAx
```

α는 scaling hyperparameter (보통 r과 같거나 2r). r이 바뀌어도 효과적인 learning rate가 유지됨.

### 어떤 레이어에 적용하나?

```
Transformer의 경우:
  - Attention: W_Q, W_K, W_V, W_O  ← 대부분 여기
  - FFN: W_up, W_down              ← 선택적
  - Embedding, LM head             ← 드물게
```

Rank 선택:
- r=4~8: 가벼운 adaptation
- r=16~64: 더 많은 파라미터, 더 큰 변화
- 도메인이 학습 데이터와 많이 다를수록 높은 r 권장

### Merge at Inference

LoRA 어댑터를 원래 weight에 합칩니다:
```
W_final = W₀ + B · A  ← 한 번에 merge
```
추론 시 추가 오버헤드 없음!

---

## QLoRA (Quantized LoRA)

2023년 Tim Dettmers 등이 제안. 70B 모델을 단일 48GB GPU에서 fine-tuning 가능하게 함.

### 3가지 혁신

**1. NF4 (NormalFloat4) Quantization**

Normal distribution을 가정하여 4-bit에서 최적의 quantization 수행:
```
기존 INT4: 값들이 선형적으로 분포
NF4:       Normal distribution에서 균등 분위수로 4-bit 표현
           → 실제 모델 weight 분포에 더 잘 맞음
```

**2. Double Quantization**

Quantization constants(scale factors)도 quantize:
```
NF4 weight: 각 64개 weight가 하나의 float32 scale factor를 공유
→ 이 scale factor도 다시 float8으로 quantize
→ 파라미터당 약 0.5 bit 추가 절약
```

**3. Paged Optimizers**

GPU 메모리가 부족해질 때 optimizer state를 CPU RAM으로 offload:
```
GPU 메모리 부족 → optimizer state를 CPU로 swap
필요할 때 → GPU로 다시 load
```

### 메모리 비교 (65B 모델 기준)

| 방법 | GPU 메모리 |
|------|----------|
| Full fine-tuning (BF16) | ~780 GB |
| LoRA (BF16) | ~160 GB |
| QLoRA (NF4) | ~48 GB ← 단일 A100! |

---

## Adapters

Transformer layer 사이에 **작은 bottleneck 모듈을 삽입**함.

```
x → LayerNorm(x + Attention(x))  →  Adapter  →  다음 레이어
x → LayerNorm(x + FFN(x))        →  Adapter  →  다음 레이어
```

### Adapter 구조

```
input (d) → down_proj (d → bottleneck_dim) → activation → up_proj (bottleneck_dim → d) → + residual
```

bottleneck_dim ≪ d 로 파라미터를 줄임.

### Houlsby vs Pfeiffer Adapter

**Houlsby**: attention 뒤와 FFN 뒤 두 곳에 삽입 (더 많은 파라미터)
**Pfeiffer**: FFN 뒤에만 삽입 (더 효율적)

추론 시 추가 레이어를 통과해야 하므로 latency가 약간 증가함.

---

## Prefix Tuning

각 Transformer 레이어의 **K, V sequence 앞에 학습 가능한 virtual token을 삽입**함.

```
원래:  Attention(Q, K, V)
→ 변경: Attention(Q, [prefix_K; K], [prefix_V; V])

prefix_K ∈ ℝ^(prefix_len × d_k)   ← 학습 가능
prefix_V ∈ ℝ^(prefix_len × d_v)   ← 학습 가능
```

모든 레이어에 독립적인 prefix가 있음.

**장점**: 입력을 변경하지 않고 모델의 behavior를 soft하게 제어
**단점**: inference 시 prefix token이 attention key/value sequence에 포함되어 약간의 비용 추가

---

## Prompt Tuning

Prefix tuning의 단순화 버전. **입력 embedding 레이어에만** learnable token을 추가함.

```
원래 입력: [token_1, token_2, ..., token_n]
Prompt Tuning: [soft_token_1, ..., soft_token_k, token_1, ..., token_n]
```

soft_token들은 vocabulary에 없는 **연속적인 벡터**로, gradient로 학습됨.

**특이한 점**: 모델이 클수록 prompt tuning의 성능이 full fine-tuning에 가까워짐. 10B+ 모델에서 효과적.

---

## IA³ (Infused Adapter by Inhibiting and Amplifying Inner Activations)

매우 적은 파라미터(LoRA보다도 적음)로 놀라운 성능을 냄.

**Attention에 학습 가능한 scaling vector 적용**:
```
K' = lₖ ⊙ K     # Key를 element-wise scale
V' = lᵥ ⊙ V     # Value를 element-wise scale
```

**FFN에도 적용**:
```
FFN'(x) = (lₘ ⊙ W₂·activation(W₁·x)) + b
```

lₖ, lᵥ, lₘ이 학습 가능한 벡터들.

**파라미터 수**: 매우 적음 (LoRA r=4보다도 적음)
**성능**: few-shot settings에서 competitive

---

## Catastrophic Forgetting Mitigation

Fine-tuning 후 pre-training에서 학습한 일반 지식이 손상되는 현상.

### LoRA as Implicit Regularizer

LoRA는 weight 변화를 low-rank space로 제한하므로, full fine-tuning보다 forgetting이 적습니다.

### EWC (Elastic Weight Consolidation)

Pre-training에서 중요한 파라미터를 보호함:
```
L_EWC = L_task + λ · Σᵢ Fᵢ · (θᵢ - θ*ᵢ)²
```
Fᵢ: 파라미터 i의 중요도 (Fisher information)
θ*ᵢ: pre-training 후 파라미터 값

중요한 파라미터는 많이 변경하면 큰 penalty, 덜 중요한 파라미터는 자유롭게 변경.

### Replay Buffer

Fine-tuning 시 pre-training 데이터를 일부 섞어서 학습함:
```
L = L_task(D_ft) + λ · L_pretrain(D_replay_subset)
```

간단하지만 효과적인 방법.
