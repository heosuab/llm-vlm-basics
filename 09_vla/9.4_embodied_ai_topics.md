# 9.4 Embodied AI Topics

> VLA를 실제 로봇 시스템에 적용할 때 마주치는 고급 주제들을 다룹니다.

---

## Tool Use in Embodied Settings

### 개념

언어 모델의 "tool use"(API 호출, 코드 실행)를 **로봇의 도구 사용**으로 확장합니다.

```
Text-based tool use:
  LLM → search_web("weather Tokyo") → 결과 → LLM → 응답

Embodied tool use:
  VLA → pick_up("pen") → physical action → success/failure → VLA → next action
```

### 도구의 종류

**물리적 도구**: 망치, 가위, 스패너 — grasp planning + manipulation 필요
**Virtual 도구**: 디스플레이 버튼 클릭, 키보드 입력 — 정밀한 포인팅 필요
**다른 로봇**: 로봇이 다른 로봇에게 작업 위임 (multi-robot collaboration)

### 언어로 도구 사용 명세

```
VLA에게 텍스트로 tool repertoire 설명:
  "screwdriver: turn screws clockwise/counterclockwise"
  "gripper: open/close to grasp objects"

LLM planning:
  "Remove the lid" → "Use gripper to grasp lid" → "Turn counterclockwise"
  → physical execution
```

---

## Hierarchical Planning

복잡한 long-horizon task를 **레벨별로 분해**합니다.

### 계층 구조

```
Level 3 (Goal):    "Make coffee"
Level 2 (Steps):   ["Get mug", "Add water to kettle", "Turn on kettle", "Pour water", "Add coffee"]
Level 1 (Actions): ["move_to(cabinet)", "open_cabinet()", "grasp(mug)", "close_cabinet()"]
Level 0 (Control): joint angles at 10Hz
```

### LLM as High-Level Planner

```python
prompt = """
Task: Make coffee
Available actions: pick(object), place(location), open(object), pour(from, to), press(button)
Current state: [image description]

Plan the next 3 steps:
"""

plan = llm.generate(prompt)
# → ["pick(mug)", "place(mug, coffee_machine)", "press(start_button)"]
```

**SayCan (Google, 2022)**: LLM의 language probability와 affordance model(현재 환경에서 실행 가능성)을 결합하여 실현 가능한 계획을 선택합니다:

```
score(action) = P_LLM(action | task) × P_affordance(action | current_state)
```

### Task Planning vs Motion Planning

**Task Planning**: 어떤 action을 어떤 순서로 할지 (symbolic level)
**Motion Planning**: 어떻게 움직일지 (continuous trajectory level)

VLA는 주로 task planning을 담당하고, motion planning은 classical robotics 방법(RRT, optimization-based)이 담당합니다.

---

## Language-Conditioned Manipulation

### 개념

자연어 명령을 이해하고, 그에 맞는 manipulation task를 수행합니다.

```
"Put the red cube in the blue bowl"
→ 색상 인식 (red, blue)
→ 물체 유형 인식 (cube, bowl)
→ 공간 관계 이해 (in)
→ pick(red_cube) → place(red_cube, inside blue_bowl)
```

### 주요 연구 방향

**Grounding**: 언어 표현을 특정 물체/위치와 연결
  - "third cup from the left" → 정확한 spatial reasoning
  - Open-vocabulary: 학습에 없던 물체도 언어로 지칭 가능

**Generalization levels**:
```
Level 1: 학습한 물체, 학습한 명령 → easy
Level 2: 새로운 물체, 학습한 명령 → medium (visual generalization)
Level 3: 학습한 물체, 새로운 명령 → medium (language generalization)
Level 4: 새로운 물체, 새로운 명령 → hard (compositional generalization)
```

RT-2가 Level 3-4에서도 동작함을 보인 것이 큰 의미가 있었습니다.

### 데이터 수집 방법

**Teleoperation**: 사람이 직접 로봇을 원격 조종하여 시연
  - 고품질이지만 느리고 비용이 높음
  - 손목 외골격(exoskeleton), VR controller 등 다양한 인터페이스

**Human video**: 사람의 손 동작 비디오에서 로봇 학습
  - 대량의 데이터 확보 가능하지만 embodiment gap 있음

**Simulation**: 시뮬레이터에서 자동으로 생성
  - 대규모 병렬 생성 가능하지만 sim2real gap 있음

**Self-supervised in real**: 로봇이 스스로 실험하며 학습 (강화학습)
  - 느리고 위험할 수 있음

---

## 현재 한계와 연구 방향

### 한계

**Data efficiency**: 사람처럼 적은 시연으로 배울 수 없음 (수백~수천 데모 필요)
**Precision**: 세밀한 조작 (바늘 꿰기, 나사 조이기)은 여전히 어려움
**Robustness**: 조명 변화, 물체 위치 변화에 민감
**Long-horizon failure recovery**: 중간에 실패했을 때 복구 어려움
**Real-world transfer**: 연구실 → 실제 환경의 격차

### 유망한 연구 방향

**Foundation models for robotics**: 대규모 multimodal 사전학습 → robot fine-tuning
**3D understanding**: 2D 이미지만이 아닌 3D point cloud, depth 활용
**Tactile sensing**: 촉각 정보를 action prediction에 통합
**Human-robot interaction**: 사람과 자연스럽게 협업하는 로봇
**Continual learning**: 새로운 task를 빠르게 배우면서 기존 skill 유지
