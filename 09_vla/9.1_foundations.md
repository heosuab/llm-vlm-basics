# 9.1 VLA Foundations

> Vision-Language-Action 모델의 기반이 되는 개념들을 설명합니다. Robot manipulation, embodied AI의 기초입니다.

---

## Behavior Cloning (BC)

### 개념

전문가(expert)의 행동 데모를 **지도학습으로 직접 모방**합니다. 가장 단순한 로봇 학습 방법입니다.

```
데이터: {(observation_t, action_t)} — 전문가의 (상태, 행동) 쌍
목표: π_θ(action | observation) 학습
손실: L = -log π_θ(a_expert | o)
```

**직관**: "전문가가 이 상황에서 이렇게 했으니, 나도 그렇게 하자"

**장점**:
- 간단하고 안정적인 학습
- 레이블 수집이 비교적 쉬움 (시연 데이터)
- 새로운 보상 함수 설계 불필요

**단점 - Covariate Shift**:
테스트 시 모델이 작은 오류를 범하면, 이후 상황이 학습 분포를 벗어납니다.
오류가 누적되어 큰 실패로 이어집니다(compounding error).

```
훈련 분포: 전문가가 방문한 상태들
테스트:    작은 오류 → 전문가가 본 적 없는 상태
           → 어떻게 행동해야 할지 모름 → 더 큰 오류
```

---

## Offline RL / Imitation Learning

### Offline Reinforcement Learning

환경과 직접 상호작용하지 않고, **미리 수집된 데이터셋으로만** 학습합니다.

```
Dataset: {(s_t, a_t, r_t, s_{t+1})} — 과거 경험 데이터
→ 환경 상호작용 없이 policy 개선
```

**Offline RL의 핵심 문제 - Distribution Shift**:
데이터셋에 없는 (s, a) 쌍에 대한 Q-value 추정이 부정확합니다.
→ Conservative Q-Learning (CQL), Implicit Q-Learning (IQL) 등으로 해결합니다.

### Imitation Learning vs Behavior Cloning

Imitation Learning은 BC보다 넓은 개념입니다:
- **BC**: 단순 지도학습으로 모방
- **IRL (Inverse RL)**: 전문가 행동에서 보상 함수 역추론
- **DAgger**: interactive하게 전문가 피드백을 받아 distribution shift 해결

---

## Autoregressive Action Modeling

LLM처럼 action을 **token by token으로 autoregressive하게 생성**합니다.

```
모델 입력: [image_tokens] [text_instruction_tokens]
모델 출력: [action_token_1, action_token_2, ..., action_token_k]
```

**Action tokenization**: 연속적인 action (6-DOF joint angles 등)을 discrete bin으로 양자화합니다:

```
joint_angle ∈ [-π, π] → 256개 bin으로 양자화
[−3.14, −3.12, ..., 3.14] → [0, 1, ..., 255]
```

**장점**: 기존 LLM 아키텍처와 학습 방법을 그대로 재사용 가능
**단점**: 연속 action의 정밀도 손실, 느린 autoregressive 생성

---

## Low-Level Control vs High-Level Planning

### 계층 구조

```
High-Level Planner (LLM/VLM):
  "냉장고를 열고 사과를 꺼내서 접시에 올려라"
  → 분해 → ["냉장고 문 열기", "사과 집기", "이동", "내려놓기"]

Low-Level Controller:
  "냉장고 문 열기" → joint angles [q1, q2, ..., q6] per timestep
  실시간 제어, 10-100Hz
```

**Low-level control 특징**:
- 연속적이고 정밀한 motor control
- 높은 주파수 (10-1000Hz)
- 물리적 제약 (joint limits, force limits)
- 센서 피드백에 즉각 반응

**High-level planning 특징**:
- 장기적 목표 분해
- 언어, 비전 이해
- 상황 인식, 추론
- 낮은 주파수 (0.1-10Hz)

### VLA의 도전

VLA는 이 두 가지를 하나의 모델로 처리하려 합니다. Language와 vision의 고수준 이해력을 갖추면서 동시에 정밀한 motor control도 해야 합니다.

---

## Simulation-to-Real Transfer (Sim2Real)

### 왜 Simulation인가?

Real robot 데이터 수집은:
- 비용이 매우 높음 (robot 하드웨어, 사람 시간)
- 위험할 수 있음 (물건 파손, 안전 문제)
- 느림 (real-time 속도로만 수집)
- 병렬화 어려움

Simulation에서는:
- 대규모 병렬 수집 가능 (수천 개 환경 동시)
- 리셋이 즉각적
- 안전함
- 대용량 데이터 수집 가능

### Sim2Real Gap

시뮬레이션과 현실의 차이로 인한 성능 저하입니다:
- **Visual gap**: 렌더링된 이미지 vs 실제 카메라 이미지
- **Physical gap**: 마찰, 유연성, 물리 특성의 차이
- **Sensor noise**: 시뮬레이션은 완벽하지만 실제는 노이즈 있음

**완화 방법**:
- **Domain randomization**: 시뮬레이션에서 texture, 조명, 물리 파라미터를 랜덤화
- **Domain adaptation**: 실제 이미지와 비슷하게 시뮬레이션 렌더링
- **Real data mixing**: 적은 양의 real data와 함께 학습

---

## World Models

**World model**은 환경의 다음 상태를 예측하는 모델입니다:

```
P(s_{t+1} | s_t, a_t) — action을 취한 결과를 예측
```

**활용**:

1. **Model-based planning**: 실제 환경을 건드리지 않고 world model 내에서 시뮬레이션
   ```
   현재 상태 s_t → world model → s_{t+1} 예측
               → 다양한 action 후보 탐색 → 최선 선택
   ```

2. **Imagination-based training**: world model에서 추가 경험을 생성하여 policy 개선 (Dreamer, etc.)

3. **Video prediction**: 비디오 시퀀스를 미래 프레임 예측으로 학습 → 물리 세계 이해

**최근 동향**: Large-scale video pretraining으로 학습된 world model이 robotics에 적용되고 있습니다 (UniSim, GR-1 등).
