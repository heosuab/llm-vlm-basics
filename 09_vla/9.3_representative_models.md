# 9.3 Representative VLA Models

> VLA 분야의 대표적인 모델들을 시간 순서와 기여점 중심으로 정리합니다.

---

## RT-1 (Robotics Transformer 1, Google 2022)

### 개요

Google Robotics의 첫 번째 대규모 로봇 학습 모델입니다.

### 아키텍처

```
입력:
  - 이미지 (카메라): EfficientNet-B3으로 인코딩
  - 텍스트 명령: USE (Universal Sentence Encoder)로 인코딩

Transformer:
  - TokenLearner로 시각 토큰 압축 (512 → 8 tokens)
  - Transformer decoder (causal, 8 layers)

출력:
  - Discrete action tokens
  - 11개 action dimensions (arm joint, gripper, base, etc.)
  - 각 dimension 256 bins
```

### 데이터

- 130,000개의 real robot 에피소드
- 700개 이상의 task
- 13개 로봇 (같은 형태의 UR5)

### 핵심 기여

- 대규모 multitask robot learning 가능성 입증
- 130K 에피소드로 97% 성공률 달성 (훈련된 task)
- Transformer 아키텍처를 로봇 제어에 성공적으로 적용

---

## RT-2 (Robotics Transformer 2, Google 2023)

### 개요

RT-1에서 한 발 더 나아가, **pretrained VLM을 기반**으로 로봇을 학습합니다.
LLM/VLM의 웹 지식을 로봇 제어로 전달합니다.

### 핵심 혁신: Visual-Language-Action (VLA)

```
Base model: PaLI-X (55B) 또는 PaLM-E (562B) VLM
→ robot action tokens를 vocabulary에 추가
→ robot 데모 데이터로 co-finetune (텍스트 + action을 같이)
```

### 결과

- **Emergent capabilities**: 학습에 없던 새로운 물체, 새로운 명령에 일반화
- "Move the energy drink to the left of the hay" → 학습된 적 없는 조합도 수행
- 웹에서 학습한 상식(e.g., 색 이름, 물체 카테고리)이 로봇으로 전이됨

**RT-1 vs RT-2**:
| | RT-1 | RT-2 |
|--|------|------|
| Base | From scratch | Pretrained VLM |
| Params | ~35M | 55B-562B |
| Web knowledge | ❌ | ✅ |
| Generalization | 제한적 | 높음 |

---

## Octo (UC Berkeley 2023)

### 개요

오픈소스 generalist robot policy입니다. 누구나 사용하고 fine-tuning할 수 있도록 설계되었습니다.

### 아키텍처

```
입력 처리 (modular):
  - RGB 이미지: patch-based tokenization
  - Language goal: T5 encoder
  - Robot proprioception: linear projection
  → 모두 transformer token으로 변환

Transformer backbone (GPT-style, ~93M params)

출력:
  - Diffusion Policy head (action chunk 예측)
  - 또는 MSE regression head
```

### 설계 철학

**Modular**: 다양한 robot 형태(embodiment)에 적응 가능
- 다른 카메라 수, 다른 action space
- 새 embodiment의 소량 데이터로 fine-tuning

### 학습 데이터

Open X-Embodiment dataset (800K+ 에피소드, 22개 로봇 타입)

---

## OpenVLA (Stanford 2024)

### 개요

Vision-Language-Action 아키텍처의 공개 baseline 모델입니다.

### 아키텍처

```
Vision Encoder: SigLIP (400M)
Projector: MLP
Language Model: Llama-2 7B (fine-tuned)

입력: [image] + [language instruction]
출력: action tokens (discrete, autoregressive)
```

### 특징

- **공개 가중치 + 학습 코드**: 연구 커뮤니티 접근성 높음
- **PRISMATIC VLM**: SigLIP + Llama-2 기반 VLM에 action head 추가
- Open X-Embodiment로 사전학습

### 한계

- Discrete tokenization → 정밀도 제한
- Autoregressive 특성 → 고주파 제어에는 느림 (6.7 Hz)

---

## π0 / π0-FAST (Physical Intelligence 2024)

### 개요

Physical Intelligence(π0 as company)에서 개발한 generalist robot policy입니다. Diffusion-based action prediction으로 높은 dexterity를 달성합니다.

### π0 아키텍처

```
Base: PaliGemma (3B) — 이미지 + 텍스트 이해

Action prediction: Flow matching
  - robot state + language + image → action chunk (50 steps)
  - Flow matching으로 continuous action 예측

Key innovation: "Action Expert" 모듈
  - Language model token과 별도로 action에 집중하는 transformer 블록
  - VLM의 텍스트 세계와 로봇의 물리 세계를 분리
```

### 훈련 전략

**Pre-training**: 10,000 시간 이상의 diverse robot 데이터
**Post-training**: 특정 task에 few-hour fine-tuning

### π0-FAST

π0의 빠른 버전입니다:
- **Discrete action tokenization**: 연속값 대신 token
- **Autoregressive generation**: 하지만 특수 token 형식으로 빠른 생성
- RT-2의 속도 + Diffusion Policy의 품질 목표

### 능력

- Laundry folding, table setting, package assembly 등 고도로 dexterous한 task
- Long-horizon manipulation (10+ 단계 작업)
- Language conditioning으로 다양한 task 처리

---

## Multi-Task Robotic Policies

### Cross-Embodiment Learning

다양한 로봇 플랫폼(UR5, Franka, Spot, etc.)에서 수집된 데이터를 함께 학습합니다.

**Open X-Embodiment (OXE)**: 22개 이상의 로봇 플랫폼, 100만+ 에피소드의 공개 데이터셋

**도전**:
- 각 로봇마다 action space 다름 (joint 수, 범위 등)
- Camera setup 다름
- Task diversity 처리

### Generalist vs Specialist

| | Generalist (RT-2, Octo) | Specialist (Task-specific) |
|--|-------------------------|---------------------------|
| 일반화 | 높음 | 낮음 |
| 특정 task 성능 | 중간 | 높음 |
| 학습 데이터 | 대규모 다양 | 소규모 집중 |
| Fine-tuning 효율 | 높음 (좋은 init) | — |

**현실적 접근**: Generalist를 pretraining으로 사용하고, task-specific fine-tuning으로 성능을 높입니다.
