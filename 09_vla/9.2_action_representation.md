# 9.2 Action Representation

> VLA에서 action을 어떻게 표현하고 예측할지에 대한 핵심 설계 선택들을 다룹니다.

---

## Action Tokenization: Continuous vs Discrete

### Discrete Action Tokenization

연속적인 action 값을 **유한한 bin으로 양자화**합니다:

```python
# 예: 각 joint angle [-π, π]를 256개 bin으로 양자화
num_bins = 256
action_min = -np.pi
action_max = np.pi

def tokenize_action(action):
    bin_idx = int((action - action_min) / (action_max - action_min) * num_bins)
    return np.clip(bin_idx, 0, num_bins - 1)

def detokenize_action(token):
    return action_min + (token / num_bins) * (action_max - action_min)
```

**장점**:
- 기존 LLM vocabulary에 action token 추가 가능
- Autoregressive generation 그대로 사용
- Cross-modal training (텍스트 + action) 자연스러움

**단점**:
- 연속 값의 정밀도 손실
- 6-DOF arm의 경우: 6 dimensions × 256 bins = 1536 tokens/step
- Autoregressive 특성상 생성이 느림 (특히 고주파 제어)

---

### Continuous Action Prediction

action을 연속 벡터로 직접 회귀합니다:

```python
# LLM의 마지막 hidden state에 regression head 추가
action = nn.Linear(d_model, action_dim)(hidden_state)  # (batch, action_dim)
loss = F.mse_loss(action, target_action)
```

**장점**:
- 정밀도 손실 없음
- 생성이 빠름 (한 번의 forward pass)

**단점**:
- Multimodal action distribution 처리 어려움 (같은 상황에서 여러 valid action이 있을 때)
- Autoregressive 텍스트 생성과 다른 output head 필요

---

## Diffusion Policy

### 핵심 아이디어

Action을 **diffusion model로 예측**합니다. Random noise에서 시작하여 반복적으로 denoising하여 action sequence를 생성합니다.

```
a_T ~ N(0, I)  ← pure noise
a_{T-1} = denoise(a_T, condition=obs, t=T)
a_{T-2} = denoise(a_{T-1}, condition=obs, t=T-1)
...
a_0  ← 최종 action sequence
```

### 왜 Diffusion인가?

**Multimodal action distribution 처리**: 같은 상황에서 여러 equally valid한 action이 있을 때, 단순 regression은 평균을 예측하여 실제로는 invalid한 action을 출력합니다. Diffusion은 여러 mode를 모두 포착할 수 있습니다.

```
예: 물건을 집는 방법이 왼쪽에서 접근 or 오른쪽에서 접근 두 가지 있을 때
  MSE regression: 두 방법의 평균 → 어느 쪽도 아닌 잘못된 trajectory
  Diffusion: 두 mode 중 하나를 일관되게 선택
```

### DDPM vs DDIM

**DDPM (Denoising Diffusion Probabilistic Models)**:
- 수백 step의 denoising 필요
- 느리지만 높은 품질

**DDIM (Denoising Diffusion Implicit Models)**:
- 10-100 step으로 줄임
- 로봇 제어 실시간성을 위해 필요

### 실제 구현 (Chi et al. 2023)

```python
class DiffusionPolicy(nn.Module):
    def __init__(self):
        self.noise_pred_net = ConditionalUNet()
        self.noise_scheduler = DDPMScheduler(num_train_timesteps=100)

    def forward(self, obs, noisy_action, timestep):
        # obs: 관찰 (이미지 + robot state)
        # noisy_action: 현재 denoising 중인 action
        # timestep: diffusion 단계
        noise_pred = self.noise_pred_net(noisy_action, obs, timestep)
        return noise_pred

    def predict_action(self, obs, num_inference_steps=10):
        action = torch.randn(action_shape)  # noise
        for t in reversed(range(num_inference_steps)):
            noise_pred = self.forward(obs, action, t)
            action = self.noise_scheduler.step(noise_pred, t, action).prev_sample
        return action
```

---

## Flow Matching for Action Prediction

Diffusion의 대안으로, 더 빠른 생성과 더 단순한 훈련을 제공합니다.

### 개념

Noise에서 데이터로의 **직선 경로(straight path)**를 학습합니다:

```
Flow: x_t = (1 - t) × noise + t × data  (t: 0→1)
학습 목표: v_θ(x_t, t) = data - noise  (velocity 예측)

inference:
  x_0 ~ N(0, I)
  x_1 = x_0 + ∫₀¹ v_θ(x_t, t) dt  ← ODE solver
```

**Diffusion vs Flow Matching**:
| | Diffusion | Flow Matching |
|--|-----------|---------------|
| 학습 | noise prediction | velocity prediction |
| 경로 | 곡선 | 직선 |
| Inference steps | 10-100 | 1-10 (더 빠름) |
| 품질 | 높음 | 동등 또는 더 나음 |

π0 (Physical Intelligence)가 flow matching을 action prediction에 적용합니다.

---

## Chunked Action Prediction

매 timestep마다 하나의 action을 예측하는 대신, **여러 step의 action을 한 번에 예측**합니다:

```
Prediction horizon H = 10 (미래 10 timestep의 action을 한 번에 예측)

[obs_t] → model → [a_t, a_{t+1}, ..., a_{t+H-1}]
```

### 장점

1. **Temporal consistency**: 연속된 action들이 일관되게 계획됨
2. **Inference 효율**: H timestep마다 1번만 model inference (10배 빠름)
3. **Temporal correlation 포착**: diffusion/flow matching과 결합 시 더 자연스러운 trajectory

### Receding Horizon Control

전체 chunk를 실행하지 않고 앞부분만 실행합니다:

```
예측: [a_t, ..., a_{t+9}]  (H=10)
실행: [a_t, a_{t+1}]       (execute only first 2)
다음 step에서 재예측: [a_{t+2}, ..., a_{t+11}]
```

실시간 피드백을 반영하면서도 temporal consistency를 유지합니다.

**Execution horizon T_a**: 실제로 실행하는 step 수. T_a < H이면 더 자주 재계획합니다.
