# 7.2 LLM 발전의 기여 분석: 아키텍처 · 학습 · 효율성

> 이 문서는 LLM 발전의 "왜"와 "어떻게"를 구조적으로 분석한다. 단순한 타임라인이 아니라, **무엇이 바뀌었고, 왜 바뀌었으며, 그 결과가 무엇이었는지**를 추적한다.

---

## 목차

1. [아키텍처 혁신 타임라인](#1-아키텍처-혁신-타임라인)
2. [Training 전략의 진화](#2-training-전략의-진화)
3. [세대별 해결한 문제](#3-세대별-해결한-문제)
4. [효율성 진화: Parameter-per-Performance](#4-효율성-진화-parameter-per-performance)
5. [연구자를 위한 핵심 교훈](#5-연구자를-위한-핵심-교훈)

---

## 1. 아키텍처 혁신 타임라인

아키텍처 혁신은 크게 다섯 가지 축으로 분류할 수 있다:

1. **Attention 메커니즘 효율화**
2. **FFN/Expert 구조 개선 (MoE)**
3. **Positional Encoding 발전**
4. **Activation Function 개선**
5. **Normalization 전략**

---

### 1.1 Attention 메커니즘 효율화

Attention은 LLM의 핵심이지만, 동시에 **메모리 병목**이기도 하다. 특히 긴 시퀀스에서 KV cache 크기가 `O(n × d_model × n_layers)`로 선형 증가하며, 추론 속도와 메모리를 지배한다.

#### 표준 Multi-Head Attention (MHA) — 기준선

```
Q: [n_heads × seq_len × d_head]
K: [n_heads × seq_len × d_head]  ← n_heads 개의 독립적 KV cache
V: [n_heads × seq_len × d_head]
```

- **문제**: `n_heads` 개의 K, V를 모두 캐시해야 하므로 메모리 사용량 많음
- **적용 모델**: GPT-1, GPT-2, GPT-3, LLaMA-1 등 초기 모델

#### Multi-Query Attention (MQA) — PaLM (2022), Falcon (2023)

```
Q: [n_heads × seq_len × d_head]
K: [1 × seq_len × d_head]       ← 단 하나의 KV head
V: [1 × seq_len × d_head]
```

- **혁신**: 모든 query head가 **단일 K, V를 공유**
- **효과**: KV cache `n_heads`배 감소. 추론 속도 대폭 향상.
- **단점**: 표현력 감소 (일부 연구에서 성능 하락 보고)
- **적용**: PaLM, Falcon, Gemini 1.5

#### Grouped Query Attention (GQA) — LLaMA-2 70B (2023), 이후 표준화

```
Q: [n_heads × seq_len × d_head]
K: [n_groups × seq_len × d_head]  ← 그룹당 하나의 KV head
V: [n_groups × seq_len × d_head]
n_heads / n_groups = 4 (예: 32 Q heads, 8 KV heads)
```

- **혁신**: MHA(n_heads KV)와 MQA(1 KV)의 **중간점**을 찾음
- **효과**: MHA 대비 KV cache 대폭 감소, MQA 대비 표현력 유지
- **결론**: GQA는 거의 모든 최신 오픈소스 모델의 표준이 됨
- **적용**: LLaMA-2 (70B), LLaMA-3 (전 크기), Mistral-7B, Qwen2, Gemma 2

#### Sliding Window Attention (SWA) — Mistral-7B (2023)

```
각 토큰이 attend하는 범위를 [i-W, i]로 제한 (W: window size)
표준 attention: O(n²) → SWA: O(n × W)
여러 레이어 통과 시 실질적 receptive field: W × n_layers
```

- **혁신**: Quadratic complexity를 Linear로 감소
- **효과**: 이론적으로 무제한 컨텍스트 처리 가능 (실제로는 제한 있음)
- **단점**: 먼 거리 토큰 정보가 희석될 수 있음
- **적용**: Mistral-7B, Mixtral 8x7B, Gemma 2 (Global과 교대 배치)

#### MLA (Multi-head Latent Attention) — DeepSeek-V2 (2024)

```
표준 MHA KV cache: [n_heads × seq_len × d_head × 2]
MLA: [seq_len × d_c]  (d_c << n_heads × d_head)

Down-projection: h → c_KV  (dim 압축)
Up-projection: c_KV → K, V  (추론 시 복원)
```

- **혁신**: KV cache를 **low-rank latent vector로 압축**, 추론 시 필요할 때만 복원
- **효과**: KV cache **93.3% 감소** (DeepSeek-V2 기준)
- **수학적 관계**: Decoupled RoPE와 함께 사용하여 positional encoding과 분리
- **적용**: DeepSeek-V2, DeepSeek-V3, DeepSeek-R1

#### Global + Local Attention 교대 — Gemma 2 (2024)

```
짝수 레이어: Global Attention (전체 시퀀스 attend)
홀수 레이어: Local SWA (window 4096 attend)
```

- **혁신**: 짧은 범위의 세부 패턴과 장거리 의존성을 **레이어별로 분담**
- **효과**: 효율성(SWA)과 표현력(Global) 동시 확보

---

**Attention 혁신 타임라인 요약**

```
2017  MHA (Transformer 원본)
2022  MQA (PaLM) — KV cache 최소화
2023  GQA (LLaMA-2 70B) — MHA/MQA 절충, 표준화
2023  SWA (Mistral-7B) — Linear attention complexity
2024  MLA (DeepSeek-V2) — Latent KV compression
2024  Global+Local hybrid (Gemma 2) — 레이어별 분담
```

---

### 1.2 Mixture of Experts (MoE): FFN의 진화

Dense 모델은 모든 파라미터를 모든 토큰에 적용한다. MoE는 **"전문화된 서브네트워크 중 일부만 활성화"**하여 파라미터 수를 늘리면서 계산량을 유지하는 방법이다.

#### 기본 MoE 구조

```
입력 토큰 x → Router (linear) → Top-K expert 선택
선택된 experts FFN 출력의 weighted sum → 다음 레이어
```

#### MoE 발전의 주요 단계

**Step 1: Switch Transformer (Google, 2021) — MoE의 실용화**
- Top-1 routing (각 토큰이 단 1개 expert에게만 전달)
- **Load balancing loss**: 특정 expert에 토큰이 몰리는 "expert collapse" 방지
- 학습 안정성 문제 → BF16 precision + 안정화 기법

**Step 2: Mixtral 8x7B (Mistral, 2023) — 오픈소스 MoE 표준화**
- **Top-2 routing**: 각 토큰이 2개 expert 선택 (성능/효율 균형)
- 8개 experts, FFN 부분만 MoE (attention은 dense)
- 46.7B total / 12.9B active: 추론 시 13B 급 비용, 70B급 성능

**Step 3: DeepSeekMoE (DeepSeek, 2024) — Fine-grained MoE**
```
기존 MoE: 적은 수의 큰 expert
DeepSeekMoE: 많은 수의 작은 expert (fine-grained)

추가 혁신: Shared Expert + Routed Expert
- Shared expert (K_s개): 항상 활성화, 공통 지식
- Routed expert (N개 중 K_r개 선택): 토큰별 특화
```
- **장점**: Expert 특화도 향상, 전체 파라미터 활용 효율 향상
- **Auxiliary-loss-free load balancing** (V3): 학습 목적함수 단순화

**Step 4: Gemini 1.5 / GPT-4 (추정) MoE**
- 세부사항 비공개이나, 대규모 MoE로 추정
- 1M context + MoE 조합으로 성능/효율 극대화

---

**MoE 핵심 트레이드오프**

| 측면 | 장점 | 단점 |
|------|------|------|
| **파라미터** | 동일 계산량 대비 더 많은 파라미터 | 전체 모델 크기가 커서 메모리 필요 |
| **계산량** | 추론 시 일부 expert만 활성화 | 분산 학습/추론에서 통신 오버헤드 |
| **성능** | 특화된 expert로 표현력 향상 | Load balancing 실패 시 성능 저하 |
| **학습** | 더 적은 FLOPs로 학습 가능 | 학습 불안정성, expert collapse 위험 |

---

### 1.3 Positional Encoding 발전

언어는 순서가 있으므로 Transformer는 위치 정보를 주입해야 한다.

#### Absolute Positional Embedding (GPT-1, 2, 3, BERT)
- 각 위치에 학습된 embedding vector 추가
- **문제**: 학습 시 보지 못한 길이(out-of-distribution length)로 일반화 불가

#### Sinusoidal PE (원본 Transformer)
- 수식 기반의 고정된 패턴
- **문제**: 여전히 특정 컨텍스트 길이에 묶여 있음

#### ALiBi (Attention with Linear Biases, 2022)
```
Attention score += -m × (i - j)  (m: head별 slope, i-j: 상대 위치 거리)
```
- 학습 길이 이상으로 extrapolation 가능
- **한계**: 성능이 RoPE 기반 모델에 비해 약함 (일부 태스크)

#### RoPE (Rotary Position Embedding, 2021) — 현재 표준

```
Q, K 벡터에 회전 변환 적용
<q_i, k_j> = <R(θ_i) × q, R(θ_j) × k> = <q, R(θ_{i-j}) × k>

결과: Attention이 자동으로 상대 위치(i-j) 의존적으로 작동
```

- **장점**:
  - Relative position이 자연스럽게 인코딩됨
  - Linear attention과 호환
  - 학습 길이 이상으로 제한적 extrapolation 가능
- **적용**: LLaMA 전 버전, Mistral, Qwen, DeepSeek 등 대부분의 현대 모델

#### RoPE 확장 기법들 (Long Context를 위해)

**1. RoPE base theta 변경** (LLaMA-3.1)
```python
# LLaMA-2: theta = 10000
# LLaMA-3.1: theta = 500000
# theta가 클수록 더 긴 거리 위치 구분 가능
position_encodings = [theta^(-2i/d) for i in range(d//2)]
```

**2. YaRN (Yet another RoPE extensioN)** — Mistral 계열에서 사용
```
고주파 차원: interpolation 없이 유지 (이미 충분한 해상도)
저주파 차원: 선형 보간 (긴 거리 정보 담당)
```

**3. NTK-aware scaling** (Qwen, Code Llama)
```
신경 탄젠트 커널(NTK) 이론에 기반
theta를 스케일 팩터에 따라 조정하여 외삽 가능
```

---

### 1.4 Normalization 전략

#### Post-Norm (원본 Transformer)
```
output = LayerNorm(x + sublayer(x))
```
- **문제**: 초기 학습에서 gradient vanishing/exploding 발생 → warm-up LR이 필수

#### Pre-Norm (GPT-2, LLaMA 계열)
```
output = x + sublayer(LayerNorm(x))
```
- **장점**: 학습 안정성 향상, 깊은 모델에서 더 잘 동작
- **단점**: 이론적으로 표현력 약간 손실 가능성 (실제로는 거의 영향 없음)

#### RMSNorm (LLaMA-1 이후 표준화)
```
RMSNorm(x) = x / sqrt(mean(x²) + ε) × weight
# LayerNorm에서 평균을 빼는 centering 제거
```
- **장점**: LayerNorm보다 약 15% 빠른 계산, 유사한 성능
- **수학적 근거**: 크기(scale) 제어만으로도 충분; 평균 이동(shift) 불필요

#### Logit Soft-Capping (Gemma 2 혁신)
```python
# Attention logits와 final logits 모두에 적용
capped_logit = cap_value * tanh(logit / cap_value)
# 예: cap_value = 50 (attention), 30 (final)
```
- **혁신**: logit 값이 극단적으로 커지는 것을 부드럽게 제한
- **효과**: 학습 안정성 향상, gradient explosion 방지
- **의의**: 매우 단순한 연산이지만 학습 안정성에 큰 기여

---

### 1.5 Activation Functions

#### ReLU → GeLU → SwiGLU

```
ReLU(x) = max(0, x)
GeLU(x) = x × Φ(x)  (Φ: 누적 정규분포)
SwiGLU(x, W, V) = Swish(xW) ⊗ (xV)
                = (xW × σ(xW)) ⊗ (xV)
```

- **SwiGLU (Noam Shazeer, 2020)**: Gated Linear Unit의 변형으로, 게이팅 메커니즘을 통해 정보 선택적 전달
- **실험적 우수성**: PaLM, LLaMA 등에서 ReLU/GeLU 대비 일관적으로 우수한 perplexity
- **비용**: 표준 FFN 대비 파라미터 수가 약 1.5배 (세 개의 행렬 사용), 하지만 성능 향상이 비용을 정당화

---

## 2. Training 전략의 진화

LLM 학습은 단순한 "더 많이, 더 크게"에서 시작하여 alignment, 효율성, 추론 능력이라는 복잡한 목표를 향해 발전했다.

---

### 2.1 Phase 1: Pretraining Only 시대 (2018-2022)

**핵심 objective: Next Token Prediction (Causal LM)**

```python
L_CLM = -Σ log P(x_t | x_1, ..., x_{t-1})
```

**주요 특징**:
- 학습 = 대규모 텍스트로 language model
- 사용 = 프롬프트 엔지니어링으로 task 수행
- 정렬(alignment) 없음: 모델이 "완성"을 목표로 하므로 instruction 이해 부족

**스케일링 법칙의 발견**:

*Kaplan et al. (2020) — OpenAI Scaling Laws*:
```
L(N) ∝ N^(-0.076)      (파라미터 N에 따른 loss)
L(C) ∝ C^(-0.050)      (compute C에 따른 loss)
L(D) ∝ D^(-0.095)      (데이터 D에 따른 loss)
→ 이 시기의 결론: 파라미터 수를 늘리는 게 최우선
```

*Hoffmann et al. (2022) — Chinchilla Scaling Laws*:
```
최적 모델 크기 N과 토큰 수 D의 관계:
N_optimal ∝ C^0.5   (compute의 제곱근)
D_optimal ∝ C^0.5

실용적 결론: N tokens ≈ 20 × N parameters
→ GPT-3는 over-parameterized, LLaMA-1은 under-trained 관점에서 Chinchilla-optimal
```

**시대 대표 모델**: GPT-3 (175B, 300B tokens), PaLM (540B, 780B tokens)

---

### 2.2 Phase 2: Instruction Tuning & RLHF 시대 (2022-2023)

**문제 인식**: 거대한 모델을 만들었지만, 사용자가 원하는 방식으로 동작하지 않는다. 모델이 instruction을 따르거나, 안전한 응답을 생성하거나, 진실된 정보만 제공하는 것이 아니었다.

**3단계 RLHF 파이프라인** (InstructGPT, 2022):

```
Stage 1: Supervised Fine-Tuning (SFT)
  데이터: 인간이 작성한 고품질 (prompt, response) 쌍
  목적: 모델이 instruction 형식에 익숙해지게 함

Stage 2: Reward Model (RM) Training
  데이터: 동일 prompt에 대한 여러 응답의 인간 ranking
  모델: pretrained LM + scalar head
  목적: "인간이 선호하는 응답"을 스칼라로 표현

Stage 3: RL Fine-Tuning (PPO)
  Policy: SFT 모델
  Reward: RM 점수 - β × KL(policy || SFT)
  목적: RM 점수 최대화 (인간 선호도 최대화)
```

**RLHF의 핵심 수식 (PPO objective)**:
```
J(θ) = E_{x~D, y~π_θ} [r_φ(x, y)] - β × KL[π_θ(y|x) || π_ref(y|x)]

r_φ: reward model
β: KL penalty 계수 (보통 0.01~0.1)
π_ref: SFT 기준 모델
```

**Alignment Tax**: RLHF는 종종 raw 성능을 약간 희생하면서 안전성/유용성 향상. 이 tradeoff를 "alignment tax"라 부른다.

**이 시대의 오픈소스 대안들**:
- **Alpaca** (2023): GPT-4로 생성한 52K instruction 데이터로 LLaMA-7B SFT. 단순하지만 효과적.
- **Vicuna** (2023): ShareGPT의 실제 ChatGPT 대화로 fine-tuning. RLHF 없이 ChatGPT 수준 일부 달성.
- **WizardLM** (2023): "Evol-Instruct" — LLM이 instruction의 복잡도를 자동으로 높임

---

### 2.3 Phase 3: DPO (Direct Preference Optimization) 시대 (2023-2024)

**RLHF의 문제점**:
- 별도의 Reward Model 학습 필요
- PPO 구현 복잡성 (4개의 모델: policy, ref policy, value, reward)
- 학습 불안정성 (PPO의 민감한 hyperparameter)
- 대규모 리소스 필요

**DPO의 핵심 통찰** (Rafailov et al., 2023):

```
RLHF의 최적해가 closed-form으로 존재한다.
→ Reward model을 explicit하게 학습할 필요 없이,
  preference 데이터에서 직접 policy를 최적화할 수 있다.

DPO Loss:
L_DPO(θ) = -E_{(x, y_w, y_l)} [
  log σ(β log(π_θ(y_w|x)/π_ref(y_w|x)) - β log(π_θ(y_l|x)/π_ref(y_l|x)))
]

y_w: 선호 응답 (winner)
y_l: 비선호 응답 (loser)
β: 온도 파라미터
```

**수학적 관계**: DPO는 내부적으로 reward model을 **policy 자체의 likelihood ratio**로 암묵적으로 정의한다. 즉, 별도의 RM 없이 LM 자체가 reward 신호 역할을 한다.

**DPO 변형들**:

| 방법 | 핵심 차이 | 장점 |
|------|----------|------|
| **DPO** | 원본, implicit RM | 단순, 안정적 |
| **IPO** (Identity PO) | overfitting 방지 정규화 추가 | DPO의 over-optimization 방지 |
| **KTO** (Kahneman-Tversky) | pairwise 비교 없이 단일 레이블 | 데이터 효율 향상 |
| **SimPO** | Length-normalized reward | 응답 길이 편향 감소 |
| **ORPO** | SFT + DPO를 단일 loss로 통합 | 학습 단순화 |

**이 시대의 의미**:
- DPO로 인해 RLHF가 더 접근하기 쉬워짐
- 소규모 팀도 alignment 가능
- Mistral, LLaMA 기반 다양한 aligned 모델들이 대거 등장

---

### 2.4 Phase 4: Reasoning / RLVR 시대 (2024-현재)

**문제 인식**: RLHF/DPO는 일반적인 helpful/harmless behavior를 가르치지만, **복잡한 수학적 추론, 복잡한 코딩 문제**에서는 여전히 한계가 있었다. 정답이 명확히 판별 가능한 domain에서는 더 강력한 신호가 가능하다.

**RLVR (Reinforcement Learning with Verifiable Rewards)**:

```
핵심 통찰: 수학/코딩 문제는 정답이 검증 가능하다.
→ 인간 feedback 없이도 binary reward 신호 가능
→ 더 깨끗하고, 더 확장 가능한 RL 가능

Verifiable reward:
- 수학: answer = correct? → r = 1 else r = 0
- 코딩: code passes all test cases? → r = 1 else r = 0
- 형식: <think></think> 태그 준수? → r ∈ {0, 1}
```

**GRPO (Group Relative Policy Optimization)** — DeepSeek-R1의 핵심:

```
표준 PPO의 문제: critic network가 별도로 필요
GRPO의 해결: 그룹 내 상대적 비교로 baseline 대체

알고리즘:
1. 동일 prompt에 대해 G개 응답 샘플링
2. 각 응답의 reward r_i 계산 (verifiable)
3. baseline = mean(r_1, ..., r_G)
4. advantage_i = (r_i - baseline) / std(r_1, ..., r_G)
5. policy gradient: E[advantage_i × log π_θ(response_i)]

장점:
- 별도 critic/value network 불필요 → 메모리 절약
- Group 내 상대적 성능으로 안정적인 gradient
```

**Process Reward Model (PRM)** — OpenAI o1에서 사용 (추정):

```
Outcome Reward Model (ORM): 최종 답만 평가
  r(x, y_final) ∈ {0, 1}

Process Reward Model (PRM): 각 추론 단계 평가
  r(x, step_1, step_2, ..., step_k) → per-step scores

PRM의 장점:
- 중간 단계의 오류를 즉시 포착
- 더 밀도 높은 reward signal
- 더 효율적인 learning
```

**Inference-Time Compute Scaling**:

```
Training-time scaling: 더 많은 GPU, 더 많은 데이터로 학습
Inference-time scaling: 답변 생성 시 더 많은 compute 사용

방법들:
1. Chain-of-Thought: 긴 reasoning chain 생성
2. Self-consistency: 여러 CoT 경로로 다수결
3. Best-of-N: N개 후보 중 reward model로 최선 선택
4. Tree-of-Thoughts: 트리 탐색 기반 추론
5. MCTS (Monte Carlo Tree Search): 더 체계적 탐색
```

**Scaling 비교**:
```
Training-time scaling curve: 수십억 달러 필요, 점점 평탄해짐
Inference-time scaling curve: 상대적으로 저렴, 아직 초기
→ o1, R1은 inference-time scaling이 새로운 frontier임을 입증
```

---

### 2.5 Training 전략 요약표

```
2018-2020  Pretraining only
           └─ GPT-1 → GPT-2 → GPT-3

2022       RLHF 등장
           └─ InstructGPT (SFT + RM + PPO)
           └─ ChatGPT (dialogue-optimized RLHF)

2023       Supervised alignment의 오픈소스화
           └─ Alpaca (synthetic SFT)
           └─ Vicuna (real conversation SFT)
           └─ WizardLM (evol-instruct)

2023       DPO 혁명
           └─ 더 단순한 alignment
           └─ 소규모 팀도 aligned 모델 가능
           └─ IPO, KTO 등 변형 등장

2024-2025  RLVR + Reasoning 시대
           └─ OpenAI o1 (inference-time scaling, PRM)
           └─ DeepSeek-R1 (GRPO, RLVR, open-source)
           └─ QwQ (open-source reasoning model)
           └─ Claude 3.7 Extended Thinking
```

---

## 3. 세대별 해결한 문제

### 3.1 GPT-3 시대: Scale → Capability

**시대 배경** (2020-2022):

이전 NLP는 특정 태스크(감성 분석, NER, 번역 등)에 특화된 모델들의 집합이었다. GPT-3는 단일 모델이 수백 가지 태스크를 few-shot으로 수행할 수 있음을 보여줬다.

**해결한 문제**:

| 기존 문제 | GPT-3의 해결 |
|-----------|-------------|
| 태스크별 별도 모델 필요 | 단일 모델로 멀티태스크 |
| Fine-tuning 데이터 수집 비용 | Few-shot in-context learning |
| 좁은 일반화 능력 | Emergent 능력 (번역, 코딩, 추론) |

**핵심 패러다임**: "Scaling is all you need" — 충분히 크고 충분한 데이터로 학습하면 능력이 창발(emergent)한다.

**한계**:
- Instruction을 따르지 못함 (프롬프트 "완성"에 집중)
- Hallucination 심각 (사실 여부 검증 없음)
- 안전성 문제 (유해한 내용 생성)
- API를 통해서만 접근 가능, 비용 높음

---

### 3.2 InstructGPT / ChatGPT 시대: Alignment → Usability

**시대 배경** (2022-2023):

"모델이 강력하지만 사용하기 어렵다"는 인식. 일반 사용자가 프롬프트 엔지니어링 없이 자연스럽게 대화할 수 있어야 했다.

**해결한 문제**:

| 기존 문제 | InstructGPT/ChatGPT의 해결 |
|-----------|---------------------------|
| 프롬프트 엔지니어링 전문 지식 필요 | 자연어 instruction 이해 |
| 유해/부정확한 응답 | RLHF로 safety + helpfulness |
| 단순 텍스트 완성 | 대화형, 맥락 유지 |
| AI 연구자만 사용 | 일반 대중이 사용 가능 |

**핵심 패러다임**: "크기보다 alignment이 중요" — 1.3B InstructGPT가 175B GPT-3보다 유용한 응답 생성.

**한계**:
- 여전히 OpenAI API 의존 (오픈소스 없음)
- 높은 API 비용
- Black-box 모델 (연구/커스터마이징 불가)

---

### 3.3 LLaMA 시대: Open-source → Democratization

**시대 배경** (2023):

강력한 모델은 모두 closed-source였다. 연구자, 스타트업, 중소기업이 frontier AI를 활용하려면 OpenAI API에 의존해야 했다. LLaMA의 등장은 이 장벽을 허물었다.

**해결한 문제**:

| 기존 문제 | LLaMA 시대의 해결 |
|-----------|-----------------|
| 오픈소스 고성능 모델 부재 | LLaMA-1/2로 weights 공개 |
| API 비용 부담 | 로컬 실행 가능 |
| Closed-source로 연구 제한 | fine-tuning, 분석, 연구 가능 |
| 비영어권 AI 종속 | 로컬 언어 fine-tuning 가능 |

**파급 효과**:
- Alpaca, Vicuna, WizardLM, Mistral, Qwen 등 수천 개 파생 모델
- Ollama, llama.cpp, vLLM 등 로컬 추론 생태계 형성
- Hugging Face의 open LLM leaderboard
- 학계와 산업계의 LLM 연구 폭발적 성장

**핵심 패러다임**: "Compute-optimal training" — 더 많은 데이터로 학습하면 작은 모델도 강해질 수 있다 (Chinchilla scaling).

---

### 3.4 DPO / Post-RLHF 시대: Simplified Alignment → Wider Adoption

**시대 배경** (2023-2024):

RLHF는 강력하지만 구현이 복잡하다. 4개의 모델(policy, ref, value, reward)을 동시에 관리하고, PPO의 민감한 hyperparameter를 튜닝해야 한다. 소규모 팀이나 개인이 수행하기 어렵다.

**해결한 문제**:

| 기존 문제 | DPO 시대의 해결 |
|-----------|----------------|
| RLHF 구현 복잡성 | 단순한 binary loss |
| 별도 RM 학습 필요 | Implicit RM (불필요) |
| 대규모 컴퓨팅 필요 | SFT와 유사한 리소스 |
| 학습 불안정성 | 안정적인 gradient |

**파급 효과**:
- 수천 개의 community-fine-tuned 모델 등장
- Mistral, Qwen, LLaMA 기반 specialized 모델들 폭발적 증가
- Hugging Face TRL 라이브러리로 DPO 민주화

---

### 3.5 Reasoning 시대: Inference-Time Compute → Harder Tasks

**시대 배경** (2024-2025):

Training-time scaling의 returns가 감소하는 조짐이 보이기 시작했다. 동시에 수학 올림피아드, 고급 코딩 문제 등 "진짜 어려운" 문제들에서는 여전히 LLM이 한계를 보였다. OpenAI o1은 새로운 축 — **inference-time compute** — 을 열었다.

**해결한 문제**:

| 기존 문제 | Reasoning 시대의 해결 |
|-----------|----------------------|
| 어려운 수학 문제 실패 | 긴 CoT + self-reflection |
| 단순 one-shot 추론 | 복잡한 다단계 계획 수립 |
| Scaling plateau 조짐 | 새로운 scaling axis (inference) |
| 추론 과정 불투명 | 사고 과정 공개 (Extended Thinking) |

**핵심 발견들**:

1. **RL이 reasoning을 만든다**: DeepSeek-R1-Zero는 SFT 없이 순수 RL만으로 self-reflection과 long CoT가 emergent하게 발생함을 보여줌

2. **Open-source도 reasoning 가능**: DeepSeek-R1이 open-weights로 o1-level 달성 → reasoning도 민주화

3. **Distillation으로 전이 가능**: R1의 reasoning 능력을 7B-32B 소형 모델에 distillation하면 놀라운 성능 달성

4. **Scaling law for inference**: 더 많은 inference compute → 더 어려운 문제 해결 (linear or better)

---

## 4. 효율성 진화: Parameter-per-Performance

"같은 파라미터 수로 얼마나 강해졌는가?"라는 관점에서 LLM 효율성이 어떻게 진화했는지 분석한다.

---

### 4.1 Training Compute 효율성

**FLOPs 기준 성능 비교** (MMLU 기준 대략적 추이):

```
GPT-3 175B (2020):        ~300B tokens,  MMLU ~43%
PaLM 540B (2022):         ~780B tokens,  MMLU ~70%
LLaMA-1 65B (2023):       ~1.4T tokens,  MMLU ~63%
LLaMA-2 70B (2023):       ~2T tokens,    MMLU ~68%
Mistral 7B (2023):        unknown,        MMLU ~60%
LLaMA-3 70B (2024):       ~15T tokens,   MMLU ~82%
Qwen2.5 72B (2024):       ~18T tokens,   MMLU ~85%
DeepSeek-V3 (2024):       ~14.8T tokens, MMLU ~88%
```

**효율성 향상의 원인**:

1. **더 많은 학습 데이터** (Chinchilla insight 적용): 같은 파라미터 수에 더 많은 토큰
2. **데이터 품질 향상**: 더 정교한 필터링, 중복 제거, 고품질 소스 비율 증가
3. **아키텍처 개선**: RMSNorm, SwiGLU, RoPE, GQA 등의 조합 최적화
4. **합성 데이터**: Phi 시리즈가 보여준 것처럼, 합성 고품질 데이터의 효과

---

### 4.2 Inference 효율성

**KV Cache 크기 비교** (GPT-3 기준):

```
표준 MHA (GPT-3 175B):
  96 layers × 96 heads × 2 (K+V) × seq_len × 128 (d_head) × 2 bytes
  ≈ 4.7TB (seq_len 2048 기준)

GQA (LLaMA-3 70B, 8 KV heads):
  80 layers × 8 heads × 2 × 128K × 128 × 2 bytes
  ≈ 대폭 감소

MQA (PaLM, Falcon):
  n_layers × 1 head × 2 × seq_len × d_head
  ≈ 최소 (정확도 희생 가능성)

MLA (DeepSeek-V2):
  MHA 대비 93.3% 감소
```

**추론 최적화 기법들**:

| 기법 | 효과 | 도입 |
|------|------|------|
| **FlashAttention (1/2/3)** | IO-aware attention, 메모리 4배 절약, 속도 2-4x | 2022-2024 |
| **Continuous Batching** | 서버 throughput 극대화 | vLLM 2023 |
| **PagedAttention** | KV cache 파편화 방지, 메모리 효율 | vLLM 2023 |
| **Speculative Decoding** | Small model로 draft → Large model verify | 2023 |
| **Quantization (GPTQ, AWQ)** | 4bit/8bit로 모델 크기 75% 감소 | 2023 |
| **GQA** | KV cache 메모리 n/g배 감소 | 2023 |
| **MLA** | KV cache 93.3% 감소 | 2024 |

---

### 4.3 데이터 효율성

**작은 모델이 큰 모델을 능가하는 방법들**:

**1. 합성 데이터 (Phi 시리즈의 교훈)**:
```
Phi-1 (1.3B): GPT-4 생성 교육용 코딩 데이터
  → HumanEval 50.6% (당시 13B 이하 최고)

Phi-4 (14B): 전 학습 과정에 합성 데이터 통합
  → GPT-4o mini보다 수학/과학 우수

교훈: 웹 데이터 100B tokens < 합성 교육 데이터 10B tokens
```

**2. Knowledge Distillation**:
```
DeepSeek-R1 → Distill-Qwen-32B:
  - 32B가 OpenAI o1-mini보다 수학 성능 우수

Gemma 2:
  - Gemini로부터 distillation → 같은 파라미터 대비 높은 성능
```

**3. Longer Training on Quality Data**:
```
LLaMA-3 (8B): 15T tokens 학습
  → LLaMA-2 70B보다 일부 태스크에서 우수

Qwen2.5 (7B): 18T tokens 학습
  → 다른 7B 모델들 압도
```

---

### 4.4 Training 비용 효율성

**FP8 Mixed-Precision Training (DeepSeek-V3 혁신)**:

```
기존 BF16/FP16 training:
  - Weights, gradients: BF16 (16-bit)
  - Activations: BF16 (16-bit)
  - Master weights: FP32 (32-bit)

DeepSeek-V3 FP8 전략:
  - 주요 행렬 연산: FP8 (8-bit) → 2배 throughput
  - 민감한 연산 (norm, attention): BF16 유지
  - Gradient accumulation: BF16/FP32 유지

결과: 학습 속도 약 30-40% 향상
비용: 671B 모델을 약 $5.6M에 학습
```

**비교: 유사 성능 모델 학습 비용 추정**
```
GPT-4 (추정): ~$100M+
Claude 3 Opus (추정): ~$50M+
DeepSeek-V3: ~$5.6M (공식 발표)
→ 약 10-20배 비용 효율 (아키텍처 + FP8 + 효율적 parallelism 덕분)
```

---

## 5. 연구자를 위한 핵심 교훈

### 5.1 반복적으로 작동한 패턴들

**Pattern 1: Scale + Quality Data는 항상 작동한다**

어떤 아키텍처 혁신보다도, **더 많은 고품질 데이터**로 더 오래 학습하는 것이 가장 확실한 성능 향상 방법이다.

```
증거:
- LLaMA-3 (8B): 15T tokens → GPT-3.5급 성능
- Qwen2.5: 18T tokens → 동급 최강
- Chinchilla law: 파라미터 수와 토큰 수를 함께 늘려야
```

**Pattern 2: 아키텍처 혁신은 점진적이지만 누적 효과가 크다**

단일 혁신의 효과는 작아 보여도, 조합되면 큰 차이를 만든다.

```
현대 표준 LLM 아키텍처:
  Pre-norm (RMSNorm) → 학습 안정성 향상
  + SwiGLU → 표현력 향상
  + RoPE → 컨텍스트 확장 용이
  + GQA → 추론 효율 향상
  + Flash Attention → 메모리/속도 최적화
  = 이 모든 것을 합친 "현대 표준"
```

**Pattern 3: RL은 검증 가능한 도메인에서 강력하다**

RLHF는 일반적 helpfulness에 효과적이고, RLVR는 수학/코딩처럼 정답이 명확한 도메인에서 극적인 성능 향상을 만든다.

```
검증 가능한 reward의 장점:
- 인간 feedback 불필요 → 더 확장 가능
- 신호가 명확 → 더 강한 gradient
- 자동화 가능 → 대규모 RL 가능

DeepSeek-R1의 RLVR로 얻은 것:
- AIME 2024: 79.8% (o1과 동등)
- 고등학교 수학에서 인간 전문가 수준
```

**Pattern 4: Distillation은 효율적 지식 전이를 가능케 한다**

큰 모델의 능력을 작은 모델에 이식하는 distillation은 계속해서 효과적임이 증명되었다.

```
방법들:
1. Output distribution distillation (Gemma 2)
2. CoT trace distillation (R1-Distill)
3. Intermediate representation distillation

결과: 7B R1-distill > GPT-4o (수학)
```

---

### 5.2 실패한 접근법 / 회의적 관점

**실패 1: 매우 큰 단일 Dense 모델의 지속적 스케일링**

```
문제점:
- 파라미터 수 증가에 따른 추론 비용 선형 증가
- Diminishing returns (Chinchilla 이후)
- 학습 비용 기하급수적 증가

현재 추세: Dense → Sparse MoE로 전환
```

**실패 2: 단순한 텍스트 데이터의 무한 확장**

```
웹 데이터는 질보다 양으로 성능 향상에 한계가 있다.
Phi 시리즈가 보여준 것:
- 소량 합성 데이터 > 대량 저품질 웹 데이터
- 데이터 다양성과 교육적 가치가 더 중요
```

**실패 3: ALiBi의 광범위한 채택**

```
ALiBi는 extrapolation 능력이 있지만,
RoPE 기반 모델들이 일관적으로 더 우수한 성능을 보여
현재는 RoPE가 압도적 표준이 됨.
```

**실패 4: RNN/SSM의 Transformer 대체**

```
Mamba, RWKV, RetNet 등 linear recurrent 모델들이
긴 컨텍스트에서 Transformer를 효율적으로 대체할 수 있다는 주장.

현실:
- 단기 순수 SSM은 아직 frontier transformer에 미치지 못함
- Hybrid (SSM + Attention 교대) 아키텍처가 더 현실적
- Jamba (AI21), Zamba 등 hybrid 모델이 등장하는 추세
- 순수 SSM의 "attention is all you need를 대체" 주장은 시기상조
```

---

### 5.3 열린 연구 문제들

**Open Question 1: Inference-Time Scaling의 한계**

```
"Think longer → solve harder problems"이 어디까지 적용되는가?

현재 알려진 것:
- 수학, 코딩: 매우 효과적
- 일반 지식 QA: 제한적

알려지지 않은 것:
- 최적 thinking 길이는?
- Diminishing returns 지점은?
- 어떤 task 유형에 효과적인가?
```

**Open Question 2: True Reasoning vs. Sophisticated Pattern Matching**

```
LLM이 정말 "추론"하는가, 아니면 훈련 데이터의 패턴을
정교하게 매칭하는 것인가?

증거 1 (추론 주장):
- 훈련에 없던 새로운 수학 문제 해결
- 다단계 논리적 추론 성공

증거 2 (패턴 매칭 주장):
- 분포 이동에 취약 (형식만 바꿔도 실패)
- ARC-AGI 같은 진정한 추상화 테스트에서 한계
- CoT가 오히려 오답으로 이어지는 경우 존재
```

**Open Question 3: Continual Learning**

```
현재 LLM: Train once → Deploy (static)
문제: 지식 컷오프, 새로운 정보 반영 불가

해결 시도:
- RAG (검색 보강): inference-time 지식 주입
- Fine-tuning: 비용 높고, catastrophic forgetting 위험
- In-context learning: 컨텍스트 길이 제한

진정한 continual learning: 아직 해결되지 않은 문제
```

**Open Question 4: Multimodal Grounding**

```
텍스트와 이미지를 함께 처리하지만:
- 진정한 이미지 "이해"인가, 텍스트-이미지 패턴 매칭인가?
- 공간적 관계, 물리적 상식에서 여전히 실패 사례 많음
- Video, 3D, 음악 등 복잡한 모달리티 통합은 초기 단계

GPT-4o, Gemini 2가 개선했지만
인간 수준의 multimodal 이해까지는 갭 존재
```

**Open Question 5: Efficient Long-Context**

```
1M 컨텍스트 처리는 가능해졌지만:
- "Lost in the middle": 컨텍스트 중간 정보가 무시되는 현상
- 실제 활용: 1M 컨텍스트 중 관련 정보 찾기가 어려움
- 비용: 1M 컨텍스트 1회 처리 = 엄청난 compute

해결 방향:
- Memory-augmented architectures
- Efficient retrieval within context
- Hierarchical processing
```

**Open Question 6: Small Models vs. Large Models**

```
Phi-4 (14B)가 GPT-4o mini를 수학에서 능가하는 시대에:
- 어디까지 작은 모델로 강한 성능이 가능한가?
- "Tiny models + synthetic data"의 한계는?
- 진정한 frontier capability는 여전히 대형 모델만 가능한가?
- Edge AI의 가능성: 스마트폰에서 GPT-4급 가능한가?
```

---

### 5.4 연구 트렌드 예측 (2025-2026)

**1. Reasoning의 주류화**
- o1, R1 스타일의 reasoning이 모든 frontier 모델의 기본 기능이 될 것
- "Thinking" vs "Non-thinking" 모드 전환이 표준화
- RLVR의 적용 도메인이 수학/코딩을 넘어 확장

**2. Agent 능력의 실용화**
- 단순 text generation → 실제 도구 사용, 멀티스텝 작업 수행
- Computer use (GUI 조작), 웹 탐색, 코드 실행이 기본 기능화
- Multi-agent 협업 시스템의 실용적 배포

**3. Efficiency의 지속적 중요성**
- MoE의 표준화 (Dense → Sparse MoE 전환 가속)
- 소형 모델 + 합성 데이터 방법론의 고도화
- Speculative decoding, quantization의 더 넓은 채택

**4. Multimodal의 성숙**
- Native multimodal (GPT-4o 방식)이 업계 표준이 될 것
- 오디오, 비디오 모달리티의 통합 심화
- 실시간 상호작용 모델의 상용화

**5. Open vs. Closed 경쟁**
- DeepSeek-R1의 충격 이후 오픈소스 모델의 성능 격차 축소
- Frontier closed 모델들이 오픈소스 대비 차별화를 유지하기 어려워질 것
- Meta, DeepSeek, Qwen 등의 공격적 오픈 전략 지속

---

## 부록: 핵심 개념 빠른 참조

### Attention 메커니즘 비교

| 방식 | KV heads | Cache 크기 | 표현력 | 적용 모델 |
|------|----------|-----------|-------|---------|
| MHA | n_heads | n_heads × d | 최고 | GPT-3, LLaMA-1 |
| MQA | 1 | 1 × d | 낮음 | PaLM, Falcon |
| GQA | n_groups | n_groups × d | 중간 | LLaMA-2+, Mistral, Qwen |
| MLA | d_c (압축) | d_c (매우 작음) | 높음 | DeepSeek V2/V3 |

### Training 방법 비교

| 방법 | 데이터 요구 | 복잡도 | 적용 | 효과 |
|------|-----------|--------|------|------|
| SFT | (prompt, response) 쌍 | 낮음 | 범용 | Instruction following |
| RLHF (PPO) | Pairwise preference | 높음 | Safety, Helpfulness | Strong alignment |
| DPO | Pairwise preference | 낮음 | Safety, Helpfulness | 비슷하지만 단순 |
| RLVR | 검증 가능한 정답 | 중간 | Math, Code | Reasoning 극대화 |

### 스케일링 법칙 요약

```
Kaplan (2020): 파라미터 ↑, 데이터 ↑, compute ↑ → 모두 성능 ↑
Chinchilla (2022): 파라미터:데이터 비율이 중요, 동일 compute → 작은 모델 + 더 많은 데이터
Inference scaling (2024): 추론 시간 compute ↑ → 어려운 문제 해결 ↑
```
