# 7.1 LLM 모델 패밀리 종합 Survey

> **읽기 전 참고**: 이 문서는 2026년 2월 기준 주요 LLM 패밀리들을 종합적으로 정리한다. 모든 모델명, 기술 용어, 아키텍처 개념은 English로 표기한다.

---

## 목차

1. [GPT Family (OpenAI)](#1-gpt-family-openai)
2. [LLaMA Family (Meta)](#2-llama-family-meta)
3. [Mistral / Mixtral Family](#3-mistral--mixtral-family)
4. [Google Family (PaLM / Gemma / Gemini)](#4-google-family-palm--gemma--gemini)
5. [Alibaba Qwen Family](#5-alibaba-qwen-family)
6. [DeepSeek Family](#6-deepseek-family)
7. [Other Notable Models](#7-other-notable-models)

---

## 1. GPT Family (OpenAI)

OpenAI의 GPT 패밀리는 현대 LLM 패러다임의 시작점이다. 각 세대는 이전 세대가 해결하지 못했던 문제를 새로운 방식으로 돌파했다.

---

### 1.1 GPT-1 (2018)

**등장 배경 및 의의**

2018년 이전까지 NLP 모델들은 주로 task-specific하게 설계되었다. 감성 분류, 기계 번역, QA 등 각 태스크마다 별도의 아키텍처와 학습 데이터가 필요했다. GPT-1은 이 패러다임을 깨고 **"하나의 pretrained model을 여러 downstream task에 fine-tuning하면 된다"**는 개념을 대규모로 입증했다.

**핵심 아키텍처 혁신**

- **Decoder-only Transformer**: 기존 Transformer (Vaswani et al., 2017)는 encoder-decoder 구조였으나, GPT-1은 decoder만 사용하는 causal language model을 채택했다. 이로써 left-to-right autoregressive 생성이 자연스럽게 가능해졌다.
- **12개 Transformer layers**, hidden size 768, 12 attention heads
- 파라미터 수: 약 **117M**
- BooksCorpus (약 7천 권의 미출판 책) 로 사전학습

**핵심 Training 혁신**

- **Pretraining + Fine-tuning 패러다임**: 대규모 unlabeled text로 language model pretraining 후, labeled downstream data로 supervised fine-tuning. Fine-tuning 시 language modeling objective를 auxiliary loss로 함께 사용하여 일반화 성능을 높였다.
- **Task-specific input transformation**: 분류/유사도/QA 등 다양한 task의 입력을 Transformer가 처리할 수 있는 단일 token sequence로 변환하는 체계적인 방법을 제시했다.

**이전 모델 대비 해결한 문제**

이전의 word2vec, ELMo 등은 contextual representation을 제공했으나, 각 task마다 독립적인 fine-tuning 구조가 필요했다. GPT-1은 단일 pretrained backbone을 다양한 NLP task에 통일된 방식으로 적용하는 것이 실제로 작동함을 증명했다.

---

### 1.2 GPT-2 (2019)

**등장 배경 및 의의**

GPT-1의 성공 이후, OpenAI는 "더 큰 모델, 더 많은 데이터를 쓰면 어떻게 될까?"라는 질문을 탐구했다. GPT-2는 **스케일링이 zero-shot 성능과 emergent capability를 만들어낸다**는 사실을 처음으로 대중에게 각인시켰다. 당시 OpenAI는 이 모델이 "too dangerous to release"라며 단계적으로 공개하여 화제가 되었다.

**핵심 아키텍처 혁신**

- **1.5B 파라미터** (GPT-1의 약 13배)
- Layer normalization을 각 sub-block의 입력에 위치 (pre-norm), residual path를 개선
- Context window **1,024 tokens**
- WebText 데이터셋: Reddit에서 karma 3 이상을 받은 링크의 웹 페이지 텍스트, 약 40GB

**핵심 Training 혁신**

- **Zero-shot task transfer**: fine-tuning 없이 task 설명만으로 번역, 요약, QA 수행 가능함을 입증
- Language model의 conditional distribution `p(output | input, task description)`을 통해 multitask learning이 자연스럽게 발생
- **Scaling reveals capabilities**: 모델 크기(117M → 345M → 762M → 1.5B)가 커질수록 능력이 비선형적으로 증가

**이전 모델 대비 해결한 문제**

GPT-1은 fine-tuning이 필수였으나, GPT-2는 fine-tuning 없이도 상당한 성능을 보여줌으로써 **"pretrained LM 자체가 task solver가 될 수 있다"**는 가능성을 열었다. 또한 스케일링이 체계적으로 성능을 높인다는 경험적 증거를 제공했다.

---

### 1.3 GPT-3 (2020)

**등장 배경 및 의의**

GPT-3는 NLP 역사의 분기점이다. **175B 파라미터**라는 전례 없는 스케일로, few-shot in-context learning이라는 새로운 패러다임을 확립했다. "Brown et al., 2020: Language Models are Few-Shot Learners" 논문은 AI 역사에서 가장 영향력 있는 논문 중 하나가 되었다.

**핵심 아키텍처 혁신**

- **175B 파라미터**: 96 layers, hidden size 12,288, 96 attention heads
- **Context window 2,048 tokens**
- Alternating dense and locally banded sparse attention (Sparse Transformer 아이디어 일부 적용)
- 학습 데이터: Common Crawl (필터링된 버전), WebText2, Books1, Books2, Wikipedia 등 총 약 570GB text (300B tokens)

**핵심 Training 혁신**

- **In-context learning (ICL)**: 모델 파라미터를 업데이트하지 않고, 프롬프트에 몇 가지 예시(few-shot examples)를 제공함으로써 새로운 task를 수행. 이는 gradient update 없는 "meta-learning"이라 볼 수 있다.
  - **Zero-shot**: task description만 제공
  - **One-shot**: 1개의 예시 제공
  - **Few-shot**: 여러 예시 제공
- **Scale as the primary lever**: Chinchilla law 이전의 시대로, compute를 주로 모델 크기 증가에 집중했다.

**이전 모델 대비 해결한 문제**

GPT-2가 zero-shot에서 가능성을 보여줬다면, GPT-3는 few-shot에서 fine-tuned smaller model을 능가하는 성능을 달성했다. **"Fine-tuning 없이도 실용적인 AI assistant를 만들 수 있다"**는 개념의 실증이었다. OpenAI API를 통해 상업적 서비스가 시작되었고, 이후 LLM 산업 전체의 틀이 잡혔다.

---

### 1.4 InstructGPT (2022)

**등장 배경 및 의의**

GPT-3는 강력했으나 **alignment 문제**가 있었다: 모델이 유저의 의도와 다른 방향으로 응답하거나, 유해한 내용을 생성하거나, 단순히 프롬프트를 "완성"하려 했다. InstructGPT는 **RLHF (Reinforcement Learning from Human Feedback)**를 통해 이 문제를 해결하고, "instruction following"을 가르쳤다.

**핵심 아키텍처 혁신**

- 아키텍처 자체는 GPT-3와 동일하지만, **1.3B, 6B, 175B** 세 가지 버전으로 실험
- **Reward Model (RM)**: 인간의 선호도를 학습하는 별도 모델. GPT-3에서 fine-tuning된 모델로 초기화하고, 같은 프롬프트에 대한 여러 응답을 ranking하도록 학습
- **Policy Model**: RL로 최적화되는 메인 모델

**핵심 Training 혁신**

**RLHF 3단계 파이프라인**:

1. **Supervised Fine-Tuning (SFT)**: 인간 라벨러들이 작성한 demonstration data로 GPT-3를 fine-tuning. 약 13,000개의 prompt-response 쌍 사용.

2. **Reward Model Training**: 같은 프롬프트에 대한 여러 모델 출력을 인간이 ranking하면, 이 ranking 데이터로 reward model을 학습. Bradley-Terry 모델 기반의 pairwise comparison.

3. **PPO (Proximal Policy Optimization)**: Reward model을 사용하여 policy (언어 모델)를 RL로 최적화. KL divergence penalty로 원래 SFT 모델에서 너무 벗어나지 않도록 제어.

**이전 모델 대비 해결한 문제**

- "Helpful, Harmless, Honest (HHH)"를 실제로 구현
- 1.3B InstructGPT가 175B GPT-3보다 사람들이 선호하는 응답을 생성함을 보여줌 → **크기보다 alignment이 중요**
- LLM을 실용적 assistant로 만드는 방법론을 확립

---

### 1.5 ChatGPT (2022년 11월)

**등장 배경 및 의의**

ChatGPT는 새로운 기술이라기보다는 **InstructGPT의 대화(dialogue) 최적화 버전**이다. 그러나 2022년 11월 출시 후 사상 최단 기간(5일)에 100만 명 사용자를 돌파하며, **AI의 대중화**를 이끈 역사적 제품이 되었다.

**핵심 특징**

- GPT-3.5 (code-davinci-002 계열) 기반
- **Multi-turn conversation**: RLHF training 시 대화 형식의 데이터를 대폭 늘려 자연스러운 multi-turn dialogue 가능
- **Refusal training**: 유해 요청 거절, 적절한 경계 설정
- **RLHF for dialogue**: 이전 InstructGPT의 RLHF를 대화 맥락에 최적화

**이전 모델 대비 해결한 문제**

API를 통해서만 접근 가능했던 GPT-3와 달리, 일반인이 웹 인터페이스로 쉽게 사용할 수 있게 되었다. 코딩 도움, 글쓰기, 분석 등 실용적 use case를 실증함으로써 "LLM은 연구용이 아니라 실용 도구"임을 입증했다.

---

### 1.6 GPT-4 (2023년 3월)

**등장 배경 및 의의**

GPT-4는 OpenAI가 "GPT-4 Technical Report"를 공개했으나 **파라미터 수, 아키텍처 세부 사항을 비공개**로 유지한 최초의 주요 모델이다. 다양한 professional exam(변호사, 의사 시험 등)에서 인간 수준을 넘어섰으며, **multimodal** 입력(이미지 + 텍스트)을 처음으로 지원했다.

**핵심 아키텍처 혁신**

- **Multimodal input**: 텍스트와 이미지를 함께 입력으로 처리 (비전 인코더 통합)
- **Mixture of Experts (MoE)** (비공식 루머 / 분석): 업계에서 GPT-4가 sparse MoE 아키텍처를 사용한다는 분석이 많다. 약 8개의 expert, 각 토큰마다 2개 activation으로 추정. 공식 확인 없음.
- **Context window**: 8K → 32K (GPT-4-32k)
- 멀티모달 학습을 위한 vision encoder (CLIP 계열로 추정)

**핵심 Training 혁신**

- **Predictable scaling**: GPT-4 Technical Report는 작은 모델 실험에서 큰 모델의 성능을 예측하는 scaling curve를 만들었다고 언급
- **RLHF + Constitutional AI 요소들** 통합
- **Red-teaming**: 출시 전 6개월간 extensive adversarial testing

**이전 모델 대비 해결한 문제**

- **Reasoning 능력의 질적 도약**: GPT-3.5 대비 복잡한 추론, 수학, 코딩에서 현격한 차이
- **Multimodal**: 텍스트만 처리하던 GPT-3 계열과 달리 이미지 이해 가능
- **Professional-level performance**: 의대 시험(USMLE), 변호사 시험(BAR), GRE 등에서 인간 상위 10% 수준

---

### 1.7 GPT-4o (2024년 5월)

**등장 배경 및 의의**

GPT-4o ("o"는 "omni")는 **native multimodal** 모델로, 텍스트/오디오/이미지/비디오를 단일 모델 내에서 end-to-end로 처리한다. 이전의 음성 기능이 ASR → LLM → TTS 파이프라인이었다면, GPT-4o는 오디오 토큰을 직접 처리하여 감정, 톤, 속도 등을 이해한다.

**핵심 아키텍처 혁신**

- **Native multimodal tokenization**: 오디오, 이미지, 텍스트를 통합된 token space에서 처리. 별도의 모달리티별 파이프라인 없음.
- **Real-time audio processing**: 평균 232ms latency로 자연스러운 대화 가능 (인간 평균 응답 시간 수준)
- **Unified architecture**: 이전 GPT-4V가 vision encoder + LLM 분리 구조였다면, GPT-4o는 단일 모델로 통합

**핵심 Training 혁신**

- **Joint modality training**: 모든 modality를 동시에 학습하여 cross-modal reasoning 향상
- 비용 및 속도: GPT-4 Turbo 대비 **2배 빠르고, 50% 저렴**

**이전 모델 대비 해결한 문제**

- 파이프라인 기반 음성 처리의 latency 문제 해결
- 오디오의 감정적 뉘앙스, 배경 소음 등 비텍스트적 정보 처리 가능
- 단일 모델로 텍스트/음성/시각 통합 → 실용적 AI assistant의 완성도 향상

---

### 1.8 o1 / o3 (2024-2025): Reasoning Models

**등장 배경 및 의의**

OpenAI o1 (2024년 9월)은 LLM 역사에서 **패러다임 전환점**이다. 기존 모델들이 "더 많은 training compute"로 발전했다면, o1은 **inference-time compute scaling**이라는 새로운 축을 도입했다. 모델이 답변하기 전에 내부적으로 긴 "thinking" 과정을 거친다.

**핵심 아키텍처 혁신**

- **Chain-of-Thought reasoning at inference**: 모델이 응답 전에 내부 reasoning trace를 생성 (사용자에게 일부 또는 전부 숨겨지거나 공개됨)
- **Long thinking tokens**: 수백~수천 개의 reasoning token을 소비한 후 최종 응답 생성
- 아키텍처 자체는 비공개이나, 사고 과정이 structured되고 반성(reflection)을 포함

**핵심 Training 혁신**

- **Reinforcement Learning on Verifiable Rewards (RLVR)**: 수학 문제, 코딩 문제처럼 정답이 명확히 검증 가능한 문제들에 대해 RL 학습
- **Process Reward Model (PRM)**: 최종 답변뿐 아니라 **중간 reasoning 과정의 각 단계**에 reward 제공
- **Search over reasoning paths**: 여러 reasoning path를 탐색하여 최적 경로 선택
- "Thinking budget": 추론에 사용할 compute 양을 조절하여 accuracy-cost tradeoff 가능

**모델 변형**

| 모델 | 특징 |
|------|------|
| o1-preview (2024.09) | 첫 번째 공개 reasoning model |
| o1 (2024.12) | 정식 버전, 이미지 입력 지원 |
| o1-mini | 경량화 버전, 추론 집중, 일반 지식 약함 |
| o3 (2025.04 출시 예정) | o1의 후속, ARC-AGI에서 최고 성능 |
| o3-mini (2025.01) | 효율적 reasoning, 세 가지 thinking intensity 레벨 |

**이전 모델 대비 해결한 문제**

- **Hard reasoning tasks**: AIME (미국 수학 올림피아드), Codeforces (상위 프로그래머 수준) 등 기존 GPT-4도 어려워했던 문제들을 해결
- **Scaling axis 추가**: training compute뿐 아니라 inference compute도 성능을 높이는 새로운 차원 발견
- **ARC-AGI**: OpenAI o3는 ARC-AGI 벤치마크에서 처음으로 인간 수준(87.5%) 달성

---

## 2. LLaMA Family (Meta)

LLaMA 패밀리는 **"open-weights LLM의 민주화"**를 이끈 가장 중요한 시리즈다. Meta의 공개로 인해 학술 연구자, 스타트업, 개인 개발자들이 frontier-class 모델을 활용할 수 있게 되었다.

---

### 2.1 LLaMA-1 (2023년 2월)

**등장 배경 및 의의**

GPT-3, PaLM 등 대형 LLM들은 모두 closed-source였다. Meta AI Research는 **"compute-optimal training"** (Chinchilla 논문의 insight)을 적용하여, 파라미터 수를 줄이고 더 많은 데이터로 학습하면 더 효율적임을 실증했다. 특히 **7B LLaMA-1이 GPT-3 (175B)를 일부 벤치마크에서 능가**한 것은 충격이었다.

**핵심 아키텍처 혁신**

- **Decoder-only Transformer** (GPT 계열과 유사)
- **Pre-normalization (RMSNorm)**: Layer Norm 대신 Root Mean Square Layer Normalization 사용으로 학습 안정성 향상
- **SwiGLU activation function**: ReLU 대신 SwiGLU (Swish-Gated Linear Unit) 사용으로 표현력 향상
- **Rotary Positional Embeddings (RoPE)**: Absolute PE 대신 RoPE 사용으로 더 나은 extrapolation 및 효율성
- 모델 크기: **7B, 13B, 33B, 65B**
- Context length: **2,048 tokens**
- 학습 데이터: 1T tokens (CommonCrawl, C4, GitHub, Wikipedia, Books, ArXiv, Stack Exchange)

**핵심 Training 혁신**

- **Compute-optimal training (Chinchilla scaling)**: 모델 크기에 비해 훨씬 많은 토큰으로 학습. 7B 모델을 1T tokens로 학습 (Chinchilla optimal의 약 2배 이상의 데이터).
- 학습 효율: FlashAttention, checkpointing 적용으로 GPU 효율 극대화

**이전 모델 대비 해결한 문제**

- **Closed-source 장벽 타파**: weights 공개로 연구자들이 직접 fine-tuning, 분석 가능
- **효율성 입증**: 175B GPT-3보다 작은 모델이 더 나은 성능 → 스케일이 전부가 아님
- Alpaca, Vicuna, WizardLM 등 수많은 파생 모델의 기반이 됨

---

### 2.2 LLaMA-2 (2023년 7월)

**등장 배경 및 의의**

LLaMA-1은 연구 목적으로만 사용 가능했으나, LLaMA-2는 **상업적 라이선스**를 포함하여 공개되었다. 또한 Chat 버전(RLHF 적용)도 함께 공개하여 GPT-3.5/ChatGPT의 실질적 대안이 되었다.

**핵심 아키텍처 혁신**

- **Context window 2배 확장**: 2,048 → **4,096 tokens**
- **Grouped Query Attention (GQA)**: 70B 모델에만 적용. Multi-head attention의 KV cache를 줄이면서 성능 유지. K, V head를 그룹화하여 공유.
- 모델 크기: **7B, 13B, 70B** (33B, 65B 제거)
- 학습 데이터: 2T tokens (LLaMA-1 대비 40% 증가), 더 최신 데이터 포함

**핵심 Training 혁신**

- **RLHF for Chat (LLaMA-2-Chat)**:
  - **SFT**: 27,540개의 high-quality annotation
  - **Reward Model**: 1M 이상의 binary comparison data로 학습
  - **RLHF**: PPO 기반으로 helpfulness와 safety reward를 함께 최적화
  - **Ghost Attention (GAtt)**: 시스템 프롬프트가 긴 대화에서도 지속적으로 영향을 미치도록 하는 training trick
- **Safety-focused**: Meta의 safety team과 협력하여 extensive red-teaming 수행

**이전 모델 대비 해결한 문제**

- LLaMA-1의 연구용 제한 → 상업적 사용 허용
- Instruction-following 및 safety가 강화된 Chat 버전 제공
- GQA로 대형 모델 추론 효율 개선 (KV cache 메모리 절약)

---

### 2.3 LLaMA-3 / 3.1 / 3.2 / 3.3 (2024)

**등장 배경 및 의의**

LLaMA-3 시리즈는 Meta가 단순한 "연구 공개"를 넘어 **frontier model과 경쟁**하겠다는 의지를 보여준다. LLaMA-3.1 405B는 GPT-4 수준의 성능을 open-weights로 달성한 최초의 모델 중 하나다.

**핵심 아키텍처 혁신 (LLaMA-3 기반)**

- **GQA 전 모델에 적용**: 7B/13B에도 GQA 도입 (LLaMA-2는 70B만 GQA)
- **Context window 8K → 128K (LLaMA-3.1)**
  - **RoPE theta 값 변경**: 500,000으로 설정하여 장거리 의존성 처리 개선
  - **Continued pretraining on long context data**: 128K 컨텍스트를 활용하는 데이터로 추가 학습
- **Tokenizer 개선**: 128,256개 토큰 어휘 (LLaMA-2의 32K 대비 4배), 더 나은 다국어 지원
- **모델 크기**: 8B, 70B (LLaMA-3), 8B/70B/405B (LLaMA-3.1)

**LLaMA-3.2의 Vision 지원**

- **11B, 90B Vision variants**: Cross-attention 기반 vision encoder 통합
- **1B, 3B 경량 모델**: Edge/mobile deployment를 위한 작은 모델. LLaMA-3.1 8B에서 pruning + knowledge distillation으로 생성.

**LLaMA-3.3**

- **70B만 출시** (405B의 성능을 70B 크기로 압축)
- Instruction following 및 tool use 능력 강화

**핵심 Training 혁신**

- **15T tokens pretraining** (LLaMA-2의 7.5배 이상)
- **Multilingual data 강화**: 다국어 비율을 대폭 늘려 한국어, 일본어 등 비영어 성능 향상
- **Tool use / function calling**: LLaMA-3.1부터 공식적으로 tool use 지원
- **Synthetic data generation**: 코딩, 수학, 추론 등을 위한 합성 데이터 대량 생성
- **DPO (Direct Preference Optimization)** 및 RLHF 혼합 적용으로 alignment 개선

**이전 모델 대비 해결한 문제**

- **Short context limitation 극복**: 4K → 128K context로 long document processing 가능
- **Multimodal**: LLaMA-3.2로 vision 능력 추가
- **405B open-weights**: 상업용 frontier model에 준하는 성능을 open-source로 제공
- **Tool use & agent**: 코드 실행, 검색 등 tool을 호출하는 agent 용도로 사용 가능

---

## 3. Mistral / Mixtral Family

Mistral AI는 2023년 설립된 프랑스 스타트업으로, **효율적인 아키텍처 혁신**으로 빠르게 주목받았다. 특히 Sliding Window Attention과 Mixture of Experts를 조합하여 소형 모델의 한계를 돌파했다.

---

### 3.1 Mistral-7B (2023년 9월)

**등장 배경 및 의의**

출시 당시 가장 강력한 7B 오픈소스 모델이었다. LLaMA-2 13B보다 뛰어난 성능을 7B 파라미터로 달성하며, **아키텍처 최적화가 단순 스케일링보다 중요할 수 있다**는 것을 입증했다.

**핵심 아키텍처 혁신**

- **Sliding Window Attention (SWA)**: 각 attention head가 **window size W 이내의 토큰만** attend. 표준 attention의 O(n²) 복잡도를 O(n×W)로 감소. 그러나 여러 레이어를 통해 실질적으로 긴 context 정보 전파 가능 (receptive field가 레이어마다 확장).
  - Window size: 4,096 토큰
  - 실질적 context span: 레이어 수 × window size (32 × 4096 = 131K 이상)
- **Grouped Query Attention (GQA)**: 8 KV heads, 32 Q heads → 추론 속도 향상
- **Rolling Buffer KV Cache**: SWA와 조합하여 KV cache를 고정 크기로 유지
- Hidden size: 4,096, 32 layers, FFN dimension: 14,336

**핵심 Training 혁신**

- 학습 데이터 및 training details 비공개 (Mistral AI의 전략)
- 추론 최적화: **FlashAttention-2** 통합, **xformers** 라이브러리 사용

**이전 모델 대비 해결한 문제**

- 7B scale에서 13B 수준 성능: 효율적 추론 가능
- SWA로 이론적으로 긴 context handling 가능 (당시 기준)
- 빠른 추론 속도와 낮은 메모리 요구사항

---

### 3.2 Mixtral 8x7B (2023년 12월)

**등장 배경 및 의의**

Mixtral 8x7B는 **오픈소스 MoE (Mixture of Experts) 모델의 표준**을 세웠다. 총 46.7B 파라미터이지만 실제 추론 시 **12.9B만 활성화**되어, 13B 모델 수준의 추론 비용으로 70B 수준의 성능을 달성했다.

**핵심 아키텍처 혁신**

- **Sparse MoE Layer**:
  - 각 FFN 레이어를 8개의 "expert" FFN으로 교체
  - **Top-2 routing**: 각 토큰마다 router network가 8개 expert 중 2개를 선택
  - 활성화 파라미터: 8 experts × 7B × (2/8) = 약 12.9B
  - 전체 파라미터: 46.7B (모든 expert 합산)
- **Router**: 단순 linear layer로 토큰을 expert에 routing. Softmax scores 상위 2개 expert의 출력을 weighted sum.
- Attention은 일반 dense attention 사용 (experts는 FFN 부분에만 적용)
- GQA + SWA 계승

**핵심 Training 혁신**

- **Expert specialization**: 명시적으로 전문성을 부여하지 않아도 각 expert가 자연스럽게 다른 특성의 토큰을 처리하도록 학습됨 (언어별, 문법적 역할별 등)
- **Load balancing loss**: 특정 expert에 토큰이 집중되는 것을 방지하기 위한 auxiliary loss 추가

**이전 모델 대비 해결한 문제**

- **추론 비용 절감**: 46.7B 파라미터를 갖지만 실제 계산량은 12.9B 수준
- **다국어 강화**: 영어, 프랑스어, 이탈리아어, 스페인어, 독일어에서 강한 성능
- **코딩**: HumanEval에서 GPT-3.5 수준 달성

---

### 3.3 Mistral Large 2 (2024년 7월)

**등장 배경 및 의의**

Mistral의 첫 번째 진정한 frontier-class 모델. **123B 파라미터**로 GPT-4와 경쟁하는 수준의 성능을 제공하며, 특히 코딩과 다국어에서 강점을 보인다.

**핵심 특징**

- **123B dense 모델**
- **128K context window**
- 코딩 성능: HumanEval 92% (당시 최고 수준 오픈 모델)
- 다국어: 영어, 프랑스어, 독일어, 스페인어, 이탈리아어, 포르투갈어, 아랍어, 힌디어, 러시아어, 중국어, 일본어, 한국어
- Function calling 및 JSON 출력 공식 지원

---

### 3.4 Mistral Small 3 (2025년 1월)

**등장 배경 및 의의**

2025년 초 출시된 **24B 파라미터** 모델로, "small but capable"의 포지셔닝. 로컬 배포 및 비용 효율적인 서비스를 위해 설계되었다.

**핵심 특징**

- 24B 파라미터
- Apache 2.0 라이선스 (완전 오픈)
- MMLU, HumanEval 등에서 GPT-4o-mini 대비 경쟁력 있는 성능
- 저지연, 저비용 API 배포에 최적화

---

## 4. Google Family (PaLM / Gemma / Gemini)

Google/DeepMind는 LLM 연구에서 Transformer 자체를 발명한 기관으로, 다양한 아키텍처적 혁신을 지속해왔다.

---

### 4.1 PaLM / PaLM-2 (2022 / 2023)

**등장 배경 및 의의**

PaLM (Pathways Language Model, 2022)은 **540B 파라미터**로 당시 최대 규모 LLM 중 하나였다. Google의 Pathways 인프라를 활용하여 6,144개의 TPU chip에서 학습되었다. PaLM-2 (2023)는 Gemini 전 세대의 최후 대형 모델로, Bard(현 Gemini)의 기반이 되었다.

**핵심 아키텍처 혁신 (PaLM)**

- **Multi-Query Attention (MQA)**: 모든 query head가 단일 K, V head를 공유 → KV cache 메모리 대폭 감소
- **Parallel Transformer Blocks**: Attention과 FFN을 sequential이 아닌 parallel로 배치하여 학습 속도 15% 향상 (일부 성능 trade-off)
- **SwiGLU activation**
- **RoPE (선택적 적용)**
- 780B tokens 학습 (고품질 multilingual, code, math 데이터 포함)

**핵심 Training 혁신**

- **Pathways training**: 분산 학습을 위한 Google 전용 인프라
- **Chain-of-thought prompting 최적화**: CoT를 위한 데이터 포함
- **Multilingual 강화**: 120개 이상 언어 데이터 포함

---

### 4.2 Gemma 1 / Gemma 2 (2024)

**등장 배경 및 의의**

Gemma는 Google DeepMind의 오픈소스 LLM 시리즈다. Gemini와 같은 연구 및 기술을 소형 모델에 적용하여 공개했다. Gemma 2는 특히 **knowledge distillation**과 새로운 attention 기법을 도입했다.

**Gemma 1 핵심 아키텍처 혁신**

- 2B, 7B 두 가지 크기
- **Multi-Head Attention (MHA)** 사용
- **RoPE**
- **GeGLU activation**
- **256K 어휘 tokenizer** (SentencePiece)
- Gemini 모델과 동일한 tokenizer 및 주요 아키텍처 공유

**Gemma 2 핵심 아키텍처 혁신**

- **2B, 9B, 27B** 세 가지 크기
- **Sliding Window Attention + Global Attention 교대 배치**: 홀수 레이어는 SWA (sliding window 4096 tokens), 짝수 레이어는 global attention. 효율성과 장거리 의존성을 모두 확보.
- **Grouped Query Attention (GQA)**: 효율적 추론
- **Logit Soft-Capping**: Attention logits와 final logits에 tanh 기반 soft-cap 적용 (`logit / cap * tanh(logit / cap)` 형태). 극단적 logit 값을 억제하여 학습 안정성 향상.
- **Knowledge Distillation from larger Gemini models**: 작은 모델 학습 시 큰 Gemini 모델의 분포를 teacher로 활용

**핵심 Training 혁신**

- **Distillation-based training**: 단순히 token prediction만 하는 것이 아니라, Gemini의 output distribution을 모방하는 distillation loss 추가
- Instruction tuning 및 RLHF 적용 버전(Gemma Instruct)도 공개

---

### 4.3 Gemma 3 (2025년 3월 예정 / 발표 2025)

**등장 배경 및 의의**

Gemma 3는 **최초의 multimodal Gemma** 시리즈로, 텍스트와 이미지를 함께 처리한다. 1B부터 27B까지 다양한 크기를 제공하며, 특히 single-GPU 로컬 배포에 최적화되어 있다.

**핵심 특징**

- **Vision support**: 이미지 입력 처리 가능 (SigLIP vision encoder 통합)
- **1B, 4B, 12B, 27B** 크기
- **128K context window**
- 다국어 지원 대폭 강화 (140개 이상 언어)
- Gemma 2 대비 향상된 instruction following 및 reasoning

---

### 4.4 Gemini 1 / 1.5 / 2 (2023-2025)

**등장 배경 및 의의**

Gemini는 Google DeepMind의 **flagship native multimodal** 모델 시리즈다. 처음부터 텍스트, 이미지, 오디오, 비디오를 통합 처리하도록 설계되었으며, Gemini 1.5에서 **1M token context window**를 달성했다.

**Gemini 1 (2023년 12월) 핵심 혁신**

- **Native multimodal from scratch**: 별도 비전 인코더가 아닌, 처음부터 모든 모달리티를 통합 학습
- **Ultra, Pro, Nano** 세 가지 크기 변형
- Gemini Ultra: GPT-4 수준의 성능, 최초로 MMLU에서 인간 전문가 수준(90%+) 달성
- **Efficient attention variants**: Efficient attention mechanisms for different deployment contexts

**Gemini 1.5 (2024년 2월) 핵심 혁신**

- **1M (일부 연구자 버전 10M) token context window**: 사실상 무제한에 가까운 컨텍스트
- 핵심 구현 기법:
  - **Multi-Query Attention (MQA)**: KV cache를 최소화
  - **Ring Attention**: 여러 TPU에 걸쳐 attention을 분산 처리
  - **Sliding Window Attention + Sparse Global Attention** 조합
- **Mixture of Experts 아키텍처** (Gemini 1.5 Pro/Flash)
- **Gemini 1.5 Flash**: 경량 버전, 빠르고 저렴 (1M context 유지)

**Gemini 2 (2024년 12월) 핵심 혁신**

- **Gemini 2.0 Flash**: 빠른 추론, multimodal input/output (이미지 생성, TTS 포함)
- **Native tool use**: 코드 실행, 검색, 서드파티 API 직접 호출
- **Agentic capabilities**: 멀티스텝 작업 자율 수행
- **Gemini 2.0 Flash Thinking**: o1처럼 visible reasoning/thinking 지원
- **Gemini 2.0 Pro (Experimental)**: 최고 성능 버전, coding benchmark 선두

**이전 모델 대비 해결한 문제**

- **Long-context understanding**: 1M 컨텍스트로 책 전체, 긴 코드베이스 분석 가능
- **Native multimodal**: 모달리티 간 seamless reasoning
- **Tool use & agent**: 검색, 코드 실행, API 호출을 자율적으로 수행

---

## 5. Alibaba Qwen Family

Qwen (通义千问)은 Alibaba Cloud가 개발한 LLM 시리즈로, **중국어와 영어 이중 언어**에서 강한 성능을 보이며 최근에는 글로벌 frontier model과 경쟁하는 수준에 이르렀다.

---

### 5.1 Qwen 1 / 1.5 / 2 / 2.5 (2023-2024)

**등장 배경 및 의의**

Qwen 시리즈는 중국어 NLP에서 출발했지만, Qwen2.5에 이르러 **수학(Math), 코딩(Coder), 과학(Science)에서 세계 최고 수준**의 specialized 버전들을 내놓았다. 0.5B부터 72B까지 폭넓은 크기 범위로 다양한 용도에 대응한다.

**Qwen1 (2023년 9월) 핵심 아키텍처**

- **1.8B, 7B, 14B, 72B** 크기
- **BF16 학습**, Transformer decoder-only
- **Attention 개선**: FlashAttention 통합, **NTK-aware interpolation**으로 컨텍스트 길이 확장
- **Multilingual tokenizer**: 152K 어휘, 중국어/영어 최적화
- **Dynamic NTK-aware scaling**: 추론 시 컨텍스트 길이에 따라 RoPE scaling을 동적으로 조정

**Qwen1.5 (2024년 2월) 핵심 개선**

- 0.5B, 1.8B, 4B, 7B, 14B, 32B, 72B 전 크기 GQA 적용
- 64개 언어 지원, 채팅 버전 품질 대폭 향상
- 모든 크기에서 **Apache 2.0 라이선스** 공개

**Qwen2 (2024년 6월) 핵심 혁신**

- **0.5B, 1.5B, 7B, 57B-A14B (MoE), 72B** 크기
- **Qwen2 57B-A14B**: sparse MoE 모델, 57B total / 14B active
- **128K context** (7B, 72B)
- 수학, 코딩 능력 대폭 향상
- 29개 언어 지원

**Qwen2.5 (2024년 9월) 핵심 혁신**

- **0.5B, 1.5B, 3B, 7B, 14B, 32B, 72B** 전 라인업 업데이트
- **18T tokens** 사전학습 (Qwen2 대비 2배 이상)
- **Math 전용**: Qwen2.5-Math (1.5B, 7B, 72B) - AIME, AMC에서 최고 수준
- **Coder 전용**: Qwen2.5-Coder (0.5B ~ 32B) - HumanEval 92.7% (32B 기준)
- **Structured output** (JSON mode) 강화
- Instruction following, role-playing 대폭 개선

---

### 5.2 QwQ (2024년 11월)

**등장 배경 및 의의**

QwQ-32B-Preview는 Qwen 팀이 개발한 **reasoning/thinking model**로, OpenAI o1처럼 긴 chain-of-thought 추론 과정을 거쳐 수학/과학 문제를 해결한다. 오픈소스 reasoning model의 첫 주요 사례 중 하나다.

**핵심 특징**

- **32B 파라미터** (Qwen2.5 기반)
- **긴 thinking process**: 문제를 풀기 전 수천 토큰의 내부 reasoning 생성
- **Self-reflection**: 중간에 자신의 추론을 검토하고 수정하는 능력
- AIME 2024에서 50% (o1 Preview 44.6% 대비 우세)
- LiveCodeBench에서 o1-preview 대비 경쟁력
- **단점**: 추론 과정에서 언어 혼용(중영 혼용), 특정 경우 circular reasoning

---

### 5.3 Qwen2.5-Max (2025년 1월)

**등장 배경 및 의의**

Qwen2.5-Max는 Alibaba의 **frontier-class MoE 모델**로, GPT-4o, DeepSeek-V3 등 최상위 모델과 경쟁하는 수준이다. 세부 파라미터 수는 비공개이나 대규모 MoE 아키텍처로 추정된다.

**핵심 특징**

- Qwen2.5 아키텍처 기반 대규모 MoE
- MMLU, HumanEval, MATH 등 주요 벤치마크에서 최상위권
- Alibaba Cloud API를 통해 상업적 서비스
- 추론, 코딩, 수학, 다국어에서 균형 잡힌 성능

---

## 6. DeepSeek Family

DeepSeek (深度求索)은 중국의 hedge fund 계열 AI 연구소로, 2024-2025년 **아키텍처 혁신과 효율적 학습**으로 글로벌 AI 업계에 큰 충격을 주었다. DeepSeek-R1의 공개는 특히 미국 AI 주식을 폭락시킬 만큼 파급효과가 컸다.

---

### 6.1 DeepSeek-V2 (2024년 5월)

**등장 배경 및 의의**

DeepSeek-V2는 **MLA (Multi-head Latent Attention)**라는 새로운 attention 메커니즘을 도입하여 KV cache를 대폭 줄이면서 성능은 유지했다. 동시에 MoE 아키텍처로 학습 및 추론 비용을 혁신적으로 절감했다.

**핵심 아키텍처 혁신**

**MLA (Multi-head Latent Attention)**:

MLA는 KV cache 문제를 근본적으로 해결하는 혁신적 메커니즘이다.

- 기존 Multi-Head Attention: 각 레이어마다 `n_heads × d_head × 2 (K+V)` 크기의 KV cache 필요
- MLA의 핵심 아이디어: **Low-rank joint compression of K and V**
  1. 입력 hidden state `h`를 작은 **latent vector** `c_KV`로 압축 (down-projection)
  2. 추론 시 `c_KV`만 캐시
  3. 필요할 때 `c_KV`에서 K, V를 복원 (up-projection)
- KV cache를 표준 MHA 대비 **93.3%** 절감
- **Decoupled RoPE**: Position encoding을 일반 K, V와 분리하여 MLA와 호환

**MoE 아키텍처 (DeepSeekMoE)**:

- **Fine-grained expert segmentation**: 각 expert를 더 작게 만들고 더 많은 수 유지
- **Shared + Routed experts**: 항상 활성화되는 shared expert + 라우팅되는 routed expert 조합
  - Shared expert: 공통 지식 담당
  - Routed expert: 특화 지식 담당
- 236B total / 21B active 파라미터

**핵심 Training 혁신**

- 총 8.1T tokens 사전학습
- MLA 도입으로 추론 시 메모리 절약 → 긴 context batch 처리 가능

---

### 6.2 DeepSeek-V3 (2024년 12월)

**등장 배경 및 의의**

DeepSeek-V3는 **671B total / 37B active** 파라미터의 대규모 MoE 모델로, 단 **약 560만 달러**의 학습 비용으로 GPT-4o, Claude-3.5-Sonnet 수준의 성능을 달성하여 업계를 충격에 빠뜨렸다.

**핵심 아키텍처 혁신**

- MLA + DeepSeekMoE (V2에서 계승, 개선)
- **Multi-Token Prediction (MTP)**: 기존 next-token prediction에서 확장하여, 각 위치에서 다음 K개의 토큰을 동시에 예측. 학습 신호 밀도 향상 + 추론 속도 향상 (speculative decoding 가능)
- **Enhanced load balancing**: Auxiliary-loss-free load balancing 전략으로 expert 불균형 없이 학습
- 671B total / 37B active, 256 routed experts + 1 shared expert

**핵심 Training 혁신**

- **FP8 mixed-precision training**: 처음으로 대규모 LLM에 FP8을 성공적으로 적용하여 메모리와 계산량 절감
- **Pipeline parallelism 최적화**: DualPipe 알고리즘으로 pipeline bubble 최소화
- **학습 비용**: 2048 H800 GPU로 약 2.8M GPU-hours, 비용 약 $5.576M
- 14.8T tokens 사전학습

---

### 6.3 DeepSeek-R1 (2025년 1월)

**등장 배경 및 의의**

DeepSeek-R1은 **RLVR (Reinforcement Learning with Verifiable Rewards)**를 통해 GPT-o1에 필적하는 reasoning 능력을 달성한 **오픈 웨이트 reasoning model**이다. 특히 OpenAI o1과 비슷한 수준의 수학/코딩 성능을 open-source로 공개하여, 미국 AI 주식 폭락과 함께 큰 화제가 되었다.

**핵심 아키텍처**

- DeepSeek-V3 베이스 모델에서 출발
- **Reasoning trace**: 긴 chain-of-thought를 생성하여 문제 해결
- `<think>...</think>` 태그로 reasoning과 최종 답변을 구분

**핵심 Training 혁신 (R1의 핵심)**

**1. DeepSeek-R1-Zero (RL only):**
- SFT 없이 순수 RL만으로 reasoning 능력 발현 실험
- **GRPO (Group Relative Policy Optimization)**: PPO의 변형으로, 참조 모델의 critic network 없이 그룹 내 상대적 보상으로 학습
- **Verifiable reward**: 수학 답의 정확성, 코드의 컴파일/실행 성공 여부 등 자동 검증 가능한 reward
- **Format reward**: `<think></think>` 형식 준수에 대한 reward
- **결과**: RL만으로도 긴 chain-of-thought와 self-reflection이 **emergent**하게 발현됨

**2. DeepSeek-R1 (Full pipeline):**
- **Stage 1 (Cold Start)**: 소량의 고품질 long CoT data로 SFT → RL 학습 시작점 안정화
- **Stage 2 (RLVR)**: 대규모 RLVR 학습 (수천 스텝)
- **Stage 3 (Rejection Sampling + SFT)**: RL 학습된 모델의 출력 중 고품질 데이터를 선별하여 다시 SFT
- **Stage 4 (Final RL)**: Helpfulness/harmlessness를 포함한 최종 RL round

**성능 비교**

| 벤치마크 | DeepSeek-R1 | OpenAI o1 |
|----------|-------------|-----------|
| AIME 2024 | 79.8% | 79.2% |
| MATH-500 | 97.3% | 96.4% |
| Codeforces | 96.3 percentile | 96.6 percentile |

---

### 6.4 DeepSeek-R1 Distill Variants

**등장 배경 및 의의**

R1의 reasoning 능력을 소형 모델에 이식하기 위해, R1이 생성한 long CoT 데이터를 사용하여 Qwen/LLaMA 기반 소형 모델을 distillation했다.

**공개 모델**

| 모델 | 베이스 | 크기 |
|------|--------|------|
| DeepSeek-R1-Distill-Qwen-1.5B | Qwen2.5-1.5B | 1.5B |
| DeepSeek-R1-Distill-Qwen-7B | Qwen2.5-7B | 7B |
| DeepSeek-R1-Distill-Qwen-14B | Qwen2.5-14B | 14B |
| DeepSeek-R1-Distill-Qwen-32B | Qwen2.5-32B | 32B |
| DeepSeek-R1-Distill-LLaMA-8B | LLaMA-3.1-8B | 8B |
| DeepSeek-R1-Distill-LLaMA-70B | LLaMA-3.3-70B | 70B |

**핵심 발견**

- R1-Distill-Qwen-7B가 GPT-4o를 수학에서 능가
- R1-Distill-Qwen-32B가 o1-mini보다 강한 수학 성능
- **Knowledge distillation from reasoning traces**가 소형 모델에서도 reasoning 능력을 효과적으로 이식함을 입증

---

## 7. Other Notable Models

### 7.1 Phi Series (Microsoft)

**등장 배경 및 의의**

Microsoft Research의 Phi 시리즈는 **"quality over quantity"** 원칙으로, **고품질 합성 데이터**로 소형 모델을 학습시키면 대형 모델에 준하는 성능을 낼 수 있음을 증명했다.

**Phi-1 (2023년 6월)**
- **1.3B 파라미터**
- "Textbooks are all you need": GPT-4로 생성한 합성 교육용 코딩 데이터로 학습
- HumanEval 50.6% - 당시 13B 이하 최고 성능

**Phi-1.5 (2023년 9월)**
- **1.3B 파라미터**
- NLP 태스크로 확장, 합성 데이터 + 웹 필터링 데이터 혼합
- 상식 추론 벤치마크에서 5B 이상 모델과 경쟁

**Phi-2 (2023년 12월)**
- **2.7B 파라미터**
- "Phi-1.5 + 합성 데이터 확장" → 13B Llama-2보다 우수한 수학/코딩 성능

**Phi-3 (2024년 4월)**
- **Mini(3.8B), Small(7B), Medium(14B)** 세 가지 크기
- 합성 데이터 전략 고도화: 다단계 큐레이션
- 모바일 배포 가능한 Mini 버전이 특히 주목

**Phi-4 (2024년 12월)**
- **14B 파라미터**
- 합성 데이터를 학습 전반에 걸쳐 통합 (pretraining + fine-tuning 양쪽)
- STEM, 수학에서 당시 동급 크기 최고 성능
- GPT-4o mini보다 수학/과학 벤치마크에서 우수

**핵심 교훈**: **데이터 품질이 데이터 양보다 중요**할 수 있으며, 소형 모델도 올바른 학습 데이터로 exceptional한 성능 달성 가능.

---

### 7.2 Command R+ (Cohere)

**등장 배경 및 의의**

Cohere의 **Command R+** (2024년 4월)은 **RAG (Retrieval-Augmented Generation)** 및 기업용 use case에 최적화된 모델이다.

**핵심 특징**

- **104B 파라미터**
- **Grounded generation**: 검색된 문서에 근거하여 답변 생성, 출처 인용
- **Multi-hop RAG**: 여러 단계의 검색을 통한 복잡한 질문 답변
- Tool use 및 multi-step reasoning
- 10개 언어 지원
- Apache 2.0 라이선스

**핵심 혁신**: RAG 성능을 명시적인 training objective로 삼아, 검색 결과를 효과적으로 활용하고 hallucination을 줄이는 능력 향상.

---

### 7.3 Claude Series (Anthropic)

**등장 배경 및 의의**

Anthropic은 OpenAI 전 연구자들이 설립한 회사로, **AI Safety**를 핵심 연구 목표로 삼는다. Claude 시리즈는 **Constitutional AI (CAI)**라는 독자적 alignment 방법론을 사용한다.

**Constitutional AI (CAI)**

기존 RLHF는 인간 라벨러의 preference에 의존하는 반면, CAI는:
1. **원칙 목록(Constitution)** 작성 (예: "유해한 내용을 피하라", "정직하라")
2. AI가 스스로 이 원칙에 따라 자신의 응답을 평가하고 수정 (Self-critique)
3. 이렇게 생성된 AI 피드백으로 reward model 학습 (RLAIF: RL from AI Feedback)

**Claude 3 (2024년 3월)**

- **Haiku** (소형, 빠름), **Sonnet** (중간), **Opus** (최고 성능) 세 티어
- Claude 3 Opus: GPT-4 수준, 200K context window
- Vision 능력 (이미지 이해)

**Claude 3.5 (2024년 6월~2024년 10월)**

- **Claude 3.5 Sonnet**: 코딩, 추론에서 Claude 3 Opus 능가. **Artifacts** (코드 실시간 실행 및 시각화) 기능 추가.
- **Claude 3.5 Haiku**: Claude 3 Opus 수준 성능을 Haiku 가격에 제공

**Claude 3.7 Sonnet (2025년 2월)**

- **Extended Thinking**: o1처럼 긴 internal reasoning process를 가시적으로 공개
- "Hybrid" 모드: thinking 없는 빠른 응답과 thinking 있는 심층 추론 선택 가능
- SWE-bench (소프트웨어 엔지니어링): 62.3% (업계 최고 수준)
- 코딩, 추론, 분석에서 현재 최고 수준 모델 중 하나

---

### 7.4 Falcon (TII)

**등장 배경 및 의의**

UAE의 Technology Innovation Institute (TII)가 개발한 Falcon은 **초기 오픈소스 경쟁력 있는 LLM**의 선구자 중 하나였다. 2023년 Falcon-40B는 당시 공개 모델 중 최고 수준이었다.

**핵심 아키텍처 혁신**

- **Multi-Query Attention (MQA)**: 추론 효율 향상
- **Parallel Attention + FFN**: PaLM과 유사한 병렬 구조
- **RefinedWeb**: 대규모 웹 데이터의 고품질 필터링 파이프라인 개발 (FineWeb의 선구자)
- Falcon-7B, 40B, 180B

**역사적 의미**: LLaMA-1과 함께 오픈소스 LLM 생태계 형성에 기여. RefinedWeb 데이터 큐레이션 방법론이 이후 데이터 필터링 연구에 영향.

---

## 핵심 요약 비교표

| 모델 | 출시 | 파라미터 | 핵심 혁신 | Open? |
|------|------|----------|-----------|-------|
| GPT-3 | 2020 | 175B | In-context learning | No |
| InstructGPT | 2022 | 175B | RLHF alignment | No |
| LLaMA-1 | 2023.02 | 7B-65B | Open-weights, compute-optimal | Yes |
| GPT-4 | 2023.03 | ? (MoE 추정) | Multimodal, capability jump | No |
| Mistral-7B | 2023.09 | 7B | SWA + GQA 효율 | Yes |
| Mixtral 8x7B | 2023.12 | 46.7B/12.9B active | Sparse MoE | Yes |
| LLaMA-2 | 2023.07 | 7B-70B | RLHF chat, GQA, commercial | Yes |
| Gemini 1 | 2023.12 | ? | Native multimodal | No |
| LLaMA-3.1 | 2024.07 | 8B-405B | 128K context, GQA all | Yes |
| GPT-4o | 2024.05 | ? | Native omni-modal | No |
| Gemini 1.5 | 2024.02 | ? (MoE) | 1M context | No |
| Qwen2.5 | 2024.09 | 0.5B-72B | Math/Code SOTA small models | Yes |
| DeepSeek-V2 | 2024.05 | 236B/21B | MLA + MoE | Yes |
| DeepSeek-V3 | 2024.12 | 671B/37B | FP8, MTP, $6M training | Yes |
| o1 | 2024.09 | ? | Inference-time reasoning | No |
| DeepSeek-R1 | 2025.01 | 671B | RLVR, open reasoning | Yes |
| Gemini 2.0 | 2024.12 | ? | Agentic, native tools | No |
| Claude 3.7 | 2025.02 | ? | Extended thinking, SWE | No |
| Gemma 3 | 2025 | 1B-27B | Multimodal, open | Yes |
| Mistral Small 3 | 2025.01 | 24B | Efficient, Apache 2.0 | Yes |
